<!DOCTYPE html>
<!-- saved from url=(0039)https://build.nvidia.com/explore/speech -->
<html class="nv-dark bg-transparent font-sans os-windows" id="app" lang="en" style="--app-bar-height: 48.66666793823242px;"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><link rel="stylesheet" href="./Explore Speech Models _ Try NVIDIA NIM APIs_files/f6d332f52e251a32.css" data-precedence="next"><link rel="stylesheet" href="./Explore Speech Models _ Try NVIDIA NIM APIs_files/3a01ffc7bcbb63dc.css" data-precedence="next"><link rel="stylesheet" href="./Explore Speech Models _ Try NVIDIA NIM APIs_files/2527a870c9602126.css" data-precedence="next"><link rel="stylesheet" href="./Explore Speech Models _ Try NVIDIA NIM APIs_files/7a03bc21c9d936ab.css" data-precedence="next"><link rel="stylesheet" href="./Explore Speech Models _ Try NVIDIA NIM APIs_files/7991542044add72e.css" data-precedence="next"><link rel="stylesheet" href="./Explore Speech Models _ Try NVIDIA NIM APIs_files/315e2c22febd8cea.css" data-precedence="next"><link rel="stylesheet" href="./Explore Speech Models _ Try NVIDIA NIM APIs_files/8eeb89dd0ee182d1.css" data-precedence="next"><link rel="preload" as="script" fetchpriority="low" href="./Explore Speech Models _ Try NVIDIA NIM APIs_files/webpack-5957dab4d02a0e59.js.下载"><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/bat.js.下载" async=""></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/bat.js.下载" async=""></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/bat.js.下载" async=""></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/bat.js.下载" async=""></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/bat.js.下载" async=""></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/bat.js.下载" async=""></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/bat.js.下载" async=""></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/bat.js.下载" async=""></script><script type="text/javascript" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/f12a4347e1080fb88155.js.下载" async="" status="error"></script><script async="" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/fbevents.js.下载"></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/bat.js.下载" async=""></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/pixel.js.下载" async=""></script><script type="text/javascript" async="" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/insight.min.js.下载"></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/cd3ff8ff-a26edc5cae0bc2c5.js.下载" async=""></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/16034-021ddcc1368f2133.js.下载" async=""></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/main-app-79c88578ffa0b6ef.js.下载" async=""></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/cef99e24-d6731e3d04a81d8a.js.下载" async=""></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/1c5dbc3d-c63152837a7859da.js.下载" async=""></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/a9670928-c55d94caff555700.js.下载" async=""></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/b794d9c1-6ad16db09e2cd0a0.js.下载" async=""></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/cb9c0786-c452c7cb5b6570dc.js.下载" async=""></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/df3b6c2d-589577bd4490795d.js.下载" async=""></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/33214-d9197868e7892301.js.下载" async=""></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/45617-cfa1b086b480deb6.js.下载" async=""></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/6753-e4b994d0c8f68c36.js.下载" async=""></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/87307-d266ed7473cbf6ae.js.下载" async=""></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/13574-6e026190fef980e8.js.下载" async=""></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/4854-39125d9813ba652b.js.下载" async=""></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/71917-5e847e6cfe5fa495.js.下载" async=""></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/38366-ccbd630daa1a2ff8.js.下载" async=""></script><link rel="preload" href="./Explore Speech Models _ Try NVIDIA NIM APIs_files/launch-f1e5e96c44d5.min.js.下载" as="script"><link rel="preload" href="./Explore Speech Models _ Try NVIDIA NIM APIs_files/otSDKStub.js.下载" as="script"><link rel="preload" href="./Explore Speech Models _ Try NVIDIA NIM APIs_files/ot-custom.js.下载" as="script"><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/polyfills-42372ed130431b0a.js.下载" nomodule=""></script><script>bazadebezolkohpepadr="1492890901"</script><script type="text/javascript" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/58fbb61e" defer=""></script><script type="speculationrules" id="speculationrules">{"prerender":[{"where":{"and":[{"not":{"href_matches":["/internal/agents*","/internal/tools*","/internal/packages*","/settings*"]}}]},"eagerness":"moderate"}]}</script><script data-document-language="true" data-domain-script="3e2b62ff-7ae7-4ac5-87c8-d5949ecafff5" type="text/javascript" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/otSDKStub.js.下载"></script><style></style><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/otBannerSdk.js.下载" async="" type="text/javascript"></script><style type="text/css">[data-sonner-toaster][dir=ltr],html[dir=ltr]{--toast-icon-margin-start:-3px;--toast-icon-margin-end:4px;--toast-svg-margin-start:-1px;--toast-svg-margin-end:0px;--toast-button-margin-start:auto;--toast-button-margin-end:0;--toast-close-button-start:0;--toast-close-button-end:unset;--toast-close-button-transform:translate(-35%, -35%)}[data-sonner-toaster][dir=rtl],html[dir=rtl]{--toast-icon-margin-start:4px;--toast-icon-margin-end:-3px;--toast-svg-margin-start:0px;--toast-svg-margin-end:-1px;--toast-button-margin-start:0;--toast-button-margin-end:auto;--toast-close-button-start:unset;--toast-close-button-end:0;--toast-close-button-transform:translate(35%, -35%)}[data-sonner-toaster]{position:fixed;width:var(--width);font-family:ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji;--gray1:hsl(0, 0%, 99%);--gray2:hsl(0, 0%, 97.3%);--gray3:hsl(0, 0%, 95.1%);--gray4:hsl(0, 0%, 93%);--gray5:hsl(0, 0%, 90.9%);--gray6:hsl(0, 0%, 88.7%);--gray7:hsl(0, 0%, 85.8%);--gray8:hsl(0, 0%, 78%);--gray9:hsl(0, 0%, 56.1%);--gray10:hsl(0, 0%, 52.3%);--gray11:hsl(0, 0%, 43.5%);--gray12:hsl(0, 0%, 9%);--border-radius:8px;box-sizing:border-box;padding:0;margin:0;list-style:none;outline:0;z-index:999999999;transition:transform .4s ease}@media (hover:none) and (pointer:coarse){[data-sonner-toaster][data-lifted=true]{transform:none}}[data-sonner-toaster][data-x-position=right]{right:var(--offset-right)}[data-sonner-toaster][data-x-position=left]{left:var(--offset-left)}[data-sonner-toaster][data-x-position=center]{left:50%;transform:translateX(-50%)}[data-sonner-toaster][data-y-position=top]{top:var(--offset-top)}[data-sonner-toaster][data-y-position=bottom]{bottom:var(--offset-bottom)}[data-sonner-toast]{--y:translateY(100%);--lift-amount:calc(var(--lift) * var(--gap));z-index:var(--z-index);position:absolute;opacity:0;transform:var(--y);touch-action:none;transition:transform .4s,opacity .4s,height .4s,box-shadow .2s;box-sizing:border-box;outline:0;overflow-wrap:anywhere}[data-sonner-toast][data-styled=true]{padding:16px;background:var(--normal-bg);border:1px solid var(--normal-border);color:var(--normal-text);border-radius:var(--border-radius);box-shadow:0 4px 12px rgba(0,0,0,.1);width:var(--width);font-size:13px;display:flex;align-items:center;gap:6px}[data-sonner-toast]:focus-visible{box-shadow:0 4px 12px rgba(0,0,0,.1),0 0 0 2px rgba(0,0,0,.2)}[data-sonner-toast][data-y-position=top]{top:0;--y:translateY(-100%);--lift:1;--lift-amount:calc(1 * var(--gap))}[data-sonner-toast][data-y-position=bottom]{bottom:0;--y:translateY(100%);--lift:-1;--lift-amount:calc(var(--lift) * var(--gap))}[data-sonner-toast][data-styled=true] [data-description]{font-weight:400;line-height:1.4;color:#3f3f3f}[data-rich-colors=true][data-sonner-toast][data-styled=true] [data-description]{color:inherit}[data-sonner-toaster][data-sonner-theme=dark] [data-description]{color:#e8e8e8}[data-sonner-toast][data-styled=true] [data-title]{font-weight:500;line-height:1.5;color:inherit}[data-sonner-toast][data-styled=true] [data-icon]{display:flex;height:16px;width:16px;position:relative;justify-content:flex-start;align-items:center;flex-shrink:0;margin-left:var(--toast-icon-margin-start);margin-right:var(--toast-icon-margin-end)}[data-sonner-toast][data-promise=true] [data-icon]>svg{opacity:0;transform:scale(.8);transform-origin:center;animation:sonner-fade-in .3s ease forwards}[data-sonner-toast][data-styled=true] [data-icon]>*{flex-shrink:0}[data-sonner-toast][data-styled=true] [data-icon] svg{margin-left:var(--toast-svg-margin-start);margin-right:var(--toast-svg-margin-end)}[data-sonner-toast][data-styled=true] [data-content]{display:flex;flex-direction:column;gap:2px}[data-sonner-toast][data-styled=true] [data-button]{border-radius:4px;padding-left:8px;padding-right:8px;height:24px;font-size:12px;color:var(--normal-bg);background:var(--normal-text);margin-left:var(--toast-button-margin-start);margin-right:var(--toast-button-margin-end);border:none;font-weight:500;cursor:pointer;outline:0;display:flex;align-items:center;flex-shrink:0;transition:opacity .4s,box-shadow .2s}[data-sonner-toast][data-styled=true] [data-button]:focus-visible{box-shadow:0 0 0 2px rgba(0,0,0,.4)}[data-sonner-toast][data-styled=true] [data-button]:first-of-type{margin-left:var(--toast-button-margin-start);margin-right:var(--toast-button-margin-end)}[data-sonner-toast][data-styled=true] [data-cancel]{color:var(--normal-text);background:rgba(0,0,0,.08)}[data-sonner-toaster][data-sonner-theme=dark] [data-sonner-toast][data-styled=true] [data-cancel]{background:rgba(255,255,255,.3)}[data-sonner-toast][data-styled=true] [data-close-button]{position:absolute;left:var(--toast-close-button-start);right:var(--toast-close-button-end);top:0;height:20px;width:20px;display:flex;justify-content:center;align-items:center;padding:0;color:var(--gray12);background:var(--normal-bg);border:1px solid var(--gray4);transform:var(--toast-close-button-transform);border-radius:50%;cursor:pointer;z-index:1;transition:opacity .1s,background .2s,border-color .2s}[data-sonner-toast][data-styled=true] [data-close-button]:focus-visible{box-shadow:0 4px 12px rgba(0,0,0,.1),0 0 0 2px rgba(0,0,0,.2)}[data-sonner-toast][data-styled=true] [data-disabled=true]{cursor:not-allowed}[data-sonner-toast][data-styled=true]:hover [data-close-button]:hover{background:var(--gray2);border-color:var(--gray5)}[data-sonner-toast][data-swiping=true]::before{content:'';position:absolute;left:-100%;right:-100%;height:100%;z-index:-1}[data-sonner-toast][data-y-position=top][data-swiping=true]::before{bottom:50%;transform:scaleY(3) translateY(50%)}[data-sonner-toast][data-y-position=bottom][data-swiping=true]::before{top:50%;transform:scaleY(3) translateY(-50%)}[data-sonner-toast][data-swiping=false][data-removed=true]::before{content:'';position:absolute;inset:0;transform:scaleY(2)}[data-sonner-toast][data-expanded=true]::after{content:'';position:absolute;left:0;height:calc(var(--gap) + 1px);bottom:100%;width:100%}[data-sonner-toast][data-mounted=true]{--y:translateY(0);opacity:1}[data-sonner-toast][data-expanded=false][data-front=false]{--scale:var(--toasts-before) * 0.05 + 1;--y:translateY(calc(var(--lift-amount) * var(--toasts-before))) scale(calc(-1 * var(--scale)));height:var(--front-toast-height)}[data-sonner-toast]>*{transition:opacity .4s}[data-sonner-toast][data-x-position=right]{right:0}[data-sonner-toast][data-x-position=left]{left:0}[data-sonner-toast][data-expanded=false][data-front=false][data-styled=true]>*{opacity:0}[data-sonner-toast][data-visible=false]{opacity:0;pointer-events:none}[data-sonner-toast][data-mounted=true][data-expanded=true]{--y:translateY(calc(var(--lift) * var(--offset)));height:var(--initial-height)}[data-sonner-toast][data-removed=true][data-front=true][data-swipe-out=false]{--y:translateY(calc(var(--lift) * -100%));opacity:0}[data-sonner-toast][data-removed=true][data-front=false][data-swipe-out=false][data-expanded=true]{--y:translateY(calc(var(--lift) * var(--offset) + var(--lift) * -100%));opacity:0}[data-sonner-toast][data-removed=true][data-front=false][data-swipe-out=false][data-expanded=false]{--y:translateY(40%);opacity:0;transition:transform .5s,opacity .2s}[data-sonner-toast][data-removed=true][data-front=false]::before{height:calc(var(--initial-height) + 20%)}[data-sonner-toast][data-swiping=true]{transform:var(--y) translateY(var(--swipe-amount-y,0)) translateX(var(--swipe-amount-x,0));transition:none}[data-sonner-toast][data-swiped=true]{user-select:none}[data-sonner-toast][data-swipe-out=true][data-y-position=bottom],[data-sonner-toast][data-swipe-out=true][data-y-position=top]{animation-duration:.2s;animation-timing-function:ease-out;animation-fill-mode:forwards}[data-sonner-toast][data-swipe-out=true][data-swipe-direction=left]{animation-name:swipe-out-left}[data-sonner-toast][data-swipe-out=true][data-swipe-direction=right]{animation-name:swipe-out-right}[data-sonner-toast][data-swipe-out=true][data-swipe-direction=up]{animation-name:swipe-out-up}[data-sonner-toast][data-swipe-out=true][data-swipe-direction=down]{animation-name:swipe-out-down}@keyframes swipe-out-left{from{transform:var(--y) translateX(var(--swipe-amount-x));opacity:1}to{transform:var(--y) translateX(calc(var(--swipe-amount-x) - 100%));opacity:0}}@keyframes swipe-out-right{from{transform:var(--y) translateX(var(--swipe-amount-x));opacity:1}to{transform:var(--y) translateX(calc(var(--swipe-amount-x) + 100%));opacity:0}}@keyframes swipe-out-up{from{transform:var(--y) translateY(var(--swipe-amount-y));opacity:1}to{transform:var(--y) translateY(calc(var(--swipe-amount-y) - 100%));opacity:0}}@keyframes swipe-out-down{from{transform:var(--y) translateY(var(--swipe-amount-y));opacity:1}to{transform:var(--y) translateY(calc(var(--swipe-amount-y) + 100%));opacity:0}}@media (max-width:600px){[data-sonner-toaster]{position:fixed;right:var(--mobile-offset-right);left:var(--mobile-offset-left);width:100%}[data-sonner-toaster][dir=rtl]{left:calc(var(--mobile-offset-left) * -1)}[data-sonner-toaster] [data-sonner-toast]{left:0;right:0;width:calc(100% - var(--mobile-offset-left) * 2)}[data-sonner-toaster][data-x-position=left]{left:var(--mobile-offset-left)}[data-sonner-toaster][data-y-position=bottom]{bottom:var(--mobile-offset-bottom)}[data-sonner-toaster][data-y-position=top]{top:var(--mobile-offset-top)}[data-sonner-toaster][data-x-position=center]{left:var(--mobile-offset-left);right:var(--mobile-offset-right);transform:none}}[data-sonner-toaster][data-sonner-theme=light]{--normal-bg:#fff;--normal-border:var(--gray4);--normal-text:var(--gray12);--success-bg:hsl(143, 85%, 96%);--success-border:hsl(145, 92%, 87%);--success-text:hsl(140, 100%, 27%);--info-bg:hsl(208, 100%, 97%);--info-border:hsl(221, 91%, 93%);--info-text:hsl(210, 92%, 45%);--warning-bg:hsl(49, 100%, 97%);--warning-border:hsl(49, 91%, 84%);--warning-text:hsl(31, 92%, 45%);--error-bg:hsl(359, 100%, 97%);--error-border:hsl(359, 100%, 94%);--error-text:hsl(360, 100%, 45%)}[data-sonner-toaster][data-sonner-theme=light] [data-sonner-toast][data-invert=true]{--normal-bg:#000;--normal-border:hsl(0, 0%, 20%);--normal-text:var(--gray1)}[data-sonner-toaster][data-sonner-theme=dark] [data-sonner-toast][data-invert=true]{--normal-bg:#fff;--normal-border:var(--gray3);--normal-text:var(--gray12)}[data-sonner-toaster][data-sonner-theme=dark]{--normal-bg:#000;--normal-bg-hover:hsl(0, 0%, 12%);--normal-border:hsl(0, 0%, 20%);--normal-border-hover:hsl(0, 0%, 25%);--normal-text:var(--gray1);--success-bg:hsl(150, 100%, 6%);--success-border:hsl(147, 100%, 12%);--success-text:hsl(150, 86%, 65%);--info-bg:hsl(215, 100%, 6%);--info-border:hsl(223, 43%, 17%);--info-text:hsl(216, 87%, 65%);--warning-bg:hsl(64, 100%, 6%);--warning-border:hsl(60, 100%, 9%);--warning-text:hsl(46, 87%, 65%);--error-bg:hsl(358, 76%, 10%);--error-border:hsl(357, 89%, 16%);--error-text:hsl(358, 100%, 81%)}[data-sonner-toaster][data-sonner-theme=dark] [data-sonner-toast] [data-close-button]{background:var(--normal-bg);border-color:var(--normal-border);color:var(--normal-text)}[data-sonner-toaster][data-sonner-theme=dark] [data-sonner-toast] [data-close-button]:hover{background:var(--normal-bg-hover);border-color:var(--normal-border-hover)}[data-rich-colors=true][data-sonner-toast][data-type=success]{background:var(--success-bg);border-color:var(--success-border);color:var(--success-text)}[data-rich-colors=true][data-sonner-toast][data-type=success] [data-close-button]{background:var(--success-bg);border-color:var(--success-border);color:var(--success-text)}[data-rich-colors=true][data-sonner-toast][data-type=info]{background:var(--info-bg);border-color:var(--info-border);color:var(--info-text)}[data-rich-colors=true][data-sonner-toast][data-type=info] [data-close-button]{background:var(--info-bg);border-color:var(--info-border);color:var(--info-text)}[data-rich-colors=true][data-sonner-toast][data-type=warning]{background:var(--warning-bg);border-color:var(--warning-border);color:var(--warning-text)}[data-rich-colors=true][data-sonner-toast][data-type=warning] [data-close-button]{background:var(--warning-bg);border-color:var(--warning-border);color:var(--warning-text)}[data-rich-colors=true][data-sonner-toast][data-type=error]{background:var(--error-bg);border-color:var(--error-border);color:var(--error-text)}[data-rich-colors=true][data-sonner-toast][data-type=error] [data-close-button]{background:var(--error-bg);border-color:var(--error-border);color:var(--error-text)}.sonner-loading-wrapper{--size:16px;height:var(--size);width:var(--size);position:absolute;inset:0;z-index:10}.sonner-loading-wrapper[data-visible=false]{transform-origin:center;animation:sonner-fade-out .2s ease forwards}.sonner-spinner{position:relative;top:50%;left:50%;height:var(--size);width:var(--size)}.sonner-loading-bar{animation:sonner-spin 1.2s linear infinite;background:var(--gray11);border-radius:6px;height:8%;left:-10%;position:absolute;top:-3.9%;width:24%}.sonner-loading-bar:first-child{animation-delay:-1.2s;transform:rotate(.0001deg) translate(146%)}.sonner-loading-bar:nth-child(2){animation-delay:-1.1s;transform:rotate(30deg) translate(146%)}.sonner-loading-bar:nth-child(3){animation-delay:-1s;transform:rotate(60deg) translate(146%)}.sonner-loading-bar:nth-child(4){animation-delay:-.9s;transform:rotate(90deg) translate(146%)}.sonner-loading-bar:nth-child(5){animation-delay:-.8s;transform:rotate(120deg) translate(146%)}.sonner-loading-bar:nth-child(6){animation-delay:-.7s;transform:rotate(150deg) translate(146%)}.sonner-loading-bar:nth-child(7){animation-delay:-.6s;transform:rotate(180deg) translate(146%)}.sonner-loading-bar:nth-child(8){animation-delay:-.5s;transform:rotate(210deg) translate(146%)}.sonner-loading-bar:nth-child(9){animation-delay:-.4s;transform:rotate(240deg) translate(146%)}.sonner-loading-bar:nth-child(10){animation-delay:-.3s;transform:rotate(270deg) translate(146%)}.sonner-loading-bar:nth-child(11){animation-delay:-.2s;transform:rotate(300deg) translate(146%)}.sonner-loading-bar:nth-child(12){animation-delay:-.1s;transform:rotate(330deg) translate(146%)}@keyframes sonner-fade-in{0%{opacity:0;transform:scale(.8)}100%{opacity:1;transform:scale(1)}}@keyframes sonner-fade-out{0%{opacity:1;transform:scale(1)}100%{opacity:0;transform:scale(.8)}}@keyframes sonner-spin{0%{opacity:1}100%{opacity:.15}}@media (prefers-reduced-motion){.sonner-loading-bar,[data-sonner-toast],[data-sonner-toast]>*{transition:none!important;animation:none!important}}.sonner-loader{position:absolute;top:50%;left:50%;transform:translate(-50%,-50%);transform-origin:center;transition:opacity .2s,transform .2s}.sonner-loader[data-visible=false]{opacity:0;transform:scale(.8) translate(-50%,-50%)}</style><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/EX3bcad69e9330462ba7ff1a05998df2ba-libraryCode_source.min.js.下载" async=""></script><script async="" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/hotjar-3655182.js.下载"></script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/RC7b2e087b89794250bb829d9db7b25a6b-source.min.js.下载" async=""></script><style id="onetrust-style">#onetrust-banner-sdk .onetrust-vendors-list-handler{cursor:pointer;color:#1f96db;font-size:inherit;font-weight:bold;text-decoration:none;margin-left:5px;white-space:normal;word-wrap:break-word;text-align:left}#onetrust-banner-sdk .onetrust-vendors-list-handler:hover{color:#1f96db}#onetrust-banner-sdk:focus{outline:2px solid #000;outline-offset:-2px}#onetrust-banner-sdk a:focus{outline:2px solid #000}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{outline-offset:1px}#onetrust-banner-sdk.ot-bnr-w-logo .ot-bnr-logo{height:64px;width:64px}#onetrust-banner-sdk #onetrust-policy svg,#onetrust-banner-sdk .banner-option svg{height:13px;width:13px;margin-left:1px;vertical-align:middle}#onetrust-banner-sdk .ot-tcf2-vendor-count.ot-text-bold{font-weight:bold}#onetrust-banner-sdk .ot-button-order-0{order:0}#onetrust-banner-sdk .ot-button-order-1{order:1}#onetrust-banner-sdk .ot-button-order-2{order:2}#onetrust-banner-sdk #onetrust-close-btn-container svg{height:10px;width:10px;pointer-events:none}#onetrust-banner-sdk .ot-close-icon,#onetrust-pc-sdk .ot-close-icon,#ot-sync-ntfy .ot-close-icon{background-size:contain;background-repeat:no-repeat;background-position:center;height:12px;width:12px}#onetrust-banner-sdk .powered-by-logo,#onetrust-banner-sdk .ot-pc-footer-logo a,#onetrust-pc-sdk .powered-by-logo,#onetrust-pc-sdk .ot-pc-footer-logo a,#ot-sync-ntfy .powered-by-logo,#ot-sync-ntfy .ot-pc-footer-logo a{background-size:contain;background-repeat:no-repeat;background-position:center;height:25px;width:152px;display:block;text-decoration:none;font-size:.75em}#onetrust-banner-sdk .powered-by-logo:hover,#onetrust-banner-sdk .ot-pc-footer-logo a:hover,#onetrust-pc-sdk .powered-by-logo:hover,#onetrust-pc-sdk .ot-pc-footer-logo a:hover,#ot-sync-ntfy .powered-by-logo:hover,#ot-sync-ntfy .ot-pc-footer-logo a:hover{color:#565656}#onetrust-banner-sdk h3 *,#onetrust-banner-sdk h4 *,#onetrust-banner-sdk h6 *,#onetrust-banner-sdk button *,#onetrust-banner-sdk a[data-parent-id] *,#onetrust-banner-sdk p[role=heading] *,#onetrust-pc-sdk h3 *,#onetrust-pc-sdk h4 *,#onetrust-pc-sdk h6 *,#onetrust-pc-sdk button *,#onetrust-pc-sdk a[data-parent-id] *,#onetrust-pc-sdk p[role=heading] *,#ot-sync-ntfy h3 *,#ot-sync-ntfy h4 *,#ot-sync-ntfy h6 *,#ot-sync-ntfy button *,#ot-sync-ntfy a[data-parent-id] *,#ot-sync-ntfy p[role=heading] *{font-size:inherit;font-weight:inherit;color:inherit}#onetrust-banner-sdk .ot-hide,#onetrust-pc-sdk .ot-hide,#ot-sync-ntfy .ot-hide{display:none !important}#onetrust-banner-sdk button.ot-link-btn:hover,#onetrust-pc-sdk button.ot-link-btn:hover,#ot-sync-ntfy button.ot-link-btn:hover{text-decoration:underline;opacity:1}#onetrust-pc-sdk .ot-sdk-row .ot-sdk-column{padding:0}#onetrust-pc-sdk .ot-sdk-container{padding-right:0}#onetrust-pc-sdk .ot-sdk-row{flex-direction:initial;width:100%}#onetrust-pc-sdk [type=checkbox]:checked,#onetrust-pc-sdk [type=checkbox]:not(:checked){pointer-events:initial}#onetrust-pc-sdk [type=checkbox]:disabled+label::before,#onetrust-pc-sdk [type=checkbox]:disabled+label:after,#onetrust-pc-sdk [type=checkbox]:disabled+label{pointer-events:none;opacity:.8}#onetrust-pc-sdk #vendor-list-content{transform:translate3d(0, 0, 0)}#onetrust-pc-sdk li input[type=checkbox]{z-index:1}#onetrust-pc-sdk li .ot-checkbox label{z-index:2}#onetrust-pc-sdk li .ot-checkbox input[type=checkbox]{height:auto;width:auto}#onetrust-pc-sdk li .host-title a,#onetrust-pc-sdk li .ot-host-name a,#onetrust-pc-sdk li .accordion-text,#onetrust-pc-sdk li .ot-acc-txt{z-index:2;position:relative}#onetrust-pc-sdk input{margin:3px .1ex}#onetrust-pc-sdk .pc-logo,#onetrust-pc-sdk .ot-pc-logo{height:60px;width:180px;background-position:center;background-size:contain;background-repeat:no-repeat;display:inline-flex;justify-content:center;align-items:center}#onetrust-pc-sdk .pc-logo img,#onetrust-pc-sdk .ot-pc-logo img{max-height:100%;max-width:100%}#onetrust-pc-sdk .pc-logo svg,#onetrust-pc-sdk .ot-pc-logo svg{height:60px;width:180px}#onetrust-pc-sdk #close-pc-btn-handler>svg{margin:auto;display:block;height:12px;width:12px}#onetrust-pc-sdk #ot-pc-desc svg,#onetrust-pc-sdk .ot-desc-cntr svg,#onetrust-pc-sdk .ot-cat-grp svg{height:13px;width:13px;margin-left:-7px;vertical-align:middle;margin-right:5px}#onetrust-pc-sdk .ot-host-hdr>a{display:inline-block}#onetrust-pc-sdk .screen-reader-only,#onetrust-pc-sdk .ot-scrn-rdr,.ot-sdk-cookie-policy .screen-reader-only,.ot-sdk-cookie-policy .ot-scrn-rdr{border:0;clip:rect(0 0 0 0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}#onetrust-pc-sdk.ot-fade-in,.onetrust-pc-dark-filter.ot-fade-in,#onetrust-banner-sdk.ot-fade-in,.ot-confirm-dialog-overlay.ot-fade-in{animation-name:onetrust-fade-in;animation-duration:400ms;animation-timing-function:ease-in-out}#onetrust-pc-sdk.ot-hide{display:none !important}.onetrust-pc-dark-filter.ot-hide{display:none !important}#ot-sdk-btn.ot-sdk-show-settings,#ot-sdk-btn.optanon-show-settings{color:#fff;background-color:#468254;height:auto;white-space:normal;word-wrap:break-word;padding:.8em 2em;font-size:.8em;line-height:1.2;cursor:pointer;-moz-transition:.1s ease;-o-transition:.1s ease;-webkit-transition:1s ease;transition:.1s ease}#ot-sdk-btn.ot-sdk-show-settings:hover,#ot-sdk-btn.optanon-show-settings:hover{color:#fff;background-color:#2c6415}#ot-sdk-btn.ot-sdk-show-settings:active,#ot-sdk-btn.optanon-show-settings:active{color:#fff;background-color:#2c6415;border:1px solid rgba(162,192,169,.5)}.onetrust-pc-dark-filter{background:rgba(0,0,0,.5);z-index:2147483646;width:100%;height:100%;overflow:hidden;position:fixed;top:0;bottom:0;left:0}@keyframes onetrust-fade-in{0%{opacity:0}100%{opacity:1}}.ot-cookie-label{text-decoration:underline}@media only screen and (min-width: 426px)and (max-width: 896px)and (orientation: landscape){#onetrust-pc-sdk p{font-size:.75em}}#onetrust-banner-sdk .banner-option-input:focus+label{outline:1px solid #000;outline-style:auto}.category-vendors-list-handler+a:focus,.category-vendors-list-handler+a:focus-visible{outline:2px solid #000}#onetrust-pc-sdk .ot-userid-title{margin-top:10px}#onetrust-pc-sdk .ot-userid-title>span,#onetrust-pc-sdk .ot-userid-timestamp>span{font-weight:700}#onetrust-pc-sdk .ot-userid-desc{font-style:italic}#onetrust-pc-sdk .ot-host-desc a{pointer-events:initial}#onetrust-pc-sdk .ot-ven-hdr>p a{position:relative;z-index:2;pointer-events:initial}#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info a,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info a{margin-right:8px}#onetrust-pc-sdk .ot-vnd-item svg.ot-ext-lnk{align-self:center}#onetrust-pc-sdk .ot-pc-footer-logo svg,#onetrust-pc-sdk .ot-pc-footer-logo img{width:136px;height:16px}#onetrust-pc-sdk .ot-pur-vdr-count{font-weight:400;font-size:.8em;padding-top:3px;display:block}#onetrust-pc-sdk p[role=heading] .ot-pur-vdr-count{font-weight:400 !important;font-size:.8em !important}#onetrust-banner-sdk #onetrust-policy .ot-optout-signal,#onetrust-pc-sdk #ot-pc-content .ot-optout-signal{border:1px solid #32ae88;border-radius:3px;padding:5px;margin-bottom:10px;background-color:#f9fffa;font-size:.85rem;line-height:2}#onetrust-banner-sdk #onetrust-policy .ot-optout-signal .ot-optout-icon,#onetrust-pc-sdk #ot-pc-content .ot-optout-signal .ot-optout-icon{display:inline;margin-right:5px}#onetrust-banner-sdk #onetrust-policy .ot-optout-signal svg,#onetrust-pc-sdk #ot-pc-content .ot-optout-signal svg{height:20px;width:30px}#onetrust-banner-sdk #onetrust-policy .ot-optout-signal svg.ot-source-sprite,#onetrust-pc-sdk #ot-pc-content .ot-optout-signal svg.ot-source-sprite{position:relative;bottom:-3px}#onetrust-banner-sdk #onetrust-policy .ot-optout-signal svg:not(.ot-source-sprite),#onetrust-pc-sdk #ot-pc-content .ot-optout-signal svg:not(.ot-source-sprite){transform:scale(0.5)}#onetrust-banner-sdk #onetrust-policy .ot-optout-signal svg:not(.ot-source-sprite) path,#onetrust-pc-sdk #ot-pc-content .ot-optout-signal svg:not(.ot-source-sprite) path{fill:#32ae88}#onetrust-consent-sdk .ot-general-modal{overflow:hidden;position:fixed;margin:0 auto;top:50%;left:50%;width:40%;padding:1.5rem;max-width:575px;min-width:575px;z-index:2147483647;border-radius:2.5px;transform:translate(-50%, -50%)}#onetrust-consent-sdk .ot-signature-health-group{margin-top:1rem;padding-left:1.25rem;padding-right:1.25rem;margin-bottom:.625rem;width:calc(100% - 2.5rem)}#onetrust-consent-sdk .ot-signature-health-group .ot-signature-health-form{gap:.5rem}#onetrust-consent-sdk .ot-signature-health .ot-signature-health-form{width:70%;gap:.35rem}#onetrust-consent-sdk .ot-signature-health .ot-signature-input{height:38px;padding:6px 10px;background-color:#fff;border:1px solid #d1d1d1;border-radius:4px;box-shadow:none;box-sizing:border-box}#onetrust-consent-sdk .ot-signature-health .ot-signature-subtitle{font-size:1.125rem}#onetrust-consent-sdk .ot-signature-health .ot-signature-group-title{font-size:1.25rem;font-weight:bold}#onetrust-consent-sdk .ot-signature-health,#onetrust-consent-sdk .ot-signature-health-group{display:flex;flex-direction:column;gap:1rem}#onetrust-consent-sdk .ot-signature-health .ot-signature-cont,#onetrust-consent-sdk .ot-signature-health-group .ot-signature-cont{display:flex;flex-direction:column;gap:.25rem}#onetrust-consent-sdk .ot-signature-health .ot-signature-paragraph,#onetrust-consent-sdk .ot-signature-health-group .ot-signature-paragraph{margin:0;line-height:20px;font-size:max(14px,.875rem)}#onetrust-consent-sdk .ot-signature-health .ot-health-signature-error,#onetrust-consent-sdk .ot-signature-health-group .ot-health-signature-error{color:#4d4d4d;font-size:min(12px,.75rem)}#onetrust-consent-sdk .ot-signature-health .ot-signature-buttons-cont,#onetrust-consent-sdk .ot-signature-health-group .ot-signature-buttons-cont{margin-top:max(.75rem,2%);gap:1rem;display:flex;justify-content:flex-end}#onetrust-consent-sdk .ot-signature-health .ot-signature-button,#onetrust-consent-sdk .ot-signature-health-group .ot-signature-button{flex:1;height:auto;color:#fff;cursor:pointer;line-height:1.2;min-width:125px;font-weight:600;font-size:.813em;border-radius:2px;padding:12px 10px;white-space:normal;word-wrap:break-word;word-break:break-word;background-color:#68b631;border:2px solid #68b631}#onetrust-consent-sdk .ot-signature-health .ot-signature-button.reject,#onetrust-consent-sdk .ot-signature-health-group .ot-signature-button.reject{background-color:#fff}#onetrust-consent-sdk .ot-input-field-cont{display:flex;flex-direction:column;gap:.5rem}#onetrust-consent-sdk .ot-input-field-cont .ot-signature-input{width:65%}#onetrust-consent-sdk .ot-signature-health-form{display:flex;flex-direction:column}#onetrust-consent-sdk .ot-signature-health-form .ot-signature-label{margin-bottom:0;line-height:20px;font-size:max(14px,.875rem)}#onetrust-consent-sdk #onetrust-sprite-svg{display:none}@media only screen and (max-width: 600px){#onetrust-consent-sdk .ot-general-modal{min-width:100%}#onetrust-consent-sdk .ot-signature-health .ot-signature-health-form{width:100%}#onetrust-consent-sdk .ot-input-field-cont .ot-signature-input{width:100%}}#onetrust-banner-sdk,#onetrust-pc-sdk,#ot-sdk-cookie-policy,#ot-sync-ntfy{font-size:16px}#onetrust-banner-sdk *,#onetrust-banner-sdk ::after,#onetrust-banner-sdk ::before,#onetrust-pc-sdk *,#onetrust-pc-sdk ::after,#onetrust-pc-sdk ::before,#ot-sdk-cookie-policy *,#ot-sdk-cookie-policy ::after,#ot-sdk-cookie-policy ::before,#ot-sync-ntfy *,#ot-sync-ntfy ::after,#ot-sync-ntfy ::before{-webkit-box-sizing:content-box;-moz-box-sizing:content-box;box-sizing:content-box}#onetrust-banner-sdk div,#onetrust-banner-sdk span,#onetrust-banner-sdk h1,#onetrust-banner-sdk h2,#onetrust-banner-sdk h3,#onetrust-banner-sdk h4,#onetrust-banner-sdk h5,#onetrust-banner-sdk h6,#onetrust-banner-sdk p,#onetrust-banner-sdk img,#onetrust-banner-sdk svg,#onetrust-banner-sdk button,#onetrust-banner-sdk section,#onetrust-banner-sdk a,#onetrust-banner-sdk label,#onetrust-banner-sdk input,#onetrust-banner-sdk ul,#onetrust-banner-sdk li,#onetrust-banner-sdk nav,#onetrust-banner-sdk table,#onetrust-banner-sdk thead,#onetrust-banner-sdk tr,#onetrust-banner-sdk td,#onetrust-banner-sdk tbody,#onetrust-banner-sdk .ot-main-content,#onetrust-banner-sdk .ot-toggle,#onetrust-banner-sdk #ot-content,#onetrust-banner-sdk #ot-pc-content,#onetrust-banner-sdk .checkbox,#onetrust-pc-sdk div,#onetrust-pc-sdk span,#onetrust-pc-sdk h1,#onetrust-pc-sdk h2,#onetrust-pc-sdk h3,#onetrust-pc-sdk h4,#onetrust-pc-sdk h5,#onetrust-pc-sdk h6,#onetrust-pc-sdk p,#onetrust-pc-sdk img,#onetrust-pc-sdk svg,#onetrust-pc-sdk button,#onetrust-pc-sdk section,#onetrust-pc-sdk a,#onetrust-pc-sdk label,#onetrust-pc-sdk input,#onetrust-pc-sdk ul,#onetrust-pc-sdk li,#onetrust-pc-sdk nav,#onetrust-pc-sdk table,#onetrust-pc-sdk thead,#onetrust-pc-sdk tr,#onetrust-pc-sdk td,#onetrust-pc-sdk tbody,#onetrust-pc-sdk .ot-main-content,#onetrust-pc-sdk .ot-toggle,#onetrust-pc-sdk #ot-content,#onetrust-pc-sdk #ot-pc-content,#onetrust-pc-sdk .checkbox,#ot-sdk-cookie-policy div,#ot-sdk-cookie-policy span,#ot-sdk-cookie-policy h1,#ot-sdk-cookie-policy h2,#ot-sdk-cookie-policy h3,#ot-sdk-cookie-policy h4,#ot-sdk-cookie-policy h5,#ot-sdk-cookie-policy h6,#ot-sdk-cookie-policy p,#ot-sdk-cookie-policy img,#ot-sdk-cookie-policy svg,#ot-sdk-cookie-policy button,#ot-sdk-cookie-policy section,#ot-sdk-cookie-policy a,#ot-sdk-cookie-policy label,#ot-sdk-cookie-policy input,#ot-sdk-cookie-policy ul,#ot-sdk-cookie-policy li,#ot-sdk-cookie-policy nav,#ot-sdk-cookie-policy table,#ot-sdk-cookie-policy thead,#ot-sdk-cookie-policy tr,#ot-sdk-cookie-policy td,#ot-sdk-cookie-policy tbody,#ot-sdk-cookie-policy .ot-main-content,#ot-sdk-cookie-policy .ot-toggle,#ot-sdk-cookie-policy #ot-content,#ot-sdk-cookie-policy #ot-pc-content,#ot-sdk-cookie-policy .checkbox,#ot-sync-ntfy div,#ot-sync-ntfy span,#ot-sync-ntfy h1,#ot-sync-ntfy h2,#ot-sync-ntfy h3,#ot-sync-ntfy h4,#ot-sync-ntfy h5,#ot-sync-ntfy h6,#ot-sync-ntfy p,#ot-sync-ntfy img,#ot-sync-ntfy svg,#ot-sync-ntfy button,#ot-sync-ntfy section,#ot-sync-ntfy a,#ot-sync-ntfy label,#ot-sync-ntfy input,#ot-sync-ntfy ul,#ot-sync-ntfy li,#ot-sync-ntfy nav,#ot-sync-ntfy table,#ot-sync-ntfy thead,#ot-sync-ntfy tr,#ot-sync-ntfy td,#ot-sync-ntfy tbody,#ot-sync-ntfy .ot-main-content,#ot-sync-ntfy .ot-toggle,#ot-sync-ntfy #ot-content,#ot-sync-ntfy #ot-pc-content,#ot-sync-ntfy .checkbox{font-family:inherit;font-weight:normal;-webkit-font-smoothing:auto;letter-spacing:normal;line-height:normal;padding:0;margin:0;height:auto;min-height:0;max-height:none;width:auto;min-width:0;max-width:none;border-radius:0;border:none;clear:none;float:none;position:static;bottom:auto;left:auto;right:auto;top:auto;text-align:left;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;white-space:normal;background:none;overflow:visible;vertical-align:baseline;visibility:visible;z-index:auto;box-shadow:none}#onetrust-banner-sdk img,#onetrust-pc-sdk img,#ot-sdk-cookie-policy img,#ot-sync-ntfy img{overflow:hidden !important}#onetrust-banner-sdk label:before,#onetrust-banner-sdk label:after,#onetrust-banner-sdk .checkbox:after,#onetrust-banner-sdk .checkbox:before,#onetrust-pc-sdk label:before,#onetrust-pc-sdk label:after,#onetrust-pc-sdk .checkbox:after,#onetrust-pc-sdk .checkbox:before,#ot-sdk-cookie-policy label:before,#ot-sdk-cookie-policy label:after,#ot-sdk-cookie-policy .checkbox:after,#ot-sdk-cookie-policy .checkbox:before,#ot-sync-ntfy label:before,#ot-sync-ntfy label:after,#ot-sync-ntfy .checkbox:after,#ot-sync-ntfy .checkbox:before{content:"";content:none}#onetrust-banner-sdk .ot-sdk-container,#onetrust-pc-sdk .ot-sdk-container,#ot-sdk-cookie-policy .ot-sdk-container{position:relative;width:100%;max-width:100%;margin:0 auto;padding:0 20px;box-sizing:border-box}#onetrust-banner-sdk .ot-sdk-column,#onetrust-banner-sdk .ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-column,#onetrust-pc-sdk .ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-column,#ot-sdk-cookie-policy .ot-sdk-columns{width:100%;float:left;box-sizing:border-box;padding:0;display:initial}@media(min-width: 400px){#onetrust-banner-sdk .ot-sdk-container,#onetrust-pc-sdk .ot-sdk-container,#ot-sdk-cookie-policy .ot-sdk-container{width:90%;padding:0}}@media(min-width: 550px){#onetrust-banner-sdk .ot-sdk-container,#onetrust-pc-sdk .ot-sdk-container,#ot-sdk-cookie-policy .ot-sdk-container{width:100%}#onetrust-banner-sdk .ot-sdk-column,#onetrust-banner-sdk .ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-column,#onetrust-pc-sdk .ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-column,#ot-sdk-cookie-policy .ot-sdk-columns{margin-left:4%}#onetrust-banner-sdk .ot-sdk-column:first-child,#onetrust-banner-sdk .ot-sdk-columns:first-child,#onetrust-pc-sdk .ot-sdk-column:first-child,#onetrust-pc-sdk .ot-sdk-columns:first-child,#ot-sdk-cookie-policy .ot-sdk-column:first-child,#ot-sdk-cookie-policy .ot-sdk-columns:first-child{margin-left:0}#onetrust-banner-sdk .ot-sdk-two.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-two.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-two.ot-sdk-columns{width:13.3333333333%}#onetrust-banner-sdk .ot-sdk-three.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-three.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-three.ot-sdk-columns{width:22%}#onetrust-banner-sdk .ot-sdk-four.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-four.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-four.ot-sdk-columns{width:30.6666666667%}#onetrust-banner-sdk .ot-sdk-eight.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-eight.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-eight.ot-sdk-columns{width:65.3333333333%}#onetrust-banner-sdk .ot-sdk-nine.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-nine.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-nine.ot-sdk-columns{width:74%}#onetrust-banner-sdk .ot-sdk-ten.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-ten.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-ten.ot-sdk-columns{width:82.6666666667%}#onetrust-banner-sdk .ot-sdk-eleven.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-eleven.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-eleven.ot-sdk-columns{width:91.3333333333%}#onetrust-banner-sdk .ot-sdk-twelve.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-twelve.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-twelve.ot-sdk-columns{width:100%;margin-left:0}}#onetrust-banner-sdk h1,#onetrust-banner-sdk h2,#onetrust-banner-sdk h3,#onetrust-banner-sdk h4,#onetrust-banner-sdk h5,#onetrust-banner-sdk h6,#onetrust-banner-sdk p[role=heading],#onetrust-pc-sdk h1,#onetrust-pc-sdk h2,#onetrust-pc-sdk h3,#onetrust-pc-sdk h4,#onetrust-pc-sdk h5,#onetrust-pc-sdk h6,#onetrust-pc-sdk p[role=heading],#ot-sdk-cookie-policy h1,#ot-sdk-cookie-policy h2,#ot-sdk-cookie-policy h3,#ot-sdk-cookie-policy h4,#ot-sdk-cookie-policy h5,#ot-sdk-cookie-policy h6,#ot-sdk-cookie-policy p[role=heading]{margin-top:0;font-weight:600;font-family:inherit}#onetrust-banner-sdk h1,#onetrust-pc-sdk h1,#ot-sdk-cookie-policy h1{font-size:1.5rem;line-height:1.2}#onetrust-banner-sdk h2,#onetrust-pc-sdk h2,#ot-sdk-cookie-policy h2{font-size:1.5rem;line-height:1.25}#onetrust-banner-sdk h3,#onetrust-pc-sdk h3,#ot-sdk-cookie-policy h3{font-size:1.5rem;line-height:1.3}#onetrust-banner-sdk h4,#onetrust-pc-sdk h4,#ot-sdk-cookie-policy h4{font-size:1.5rem;line-height:1.35}#onetrust-banner-sdk h5,#onetrust-pc-sdk h5,#ot-sdk-cookie-policy h5{font-size:1.5rem;line-height:1.5}#onetrust-banner-sdk h6,#onetrust-pc-sdk h6,#ot-sdk-cookie-policy h6{font-size:1.5rem;line-height:1.6}@media(min-width: 550px){#onetrust-banner-sdk h1,#onetrust-pc-sdk h1,#ot-sdk-cookie-policy h1{font-size:1.5rem}#onetrust-banner-sdk h2,#onetrust-pc-sdk h2,#ot-sdk-cookie-policy h2{font-size:1.5rem}#onetrust-banner-sdk h3,#onetrust-pc-sdk h3,#ot-sdk-cookie-policy h3{font-size:1.5rem}#onetrust-banner-sdk h4,#onetrust-pc-sdk h4,#ot-sdk-cookie-policy h4{font-size:1.5rem}#onetrust-banner-sdk h5,#onetrust-pc-sdk h5,#ot-sdk-cookie-policy h5{font-size:1.5rem}#onetrust-banner-sdk h6,#onetrust-pc-sdk h6,#ot-sdk-cookie-policy h6{font-size:1.5rem}}#onetrust-banner-sdk p:not([role=heading]),#onetrust-pc-sdk p:not([role=heading]),#ot-sdk-cookie-policy p:not([role=heading]){margin:0 0 1em 0;font-family:inherit;line-height:normal}#onetrust-banner-sdk a,#onetrust-pc-sdk a,#ot-sdk-cookie-policy a{color:#565656;text-decoration:underline}#onetrust-banner-sdk a:hover,#onetrust-pc-sdk a:hover,#ot-sdk-cookie-policy a:hover{color:#565656;text-decoration:none}#onetrust-banner-sdk .ot-sdk-button,#onetrust-banner-sdk button,#onetrust-pc-sdk .ot-sdk-button,#onetrust-pc-sdk button,#ot-sdk-cookie-policy .ot-sdk-button,#ot-sdk-cookie-policy button{margin-bottom:1rem;font-family:inherit}#onetrust-banner-sdk .ot-sdk-button,#onetrust-banner-sdk button,#onetrust-pc-sdk .ot-sdk-button,#onetrust-pc-sdk button,#ot-sdk-cookie-policy .ot-sdk-button,#ot-sdk-cookie-policy button{display:inline-block;height:38px;padding:0 30px;color:#555;text-align:center;font-size:.9em;font-weight:400;line-height:38px;letter-spacing:.01em;text-decoration:none;white-space:nowrap;background-color:rgba(0,0,0,0);border-radius:2px;border:1px solid #bbb;cursor:pointer;box-sizing:border-box}#onetrust-banner-sdk .ot-sdk-button:hover,#onetrust-banner-sdk :not(.ot-leg-btn-container):not(.ot-confirm-dialog-buttons)>button:not(.ot-link-btn):hover,#onetrust-banner-sdk :not(.ot-leg-btn-container):not(.ot-confirm-dialog-buttons)>button:not(.ot-link-btn):focus,#onetrust-pc-sdk .ot-sdk-button:hover,#onetrust-pc-sdk :not(.ot-leg-btn-container):not(.ot-confirm-dialog-buttons)>button:not(.ot-link-btn):hover,#onetrust-pc-sdk :not(.ot-leg-btn-container):not(.ot-confirm-dialog-buttons)>button:not(.ot-link-btn):focus,#ot-sdk-cookie-policy .ot-sdk-button:hover,#ot-sdk-cookie-policy :not(.ot-leg-btn-container):not(.ot-confirm-dialog-buttons)>button:not(.ot-link-btn):hover,#ot-sdk-cookie-policy :not(.ot-leg-btn-container):not(.ot-confirm-dialog-buttons)>button:not(.ot-link-btn):focus{color:#333;border-color:#888;opacity:.9}#onetrust-banner-sdk .ot-sdk-button:focus,#onetrust-banner-sdk :not(.ot-leg-btn-container)>button:focus,#onetrust-pc-sdk .ot-sdk-button:focus,#onetrust-pc-sdk :not(.ot-leg-btn-container)>button:focus,#ot-sdk-cookie-policy .ot-sdk-button:focus,#ot-sdk-cookie-policy :not(.ot-leg-btn-container)>button:focus{outline:2px solid #000}#onetrust-banner-sdk .ot-sdk-button.ot-sdk-button-primary,#onetrust-banner-sdk button.ot-sdk-button-primary,#onetrust-banner-sdk input[type=submit].ot-sdk-button-primary,#onetrust-banner-sdk input[type=reset].ot-sdk-button-primary,#onetrust-banner-sdk input[type=button].ot-sdk-button-primary,#onetrust-pc-sdk .ot-sdk-button.ot-sdk-button-primary,#onetrust-pc-sdk button.ot-sdk-button-primary,#onetrust-pc-sdk input[type=submit].ot-sdk-button-primary,#onetrust-pc-sdk input[type=reset].ot-sdk-button-primary,#onetrust-pc-sdk input[type=button].ot-sdk-button-primary,#ot-sdk-cookie-policy .ot-sdk-button.ot-sdk-button-primary,#ot-sdk-cookie-policy button.ot-sdk-button-primary,#ot-sdk-cookie-policy input[type=submit].ot-sdk-button-primary,#ot-sdk-cookie-policy input[type=reset].ot-sdk-button-primary,#ot-sdk-cookie-policy input[type=button].ot-sdk-button-primary{color:#fff;background-color:#33c3f0;border-color:#33c3f0}#onetrust-banner-sdk .ot-sdk-button.ot-sdk-button-primary:hover,#onetrust-banner-sdk button.ot-sdk-button-primary:hover,#onetrust-banner-sdk input[type=submit].ot-sdk-button-primary:hover,#onetrust-banner-sdk input[type=reset].ot-sdk-button-primary:hover,#onetrust-banner-sdk input[type=button].ot-sdk-button-primary:hover,#onetrust-banner-sdk .ot-sdk-button.ot-sdk-button-primary:focus,#onetrust-banner-sdk button.ot-sdk-button-primary:focus,#onetrust-banner-sdk input[type=submit].ot-sdk-button-primary:focus,#onetrust-banner-sdk input[type=reset].ot-sdk-button-primary:focus,#onetrust-banner-sdk input[type=button].ot-sdk-button-primary:focus,#onetrust-pc-sdk .ot-sdk-button.ot-sdk-button-primary:hover,#onetrust-pc-sdk button.ot-sdk-button-primary:hover,#onetrust-pc-sdk input[type=submit].ot-sdk-button-primary:hover,#onetrust-pc-sdk input[type=reset].ot-sdk-button-primary:hover,#onetrust-pc-sdk input[type=button].ot-sdk-button-primary:hover,#onetrust-pc-sdk .ot-sdk-button.ot-sdk-button-primary:focus,#onetrust-pc-sdk button.ot-sdk-button-primary:focus,#onetrust-pc-sdk input[type=submit].ot-sdk-button-primary:focus,#onetrust-pc-sdk input[type=reset].ot-sdk-button-primary:focus,#onetrust-pc-sdk input[type=button].ot-sdk-button-primary:focus,#ot-sdk-cookie-policy .ot-sdk-button.ot-sdk-button-primary:hover,#ot-sdk-cookie-policy button.ot-sdk-button-primary:hover,#ot-sdk-cookie-policy input[type=submit].ot-sdk-button-primary:hover,#ot-sdk-cookie-policy input[type=reset].ot-sdk-button-primary:hover,#ot-sdk-cookie-policy input[type=button].ot-sdk-button-primary:hover,#ot-sdk-cookie-policy .ot-sdk-button.ot-sdk-button-primary:focus,#ot-sdk-cookie-policy button.ot-sdk-button-primary:focus,#ot-sdk-cookie-policy input[type=submit].ot-sdk-button-primary:focus,#ot-sdk-cookie-policy input[type=reset].ot-sdk-button-primary:focus,#ot-sdk-cookie-policy input[type=button].ot-sdk-button-primary:focus{color:#fff;background-color:#1eaedb;border-color:#1eaedb}#onetrust-banner-sdk input[type=text],#onetrust-pc-sdk input[type=text],#ot-sdk-cookie-policy input[type=text]{height:38px;padding:6px 10px;background-color:#fff;border:1px solid #707070;border-radius:4px;box-shadow:none;box-sizing:border-box}#onetrust-banner-sdk input[type=text],#onetrust-pc-sdk input[type=text],#ot-sdk-cookie-policy input[type=text]{-webkit-appearance:none;-moz-appearance:none;appearance:none}#onetrust-banner-sdk input[type=text]:focus,#onetrust-pc-sdk input[type=text]:focus,#ot-sdk-cookie-policy input[type=text]:focus{border:1px solid #000;outline:0}#onetrust-banner-sdk label,#onetrust-pc-sdk label,#ot-sdk-cookie-policy label{display:block;margin-bottom:.5rem;font-weight:600}#onetrust-banner-sdk input[type=checkbox],#onetrust-pc-sdk input[type=checkbox],#ot-sdk-cookie-policy input[type=checkbox]{display:inline}#onetrust-banner-sdk ul,#onetrust-pc-sdk ul,#ot-sdk-cookie-policy ul{list-style:circle inside}#onetrust-banner-sdk ul,#onetrust-pc-sdk ul,#ot-sdk-cookie-policy ul{padding-left:0;margin-top:0}#onetrust-banner-sdk ul ul,#onetrust-pc-sdk ul ul,#ot-sdk-cookie-policy ul ul{margin:1.5rem 0 1.5rem 3rem;font-size:90%}#onetrust-banner-sdk li,#onetrust-pc-sdk li,#ot-sdk-cookie-policy li{margin-bottom:1rem}#onetrust-banner-sdk th,#onetrust-banner-sdk td,#onetrust-pc-sdk th,#onetrust-pc-sdk td,#ot-sdk-cookie-policy th,#ot-sdk-cookie-policy td{padding:12px 15px;text-align:left;border-bottom:1px solid #e1e1e1}#onetrust-banner-sdk button,#onetrust-pc-sdk button,#ot-sdk-cookie-policy button{margin-bottom:1rem;font-family:inherit}#onetrust-banner-sdk .ot-sdk-container:after,#onetrust-banner-sdk .ot-sdk-row:after,#onetrust-pc-sdk .ot-sdk-container:after,#onetrust-pc-sdk .ot-sdk-row:after,#ot-sdk-cookie-policy .ot-sdk-container:after,#ot-sdk-cookie-policy .ot-sdk-row:after{content:"";display:table;clear:both}#onetrust-banner-sdk .ot-sdk-row,#onetrust-pc-sdk .ot-sdk-row,#ot-sdk-cookie-policy .ot-sdk-row{margin:0;max-width:none;display:block}#onetrust-banner-sdk{box-shadow:0 0 18px rgba(0,0,0,.2)}#onetrust-banner-sdk.otCenterRounded{z-index:2147483645;top:10%;position:fixed;right:0;background-color:#fff;width:60%;max-width:650px;border-radius:2.5px;left:1em;margin:0 auto;font-size:14px;max-height:90%;overflow-x:hidden;overflow-y:auto}#onetrust-banner-sdk.otRelFont{font-size:.875rem}#onetrust-banner-sdk::-webkit-scrollbar{width:11px}#onetrust-banner-sdk::-webkit-scrollbar-thumb{border-radius:10px;background:#c1c1c1}#onetrust-banner-sdk{scrollbar-arrow-color:#c1c1c1;scrollbar-darkshadow-color:#c1c1c1;scrollbar-face-color:#c1c1c1;scrollbar-shadow-color:#c1c1c1}#onetrust-banner-sdk h3,#onetrust-banner-sdk p{color:dimgray}#onetrust-banner-sdk #onetrust-policy{margin-top:40px}#onetrust-banner-sdk #onetrust-policy-title{float:left;text-align:left;font-size:1em;line-height:1.4;margin-bottom:0;padding:0 0 10px 30px;width:calc(100% - 90px)}#onetrust-banner-sdk #onetrust-policy-text,#onetrust-banner-sdk .ot-b-addl-desc,#onetrust-banner-sdk .ot-gv-list-handler{clear:both;float:left;margin:0 30px 10px 30px;font-size:.813em;line-height:1.5}#onetrust-banner-sdk #onetrust-policy-text *,#onetrust-banner-sdk .ot-b-addl-desc *,#onetrust-banner-sdk .ot-gv-list-handler *{line-height:inherit;font-size:inherit;margin:0}#onetrust-banner-sdk .ot-optout-signal{margin:0 1.875rem .625rem 1.875rem}#onetrust-banner-sdk .ot-gv-list-handler{padding:0;border:0;height:auto;width:auto}#onetrust-banner-sdk .ot-b-addl-desc{display:block}#onetrust-banner-sdk #onetrust-button-group-parent{padding:15px 30px;text-align:center}#onetrust-banner-sdk #onetrust-button-group-parent:not(.has-reject-all-button) #onetrust-button-group{text-align:right}#onetrust-banner-sdk #onetrust-button-group{text-align:center;display:inline-block;width:100%}#onetrust-banner-sdk #onetrust-button-group.ot-button-order-container{display:flex;flex-wrap:wrap;justify-content:flex-end}#onetrust-banner-sdk #onetrust-button-group.ot-button-order-container *[class*=ot-button-order-]:nth-of-type(1){margin-right:auto !important}#onetrust-banner-sdk #onetrust-button-group.ot-button-order-container *[class*=ot-button-order-]:nth-of-type(2){margin-right:1em !important}#onetrust-banner-sdk #onetrust-button-group.ot-button-order-container *[class*=ot-button-order-]:last-of-type{margin-right:0 !important}#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{margin-right:1em}#onetrust-banner-sdk #onetrust-pc-btn-handler{border:1px solid #6cc04a;max-width:45%}#onetrust-banner-sdk .banner-actions-container{float:right;width:50%}#onetrust-banner-sdk #onetrust-pc-btn-handler.cookie-setting-link{background-color:#fff;border:none;color:#6cc04a;text-decoration:underline;padding-left:0;padding-right:0}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{background-color:#6cc04a;color:#fff;border-color:#6cc04a;min-width:135px;padding:12px 10px;letter-spacing:.05em;line-height:1.4;font-size:.813em;font-weight:600;height:auto;white-space:normal;word-break:break-word;word-wrap:break-word}#onetrust-banner-sdk .has-reject-all-button #onetrust-pc-btn-handler{float:left;max-width:calc(40% - 18px)}#onetrust-banner-sdk .has-reject-all-button #onetrust-pc-btn-handler.cookie-setting-link{text-align:left;margin-right:0}#onetrust-banner-sdk .has-reject-all-button .banner-actions-container{max-width:60%;width:auto}#onetrust-banner-sdk .ot-close-icon{width:44px;height:44px;background-size:12px;margin:-18px -18px 0 0;border:none;display:inline-block;padding:0}#onetrust-banner-sdk #onetrust-close-btn-container{position:absolute;right:24px;top:20px}#onetrust-banner-sdk .banner_logo{display:none}#onetrust-banner-sdk.ot-bnr-w-logo #onetrust-policy{margin-top:10px}#onetrust-banner-sdk.ot-bnr-w-logo .ot-bnr-logo{margin:4px 25px}#onetrust-banner-sdk #banner-options{float:left;padding:0 30px;width:calc(100% - 90px)}#onetrust-banner-sdk .banner-option{margin-bottom:10px}#onetrust-banner-sdk .banner-option-input{cursor:pointer;width:auto;height:auto;border:none;padding:0;padding-right:3px;margin:0 0 6px;font-size:.82em;line-height:1.4}#onetrust-banner-sdk .banner-option-input *{pointer-events:none;font-size:inherit;line-height:inherit}#onetrust-banner-sdk .banner-option-input[aria-expanded=true] .ot-arrow-container{transform:rotate(90deg)}#onetrust-banner-sdk .banner-option-input[aria-expanded=true]~.banner-option-details{height:auto;display:block}#onetrust-banner-sdk .banner-option-header{cursor:pointer;display:inline-block}#onetrust-banner-sdk .banner-option-header :first-child{color:dimgray;font-weight:bold;float:left}#onetrust-banner-sdk .ot-arrow-container,#onetrust-banner-sdk .banner-option-details{transition:all 300ms ease-in 0s;-webkit-transition:all 300ms ease-in 0s;-moz-transition:all 300ms ease-in 0s;-o-transition:all 300ms ease-in 0s}#onetrust-banner-sdk .ot-arrow-container{display:inline-block;border-top:6px solid rgba(0,0,0,0);border-bottom:6px solid rgba(0,0,0,0);border-left:6px solid dimgray;margin-left:10px;vertical-align:middle}#onetrust-banner-sdk .banner-option-details{display:none;font-size:.83em;line-height:1.5;height:0px;padding:10px 10px 5px 10px}#onetrust-banner-sdk .banner-option-details *{font-size:inherit;line-height:inherit;color:dimgray}#onetrust-banner-sdk .ot-dpd-container{float:left;margin:0 30px 10px 30px}#onetrust-banner-sdk .ot-dpd-title{font-weight:bold;padding-bottom:10px}#onetrust-banner-sdk .ot-dpd-title{font-size:1em;line-height:1.4}#onetrust-banner-sdk .ot-dpd-desc{font-size:.813em;line-height:1.5;margin-bottom:0}#onetrust-banner-sdk .ot-dpd-desc *{margin:0}#onetrust-banner-sdk .onetrust-vendors-list-handler{display:block;margin-left:0px;margin-top:5px;padding:0;margin-bottom:0;border:0;line-height:normal;height:auto;width:auto}#onetrust-banner-sdk :not(.ot-dpd-desc)>.ot-b-addl-desc{float:left;margin:0 30px 10px 30px}#onetrust-banner-sdk .ot-dpd-desc>.ot-b-addl-desc{margin-top:10px;margin-bottom:10px;font-size:1em;line-height:1.5;float:none}#onetrust-banner-sdk #onetrust-policy-text a{font-weight:bold}#onetrust-banner-sdk.ot-close-btn-link #onetrust-close-btn-container{top:15px;transform:none;right:15px}#onetrust-banner-sdk.ot-close-btn-link #onetrust-close-btn-container button{padding:0;white-space:pre-wrap;border:none;height:auto;line-height:1.5;text-decoration:underline;font-size:.75em}#onetrust-banner-sdk.ot-close-btn-link.ot-wo-title #onetrust-group-container{margin-top:20px}@media only screen and (max-width: 425px){#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{width:100%;margin-bottom:10px}#onetrust-banner-sdk #onetrust-pc-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler{margin-right:0}#onetrust-banner-sdk .has-reject-all-button #onetrust-pc-btn-handler.cookie-setting-link{text-align:center}#onetrust-banner-sdk .banner-actions-container,#onetrust-banner-sdk #onetrust-pc-btn-handler{width:100%;max-width:none}#onetrust-banner-sdk.otCenterRounded{left:0;width:95%;top:50%;transform:translateY(-50%);-webkit-transform:translateY(-50%)}}@media only screen and (max-width: 600px){#onetrust-banner-sdk .ot-sdk-container{width:auto;padding:0}#onetrust-banner-sdk #onetrust-policy-title{padding:0 22px 10px 22px}#onetrust-banner-sdk #onetrust-policy-text,#onetrust-banner-sdk :not(.ot-dpd-desc)>.ot-b-addl-desc,#onetrust-banner-sdk .ot-dpd-container{margin:0 22px 10px 22px;width:calc(100% - 44px)}#onetrust-banner-sdk #onetrust-button-group-parent{padding:15px 22px}#onetrust-banner-sdk #banner-options{padding:0 22px;width:calc(100% - 44px)}#onetrust-banner-sdk .banner-option{margin-bottom:6px}#onetrust-banner-sdk .has-reject-all-button #onetrust-pc-btn-handler{float:none;max-width:100%}#onetrust-banner-sdk .has-reject-all-button .banner-actions-container{width:100%;text-align:center;max-width:100%}#onetrust-banner-sdk.ot-close-btn-link #onetrust-group-container{margin-top:20px}#onetrust-banner-sdk #onetrust-button-group.ot-button-order-container *[class*=ot-button-order-]:nth-of-type(1),#onetrust-banner-sdk #onetrust-button-group.ot-button-order-container *[class*=ot-button-order-]:nth-of-type(2),#onetrust-banner-sdk #onetrust-button-group.ot-button-order-container *[class*=ot-button-order-]:last-of-type{flex-basis:100%;margin-right:0 !important}}@media only screen and (min-width: 426px)and (max-width: 896px){#onetrust-banner-sdk.otCenterRounded{left:0;top:15%;transform:translateY(-13%);-webkit-transform:translateY(-13%);max-width:600px;width:95%}}
        #onetrust-consent-sdk #onetrust-banner-sdk {background-color: #FFFFFF;}
            #onetrust-consent-sdk #onetrust-policy-title,
                    #onetrust-consent-sdk #onetrust-policy-text,
                    #onetrust-consent-sdk .ot-b-addl-desc,
                    #onetrust-consent-sdk .ot-dpd-desc,
                    #onetrust-consent-sdk .ot-dpd-title,
                    #onetrust-consent-sdk #onetrust-policy-text *:not(.onetrust-vendors-list-handler),
                    #onetrust-consent-sdk .ot-dpd-desc *:not(.onetrust-vendors-list-handler),
                    #onetrust-consent-sdk #onetrust-banner-sdk #banner-options *,
                    #onetrust-banner-sdk .ot-cat-header,
                    #onetrust-banner-sdk .ot-optout-signal
                    {
                        color: #1A1A1A;
                    }
            #onetrust-consent-sdk #onetrust-banner-sdk .banner-option-details {
                    background-color: #E9E9E9;}
             #onetrust-consent-sdk #onetrust-banner-sdk a[href],
                    #onetrust-consent-sdk #onetrust-banner-sdk a[href] font,
                    #onetrust-consent-sdk #onetrust-banner-sdk .ot-link-btn
                        {
                            color: #76B900;
                        }#onetrust-consent-sdk #onetrust-accept-btn-handler,
                         #onetrust-banner-sdk #onetrust-reject-all-handler,
                         #onetrust-banner-sdk #ot-dialog-confirm-handler {
                            background-color: #76B900;border-color: #76B900;
                color: #000000;
            }
            #onetrust-consent-sdk #onetrust-banner-sdk *:focus,
            #onetrust-consent-sdk #onetrust-banner-sdk:focus {
               outline-color: #1A1A1A;
               outline-width: 1px;
            }
            #onetrust-consent-sdk #onetrust-pc-btn-handler,
            #onetrust-consent-sdk #onetrust-pc-btn-handler.cookie-setting-link,
            #onetrust-consent-sdk #ot-dialog-cancel-handler {
                color: #000000; border-color: #000000;
                background-color:
                #76B900;
            }#onetrust-banner-sdk #onetrust-policy-text a.ot-cookie-policy-link,
                         #onetrust-banner-sdk #onetrust-policy-text a.ot-imprint-link {
                    margin-left: 5px;
                }/** NV Extra Cautious Banner styles start **/
html[lang="de-de"] #onetrust-consent-sdk #onetrust-banner-sdk,
html[lang="de-at"] #onetrust-consent-sdk #onetrust-banner-sdk,
html[lang="fr-fr"] #onetrust-consent-sdk #onetrust-banner-sdk,
html[lang="fr-be"] #onetrust-consent-sdk #onetrust-banner-sdk,
html[lang="pl-pl"] #onetrust-consent-sdk #onetrust-banner-sdk,
html[lang="ru-ru"] #onetrust-consent-sdk #onetrust-banner-sdk,
html[lang="tr-tr"] #onetrust-consent-sdk #onetrust-banner-sdk,
html[lang="ja-jp"] #onetrust-consent-sdk #onetrust-banner-sdk
 { max-width: 780px;}
 
#onetrust-consent-sdk #onetrust-pc-btn-handler,
#onetrust-consent-sdk #onetrust-reject-all-handler,
#onetrust-consent-sdk #onetrust-accept-btn-handler,
#onetrust-pc-sdk .ot-btn-container .ot-pc-refuse-all-handler,
#onetrust-pc-sdk .ot-btn-container .save-preference-btn-handler{
    font-size: 16px;
    font-weight: 700;
    line-height: inherit;
    min-width: 50px;
    text-decoration: none;
    -webkit-transition: background-color .2s ease-out;
    transition: background-color .2s ease-out;
    white-space: nowrap;
    opacity: 1 !important;
    padding: 11px 13px !important;
    border-radius: 0;
    border: 1px solid #76b900;
    letter-spacing: inherit;
}
/*
#onetrust-consent-sdk #onetrust-pc-btn-handler:hover,
#onetrust-consent-sdk #onetrust-reject-all-handler:hover,
#onetrust-consent-sdk #onetrust-accept-btn-handler:hover,
#onetrust-pc-sdk .ot-btn-container>button.ot-pc-refuse-all-handler:hover,
#onetrust-pc-sdk .ot-btn-container>button.save-preference-btn-handler:hover {
    background-color: #91c733 !important;
}*/

#onetrust-consent-sdk #onetrust-banner-sdk #nv-non-gpc-text a,
#onetrust-consent-sdk #onetrust-banner-sdk #nv-gpc-text a {
    color: #000;
    text-decoration: underline;
    text-decoration-color: currentcolor;
    text-decoration-thickness: auto;
    -webkit-text-decoration-color: #76b900;
    text-decoration-color: #76b900;
    text-decoration-thickness: 2px;
    text-underline-offset: .3125em;
	font-weight: normal;
}

#onetrust-consent-sdk #onetrust-banner-sdk #onetrust-policy-text a:hover,
#onetrust-consent-sdk #onetrust-banner-sdk #onetrust-policy-text a:hover {
    -webkit-text-decoration-color: #000;
    text-decoration-color: #000;
}

#onetrust-consent-sdk #onetrust-banner-sdk #onetrust-policy-text {
    font-size: 15px;
}

#onetrust-banner-sdk #onetrust-policy-text #nv-non-gpc-text a,
#onetrust-banner-sdk #onetrust-policy-text #nv-gpc-text a {
    margin-left: 0px;
}

#onetrust-banner-sdk #onetrust-pc-btn-handler,
#onetrust-consent-sdk #onetrust-reject-all-handler,
#onetrust-consent-sdk #onetrust-accept-btn-handler {
    margin-right: 10px !important;
    background-color: transparent;
    border: 2px solid #76b900;
    color: #000000;
}

#onetrust-banner-sdk #onetrust-pc-btn-handler:hover,
#onetrust-consent-sdk #onetrust-reject-all-handler:hover,
#onetrust-consent-sdk #onetrust-accept-btn-handler:hover {
    background-color: transparent !important;
    border: 2px solid #000000;;
    opacity: 1;
}

#onetrust-consent-sdk #onetrust-banner-sdk *:focus,
#onetrust-consent-sdk #onetrust-banner-sdk:focus{
    outline-width:0;
}

/**** GPC detect styles start ****/

#onetrust-banner-sdk .ot-optout-signal ~ #onetrust-policy-text,
#onetrust-banner-sdk .ot-optout-signal ~ #onetrust-policy-text #nv-gpc-text {
    display: block;
}

#onetrust-policy-text #nv-gpc-text,
#onetrust-banner-sdk .ot-optout-signal,
#onetrust-banner-sdk .ot-optout-signal ~ #onetrust-policy-text #nv-non-gpc-text{
    display: none;
}

#nv-done-btn-handler {
    color: #000 !important;
    padding: 13px 15px !important;
    background: #transparent !important;
    border: 2px solid #76b900 !important;
    border-radius: 0 !important;
    font-size: 16px !important;
    font-weight: 700 !important;
    height: auto !important;
    line-height: 1.25em !important;
    margin: 0 auto !important;
}
#nv-done-btn-handler:hover {
    border: 2px solid #000000 !important;;
    opacity: 1 !important;
}
/**** GPC detect styles end ****/
/** NV Extra Cautious Banner styles end **/
#onetrust-pc-sdk.otPcCenter{--ot-footer-space: 160px;overflow:hidden;position:fixed;margin:0 auto;top:5%;right:0;left:0;width:40%;max-width:575px;min-width:575px;border-radius:2.5px;z-index:2147483647;background-color:#fff;-webkit-box-shadow:0px 2px 10px -3px #999;-moz-box-shadow:0px 2px 10px -3px #999;box-shadow:0px 2px 10px -3px #999}#onetrust-pc-sdk.otPcCenter[dir=rtl]{right:0;left:0}#onetrust-pc-sdk.otRelFont{font-size:1rem}#onetrust-pc-sdk .ot-optout-signal{margin-top:.625rem}#onetrust-pc-sdk #ot-addtl-venlst .ot-arw-cntr,#onetrust-pc-sdk #ot-addtl-venlst .ot-plus-minus,#onetrust-pc-sdk .ot-hide-tgl{visibility:hidden}#onetrust-pc-sdk #ot-addtl-venlst .ot-arw-cntr *,#onetrust-pc-sdk #ot-addtl-venlst .ot-plus-minus *,#onetrust-pc-sdk .ot-hide-tgl *{visibility:hidden}#onetrust-pc-sdk #ot-gn-venlst .ot-ven-item .ot-acc-hdr{min-height:40px}#onetrust-pc-sdk .ot-pc-header{height:39px;padding:10px 0 10px 30px;border-bottom:1px solid #e9e9e9}#onetrust-pc-sdk #ot-pc-title,#onetrust-pc-sdk #ot-category-title,#onetrust-pc-sdk .ot-cat-header,#onetrust-pc-sdk #ot-lst-title,#onetrust-pc-sdk .ot-ven-hdr .ot-ven-name,#onetrust-pc-sdk .ot-always-active{font-weight:bold;color:dimgray}#onetrust-pc-sdk .ot-always-active-group .ot-cat-header{width:55%;font-weight:700}#onetrust-pc-sdk .ot-cat-item p:not([role=heading]){clear:both;float:left;margin-top:10px;margin-bottom:5px;line-height:1.5;font-size:.812em;color:dimgray}#onetrust-pc-sdk .ot-close-icon{height:44px;width:44px;background-size:10px}#onetrust-pc-sdk #ot-pc-title{float:left;font-size:1em;line-height:1.5;margin-bottom:10px;margin-top:10px;width:100%}#onetrust-pc-sdk #accept-recommended-btn-handler{position:relative;outline-offset:-1px}#onetrust-pc-sdk #ot-pc-desc{clear:both;width:100%;font-size:.812em;line-height:1.5;margin-bottom:25px}#onetrust-pc-sdk #ot-pc-desc *{font-size:inherit;line-height:inherit}#onetrust-pc-sdk #ot-pc-desc ul li{padding:10px 0px}#onetrust-pc-sdk a{color:#656565;cursor:pointer}#onetrust-pc-sdk a:hover{color:#3860be}#onetrust-pc-sdk label{margin-bottom:0}#onetrust-pc-sdk #vdr-lst-dsc{font-size:.812em;line-height:1.5;padding:10px 15px 5px 15px}#onetrust-pc-sdk button{max-width:394px;padding:12px 30px;line-height:1;word-break:break-word;word-wrap:break-word;white-space:normal;font-weight:bold;height:auto}#onetrust-pc-sdk .ot-link-btn{padding:0;margin-bottom:0;border:0;font-weight:normal;line-height:normal;width:auto;height:auto}#onetrust-pc-sdk #ot-pc-content{position:absolute;overflow-y:scroll;padding-left:2px;padding-right:30px;top:60px;bottom:110px;margin:1px 3px 0 30px;width:calc(100% - 63px)}#onetrust-pc-sdk .ot-vs-list .ot-always-active,#onetrust-pc-sdk .ot-cat-grp .ot-always-active{float:right;clear:none;color:#3860be;margin:0;font-size:.813em;line-height:1.3}#onetrust-pc-sdk .ot-pc-scrollbar::-webkit-scrollbar-track{margin-right:20px}#onetrust-pc-sdk .ot-pc-scrollbar::-webkit-scrollbar{width:11px}#onetrust-pc-sdk .ot-pc-scrollbar::-webkit-scrollbar-thumb{border-radius:10px;background:#d8d8d8}#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:#000 1px solid}#onetrust-pc-sdk .ot-pc-scrollbar{scrollbar-arrow-color:#d8d8d8;scrollbar-darkshadow-color:#d8d8d8;scrollbar-face-color:#d8d8d8;scrollbar-shadow-color:#d8d8d8}#onetrust-pc-sdk .save-preference-btn-handler{margin-right:20px}#onetrust-pc-sdk .ot-pc-refuse-all-handler{margin-right:10px}#onetrust-pc-sdk #ot-pc-desc .privacy-notice-link{margin-left:0;margin-right:8px}#onetrust-pc-sdk #ot-pc-desc .ot-imprint-handler{margin-left:0;margin-right:8px}#onetrust-pc-sdk .ot-subgrp-cntr{display:inline-block;clear:both;width:100%;padding-top:15px}#onetrust-pc-sdk .ot-switch+.ot-subgrp-cntr{padding-top:10px}#onetrust-pc-sdk ul.ot-subgrps{margin:0;font-size:initial}#onetrust-pc-sdk ul.ot-subgrps li p,#onetrust-pc-sdk ul.ot-subgrps li h5{font-size:.813em;line-height:1.4;color:dimgray}#onetrust-pc-sdk ul.ot-subgrps .ot-switch{min-height:auto}#onetrust-pc-sdk ul.ot-subgrps .ot-switch-nob{top:0}#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr{display:inline-block;width:100%}#onetrust-pc-sdk ul.ot-subgrps .ot-acc-txt{margin:0}#onetrust-pc-sdk ul.ot-subgrps li{padding:0;border:none}#onetrust-pc-sdk ul.ot-subgrps li h5,#onetrust-pc-sdk ul.ot-subgrps li p[aria-level="5"]{position:relative;top:5px;font-weight:bold;margin-bottom:0;float:left}#onetrust-pc-sdk li.ot-subgrp{margin-left:20px;overflow:auto}#onetrust-pc-sdk li.ot-subgrp>h5,#onetrust-pc-sdk li.ot-subgrp>p[aria-level="5"]{width:calc(100% - 100px)}#onetrust-pc-sdk .ot-cat-item p>ul,#onetrust-pc-sdk li.ot-subgrp p>ul{margin:0px;list-style:disc;margin-left:15px;font-size:inherit}#onetrust-pc-sdk .ot-cat-item p>ul li,#onetrust-pc-sdk li.ot-subgrp p>ul li{font-size:inherit;padding-top:10px;padding-left:0px;padding-right:0px;border:none}#onetrust-pc-sdk .ot-cat-item p>ul li:last-child,#onetrust-pc-sdk li.ot-subgrp p>ul li:last-child{padding-bottom:10px}#onetrust-pc-sdk .ot-pc-logo{height:40px;width:120px}#onetrust-pc-sdk .ot-pc-footer{position:absolute;bottom:0px;width:100%;max-height:var(--ot-footer-space);border-top:1px solid #d8d8d8}#onetrust-pc-sdk.ot-ftr-stacked #ot-pc-lst{bottom:var(--ot-footer-space)}#onetrust-pc-sdk.ot-ftr-stacked #ot-pc-content{bottom:var(--ot-footer-space)}#onetrust-pc-sdk.ot-ftr-stacked .ot-pc-footer button{margin-left:15px !important;margin-right:15px !important;width:calc(100% - 33px);max-width:none}#onetrust-pc-sdk.ot-ftr-stacked .ot-pc-footer button:not(:last-child){margin-bottom:0}#onetrust-pc-sdk .ot-pc-footer-logo{height:30px;width:100%;text-align:right;background:#f4f4f4}#onetrust-pc-sdk .ot-pc-footer-logo a{display:inline-block;margin-top:5px;margin-right:10px}#onetrust-pc-sdk[dir=rtl] .ot-pc-footer-logo{direction:rtl}#onetrust-pc-sdk[dir=rtl] .ot-pc-footer-logo a{margin-right:25px}#onetrust-pc-sdk .ot-tgl{float:right;position:relative;z-index:1}#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob{background-color:#468254;border:1px solid #fff}#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob:before{-webkit-transform:translateX(20px);-ms-transform:translateX(20px);transform:translateX(20px);background-color:#fff;border-color:#fff}#onetrust-pc-sdk .ot-tgl input:focus+.ot-switch{outline:#000 solid 1px}#onetrust-pc-sdk .ot-switch{position:relative;display:inline-block;width:45px;height:25px}#onetrust-pc-sdk .ot-switch-nob{position:absolute;cursor:pointer;top:0;left:0;right:0;bottom:0;background-color:#767676;border:1px solid #ddd;transition:all .2s ease-in 0s;-moz-transition:all .2s ease-in 0s;-o-transition:all .2s ease-in 0s;-webkit-transition:all .2s ease-in 0s;border-radius:20px}#onetrust-pc-sdk .ot-switch-nob:before{position:absolute;content:"";height:18px;width:18px;bottom:3px;left:3px;background-color:#fff;-webkit-transition:.4s;transition:.4s;border-radius:20px}#onetrust-pc-sdk .ot-chkbox input{width:28px !important;height:28px !important}#onetrust-pc-sdk .ot-chkbox input:checked~label::before{background-color:#3860be}#onetrust-pc-sdk .ot-chkbox input+label::after{content:none;color:#fff}#onetrust-pc-sdk .ot-chkbox input:checked+label::after{content:""}#onetrust-pc-sdk .ot-chkbox input:focus+label::before{outline-style:solid;outline-width:2px;outline-style:auto}#onetrust-pc-sdk .ot-chkbox input[aria-checked=mixed]~label::before{background-color:#3860be}#onetrust-pc-sdk .ot-chkbox input[aria-checked=mixed]+label::after{content:""}#onetrust-pc-sdk .ot-chkbox label{position:relative;display:flex;align-items:center;padding-left:30px;cursor:pointer;font-weight:500;min-height:28px}#onetrust-pc-sdk .ot-chkbox label::before,#onetrust-pc-sdk .ot-chkbox label::after{position:absolute;content:"";display:inline-block;border-radius:3px}#onetrust-pc-sdk .ot-chkbox label::before{height:18px;width:18px;border:1px solid #3860be;left:4px;top:4px}#onetrust-pc-sdk .ot-chkbox label::after{height:5px;width:9px;border-left:3px solid;border-bottom:3px solid;transform:rotate(-45deg);-o-transform:rotate(-45deg);-ms-transform:rotate(-45deg);-webkit-transform:rotate(-45deg);left:8px;top:8px}#onetrust-pc-sdk .ot-label-txt{display:none}#onetrust-pc-sdk .ot-chkbox input,#onetrust-pc-sdk .ot-tgl input{position:absolute;opacity:0;width:0;height:0}#onetrust-pc-sdk .ot-arw-cntr{float:right;position:relative;pointer-events:none}#onetrust-pc-sdk .ot-arw-cntr .ot-arw{width:16px;height:16px;margin-left:5px;color:dimgray;display:inline-block;vertical-align:middle;-webkit-transition:all 150ms ease-in 0s;-moz-transition:all 150ms ease-in 0s;-o-transition:all 150ms ease-in 0s;transition:all 150ms ease-in 0s}#onetrust-pc-sdk input:checked~.ot-acc-hdr .ot-arw,#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-arw-cntr svg{transform:rotate(90deg);-o-transform:rotate(90deg);-ms-transform:rotate(90deg);-webkit-transform:rotate(90deg)}#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:#000 1px solid}#onetrust-pc-sdk .ot-tgl-cntr,#onetrust-pc-sdk .ot-arw-cntr{display:inline-block}#onetrust-pc-sdk .ot-tgl-cntr{float:right;margin-top:2px}#onetrust-pc-sdk #ot-lst-cnt .ot-tgl-cntr{margin-top:10px}#onetrust-pc-sdk .ot-always-active-subgroup{width:auto;padding-left:0px !important;top:3px;position:relative}#onetrust-pc-sdk .ot-label-status{font-size:.75em;display:none;font-size:.75em;position:relative;top:2px;padding-right:5px;float:left}#onetrust-pc-sdk .ot-arw-cntr{margin-top:-1px}#onetrust-pc-sdk .ot-arw-cntr svg{-webkit-transition:all 300ms ease-in 0s;-moz-transition:all 300ms ease-in 0s;-o-transition:all 300ms ease-in 0s;transition:all 300ms ease-in 0s;height:10px;width:10px}#onetrust-pc-sdk input:checked~.ot-acc-hdr .ot-arw{transform:rotate(90deg);-o-transform:rotate(90deg);-ms-transform:rotate(90deg);-webkit-transform:rotate(90deg)}#onetrust-pc-sdk .ot-arw{width:10px;margin-left:15px;transition:all 300ms ease-in 0s;-webkit-transition:all 300ms ease-in 0s;-moz-transition:all 300ms ease-in 0s;-o-transition:all 300ms ease-in 0s}#onetrust-pc-sdk .ot-vlst-cntr{margin-bottom:0}#onetrust-pc-sdk .ot-hlst-cntr{margin-top:5px;display:inline-block;width:100%}#onetrust-pc-sdk .category-vendors-list-handler,#onetrust-pc-sdk .category-vendors-list-handler+a,#onetrust-pc-sdk .category-host-list-handler{clear:both;color:#3860be;margin-left:0;font-size:.813em;text-decoration:none;float:left;overflow:hidden}#onetrust-pc-sdk .category-vendors-list-handler:hover,#onetrust-pc-sdk .category-vendors-list-handler+a:hover,#onetrust-pc-sdk .category-host-list-handler:hover{text-decoration-line:underline}#onetrust-pc-sdk .category-vendors-list-handler+a{clear:none}#onetrust-pc-sdk .ot-vlst-cntr svg.ot-ext-lnk,#onetrust-pc-sdk .ot-ven-hdr svg.ot-ext-lnk,#onetrust-pc-sdk .ot-host-hdr svg.ot-ext-lnk,#onetrust-pc-sdk .ot-cat-grp svg.ot-ext-lnk{display:inline-block;height:13px;width:13px;background-repeat:no-repeat;margin-left:1px;cursor:pointer;vertical-align:middle}#onetrust-pc-sdk .ot-ven-hdr svg.ot-ext-lnk{margin-bottom:-1px}#onetrust-pc-sdk .back-btn-handler{font-size:1em;text-decoration:none}#onetrust-pc-sdk .back-btn-handler:hover{opacity:.6}#onetrust-pc-sdk #ot-lst-title h3,#onetrust-pc-sdk #ot-lst-title p[aria-level="3"]{display:inline-block;word-break:break-word;word-wrap:break-word;margin-bottom:0;color:#656565;font-size:1em;font-weight:bold;margin-left:15px}#onetrust-pc-sdk #ot-lst-title{margin:10px 0 10px 0px;font-size:1em;text-align:left}#onetrust-pc-sdk #ot-pc-hdr{margin:0 0 0 30px;height:auto;width:auto}#onetrust-pc-sdk #ot-pc-hdr input::placeholder{color:#707070;font-style:italic}#onetrust-pc-sdk #vendor-search-handler{height:31px;width:100%;border-radius:50px;font-size:.8em;padding-right:35px;padding-left:15px;float:left;margin-left:15px}#onetrust-pc-sdk .ot-ven-name{display:block;width:auto;padding-right:5px}#onetrust-pc-sdk #ot-lst-cnt{overflow-y:auto;margin-left:20px;margin-right:7px;width:calc(100% - 27px);max-height:calc(100% - 80px);height:100%;transform:translate3d(0, 0, 0)}#onetrust-pc-sdk #ot-pc-lst{width:100%;bottom:100px;position:absolute;top:60px}#onetrust-pc-sdk #ot-pc-lst:not(.ot-enbl-chr) .ot-tgl-cntr .ot-arw-cntr,#onetrust-pc-sdk #ot-pc-lst:not(.ot-enbl-chr) .ot-tgl-cntr .ot-arw-cntr *{visibility:hidden}#onetrust-pc-sdk #ot-pc-lst .ot-tgl-cntr{right:12px;position:absolute}#onetrust-pc-sdk #ot-pc-lst .ot-arw-cntr{float:right;position:relative}#onetrust-pc-sdk #ot-pc-lst .ot-arw{margin-left:10px}#onetrust-pc-sdk #ot-pc-lst .ot-acc-hdr{overflow:hidden;cursor:pointer}#onetrust-pc-sdk .ot-vlst-cntr{overflow:hidden}#onetrust-pc-sdk #ot-sel-blk{overflow:hidden;width:100%;position:sticky;position:-webkit-sticky;top:0;z-index:3}#onetrust-pc-sdk #ot-back-arw{height:12px;width:12px}#onetrust-pc-sdk .ot-lst-subhdr{width:100%;display:inline-block}#onetrust-pc-sdk .ot-search-cntr{float:left;width:78%;position:relative}#onetrust-pc-sdk .ot-search-cntr>svg{width:30px;height:30px;position:absolute;float:left;right:-15px}#onetrust-pc-sdk .ot-fltr-cntr{float:right;right:50px;position:relative}#onetrust-pc-sdk #ot-filter-list-header{margin-top:20px;margin-bottom:10px;float:left;max-width:150px;text-decoration:none;color:#3860be;font-size:.9em;font-weight:bold;background-color:rgba(0,0,0,0);border-color:rgba(0,0,0,0);padding:1px 1px 1px 15px;overflow:hidden;text-overflow:ellipsis;white-space:nowrap}#onetrust-pc-sdk #filter-btn-handler{background-color:#3860be;border-radius:17px;display:inline-block;position:relative;width:32px;height:32px;-moz-transition:.1s ease;-o-transition:.1s ease;-webkit-transition:1s ease;transition:.1s ease;padding:0;margin:0}#onetrust-pc-sdk #filter-btn-handler:hover{background-color:#3860be}#onetrust-pc-sdk #filter-btn-handler svg{width:12px;height:12px;margin:3px 10px 0 10px;display:block;position:static;right:auto;top:auto}#onetrust-pc-sdk .ot-ven-link,#onetrust-pc-sdk .ot-ven-legclaim-link{color:#3860be;text-decoration:none;font-weight:100;display:inline-block;padding-top:10px;transform:translate(0, 1%);-o-transform:translate(0, 1%);-ms-transform:translate(0, 1%);-webkit-transform:translate(0, 1%);position:relative;z-index:2}#onetrust-pc-sdk .ot-ven-link *,#onetrust-pc-sdk .ot-ven-legclaim-link *{font-size:inherit}#onetrust-pc-sdk .ot-ven-link:hover,#onetrust-pc-sdk .ot-ven-legclaim-link:hover{text-decoration:underline}#onetrust-pc-sdk .ot-ven-hdr{width:calc(100% - 160px);height:auto;float:left;word-break:break-word;word-wrap:break-word;vertical-align:middle;padding-bottom:3px}#onetrust-pc-sdk .ot-ven-link,#onetrust-pc-sdk .ot-ven-legclaim-link{letter-spacing:.03em;font-size:.75em;font-weight:400}#onetrust-pc-sdk .ot-ven-dets{border-radius:2px;background-color:#f8f8f8}#onetrust-pc-sdk .ot-ven-dets li:first-child p:first-child{border-top:none}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:not(:first-child){border-top:1px solid #ddd !important}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:nth-child(n+3) p{display:inline-block}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:nth-child(n+3) p:nth-of-type(odd){width:30%}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:nth-child(n+3) p:nth-of-type(even){width:50%;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc p,#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc h5{padding-top:5px;padding-bottom:5px;display:block}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc h5,#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc p[aria-level="5"]{display:inline-block}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc p:nth-last-child(-n+1){padding-bottom:10px}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc p:nth-child(-n+2):not(.disc-pur):not([role=heading]){padding-top:10px}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc .disc-pur-cont{display:inline}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc .disc-pur{position:relative;width:50% !important;word-break:break-word;word-wrap:break-word;left:calc(30% + 17px)}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc .disc-pur:nth-child(-n+1){position:static}#onetrust-pc-sdk .ot-ven-dets p,#onetrust-pc-sdk .ot-ven-dets h5,#onetrust-pc-sdk .ot-ven-dets span{font-size:.69em;text-align:left;vertical-align:middle;word-break:break-word;word-wrap:break-word;margin:0;padding-bottom:10px;padding-left:15px;color:#2e3644}#onetrust-pc-sdk .ot-ven-dets h5,#onetrust-pc-sdk .ot-ven-dets p[aria-level="5"]{padding-top:5px;line-height:1.5}#onetrust-pc-sdk .ot-ven-dets span{color:dimgray;padding:0;vertical-align:baseline}#onetrust-pc-sdk .ot-ven-dets .ot-ven-pur h5,#onetrust-pc-sdk .ot-ven-dets .ot-ven-pur p[aria-level="5"]{border-top:1px solid #e9e9e9;border-bottom:1px solid #e9e9e9;padding-bottom:5px;margin-bottom:5px;font-weight:bold}#onetrust-pc-sdk #ot-host-lst .ot-sel-all{float:right;position:relative;margin-right:42px;top:10px}#onetrust-pc-sdk #ot-host-lst .ot-sel-all input[type=checkbox]{width:auto;height:auto}#onetrust-pc-sdk #ot-host-lst .ot-sel-all label{height:20px;width:20px;padding-left:0px}#onetrust-pc-sdk #ot-host-lst .ot-acc-txt{overflow:hidden;width:95%}#onetrust-pc-sdk .ot-host-hdr{position:relative;z-index:1;pointer-events:none;width:calc(100% - 125px);float:left}#onetrust-pc-sdk .ot-host-name,#onetrust-pc-sdk .ot-host-desc{display:inline-block;width:90%}#onetrust-pc-sdk .ot-host-name{pointer-events:none}#onetrust-pc-sdk .ot-host-hdr>a{text-decoration:underline;font-size:.82em;position:relative;z-index:2;margin-bottom:5px;pointer-events:initial}#onetrust-pc-sdk .ot-host-name+a{margin-top:5px}#onetrust-pc-sdk .ot-host-name,#onetrust-pc-sdk .ot-host-name a,#onetrust-pc-sdk .ot-host-desc,#onetrust-pc-sdk .ot-host-info{color:dimgray;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-host-name,#onetrust-pc-sdk .ot-host-name a{font-weight:bold;font-size:.82em;line-height:1.3}#onetrust-pc-sdk .ot-host-name a{font-size:1em}#onetrust-pc-sdk .ot-host-expand{margin-top:3px;margin-bottom:3px;clear:both;display:block;color:#3860be;font-size:.72em;font-weight:normal}#onetrust-pc-sdk .ot-host-expand *{font-size:inherit}#onetrust-pc-sdk .ot-host-desc,#onetrust-pc-sdk .ot-host-info{font-size:.688em;line-height:1.4;font-weight:normal}#onetrust-pc-sdk .ot-host-desc{margin-top:10px}#onetrust-pc-sdk .ot-host-opt{margin:0;font-size:inherit;display:inline-block;width:100%}#onetrust-pc-sdk .ot-host-opt li>dl{font-size:.8em;padding:5px 0;margin:5px 0;display:flex}#onetrust-pc-sdk .ot-host-opt li>dl dt{width:30%;float:left}#onetrust-pc-sdk .ot-host-opt li>dl dd{width:70%;float:left;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-host-info{border:none;display:inline-block;width:calc(100% - 10px);padding:10px;margin-bottom:10px;background-color:#f8f8f8}#onetrust-pc-sdk .ot-host-info>div{overflow:auto}#onetrust-pc-sdk #no-results{text-align:center;margin-top:30px}#onetrust-pc-sdk #no-results p{font-size:1em;color:#2e3644;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk #no-results p span{font-weight:bold}#onetrust-pc-sdk #ot-fltr-modal{width:100%;height:auto;display:none;-moz-transition:.2s ease;-o-transition:.2s ease;-webkit-transition:2s ease;transition:.2s ease;overflow:hidden;opacity:1;right:0}#onetrust-pc-sdk #ot-fltr-modal .ot-label-txt{display:inline-block;font-size:.85em;color:dimgray}#onetrust-pc-sdk #ot-fltr-cnt{z-index:2147483646;background-color:#fff;position:absolute;height:90%;max-height:300px;width:325px;left:210px;margin-top:10px;margin-bottom:20px;padding-right:10px;border-radius:3px;-webkit-box-shadow:0px 0px 12px 2px #c7c5c7;-moz-box-shadow:0px 0px 12px 2px #c7c5c7;box-shadow:0px 0px 12px 2px #c7c5c7}#onetrust-pc-sdk .ot-fltr-scrlcnt{overflow-y:auto;overflow-x:hidden;clear:both;max-height:calc(100% - 60px)}#onetrust-pc-sdk #ot-anchor{border:12px solid rgba(0,0,0,0);display:none;position:absolute;z-index:2147483647;right:55px;top:75px;transform:rotate(45deg);-o-transform:rotate(45deg);-ms-transform:rotate(45deg);-webkit-transform:rotate(45deg);background-color:#fff;-webkit-box-shadow:-3px -3px 5px -2px #c7c5c7;-moz-box-shadow:-3px -3px 5px -2px #c7c5c7;box-shadow:-3px -3px 5px -2px #c7c5c7}#onetrust-pc-sdk .ot-fltr-btns{margin-left:15px}#onetrust-pc-sdk #filter-apply-handler{margin-right:15px}#onetrust-pc-sdk .ot-fltr-opt{margin-bottom:5px;margin-left:15px;min-height:20px;width:75%;position:relative}#onetrust-pc-sdk .ot-fltr-opt p{display:inline-block;margin:0;font-size:.9em;color:#2e3644}#onetrust-pc-sdk .ot-chkbox label span{font-size:.85em;color:dimgray}#onetrust-pc-sdk .ot-chkbox input[type=checkbox]+label::after{content:none;color:#fff}#onetrust-pc-sdk .ot-chkbox input[type=checkbox]:checked+label::after{content:""}#onetrust-pc-sdk .ot-chkbox input[type=checkbox][aria-checked=mixed]+label::after{content:""}#onetrust-pc-sdk .ot-chkbox input[type=checkbox]:focus+label::before{outline-style:solid;outline-width:2px;outline-style:auto}#onetrust-pc-sdk #ot-selall-vencntr,#onetrust-pc-sdk #ot-selall-adtlvencntr,#onetrust-pc-sdk #ot-selall-hostcntr,#onetrust-pc-sdk #ot-selall-licntr,#onetrust-pc-sdk #ot-selall-gnvencntr{right:15px;position:relative;width:20px;height:20px;float:right}#onetrust-pc-sdk #ot-selall-vencntr label,#onetrust-pc-sdk #ot-selall-adtlvencntr label,#onetrust-pc-sdk #ot-selall-hostcntr label,#onetrust-pc-sdk #ot-selall-licntr label,#onetrust-pc-sdk #ot-selall-gnvencntr label{float:left;padding-left:0}#onetrust-pc-sdk #ot-ven-lst:first-child{border-top:1px solid #e2e2e2}#onetrust-pc-sdk ul{list-style:none;padding:0}#onetrust-pc-sdk ul li:not(.ot-fltr-opt){position:relative;margin:0;padding:15px 15px 15px 10px;border-bottom:1px solid #e2e2e2}#onetrust-pc-sdk ul li h3,#onetrust-pc-sdk ul li h4,#onetrust-pc-sdk ul li p[aria-level="3"]:not(.ot-host-name,.ot-host-desc),#onetrust-pc-sdk ul li p[aria-level="4"]:not(.ot-host-name,.ot-host-desc){font-size:.75em;color:#656565;margin:0;display:inline-block;width:70%;height:auto;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk ul li p:not([role=heading]){margin:0;font-size:.7em}#onetrust-pc-sdk ul li input[type=checkbox]{position:absolute;cursor:pointer;width:100%;height:100%;opacity:0;margin:0;top:0;left:0}#onetrust-pc-sdk .ot-cat-item>button:focus,#onetrust-pc-sdk .ot-acc-cntr>button:focus,#onetrust-pc-sdk li>button:focus{outline:#000 solid 2px}#onetrust-pc-sdk .ot-cat-item>button,#onetrust-pc-sdk .ot-acc-cntr>button,#onetrust-pc-sdk li>button{position:absolute;cursor:pointer;width:100%;height:100%;margin:0;top:0;left:0;z-index:1;max-width:none;border:none}#onetrust-pc-sdk .ot-cat-item>button[aria-expanded=false]~.ot-acc-txt,#onetrust-pc-sdk .ot-acc-cntr>button[aria-expanded=false]~.ot-acc-txt,#onetrust-pc-sdk li>button[aria-expanded=false]~.ot-acc-txt{margin-top:0;max-height:0;opacity:0;overflow:hidden;width:100%;transition:.25s ease-out;display:none}#onetrust-pc-sdk .ot-cat-item>button[aria-expanded=true]~.ot-acc-txt,#onetrust-pc-sdk .ot-acc-cntr>button[aria-expanded=true]~.ot-acc-txt,#onetrust-pc-sdk li>button[aria-expanded=true]~.ot-acc-txt{transition:.1s ease-in;margin-top:10px;width:100%;overflow:auto;display:block}#onetrust-pc-sdk .ot-cat-item>button[aria-expanded=true]~.ot-acc-grpcntr,#onetrust-pc-sdk .ot-acc-cntr>button[aria-expanded=true]~.ot-acc-grpcntr,#onetrust-pc-sdk li>button[aria-expanded=true]~.ot-acc-grpcntr{width:auto;margin-top:0px;padding-bottom:10px}#onetrust-pc-sdk .ot-host-item>button:focus,#onetrust-pc-sdk .ot-ven-item>button:focus{outline:0;border:2px solid #000}#onetrust-pc-sdk .ot-hide-acc>button{pointer-events:none}#onetrust-pc-sdk .ot-hide-acc .ot-plus-minus>*,#onetrust-pc-sdk .ot-hide-acc .ot-arw-cntr>*{visibility:hidden}#onetrust-pc-sdk .ot-hide-acc .ot-acc-hdr{min-height:30px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt){padding-right:10px;width:calc(100% - 37px);margin-top:10px;max-height:calc(100% - 90px)}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk{background-color:#f9f9fc;border:1px solid #e2e2e2;width:calc(100% - 2px);padding-bottom:5px;padding-top:5px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt{border:unset;background-color:unset}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt .ot-sel-all-hdr{display:none}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt .ot-sel-all{padding-right:.5rem}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt .ot-sel-all .ot-chkbox{right:0}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) .ot-sel-all{padding-right:34px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) .ot-sel-all-chkbox{width:auto}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) ul li{border:1px solid #e2e2e2;margin-bottom:10px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) .ot-acc-cntr>.ot-acc-hdr{padding:10px 0 10px 15px}#onetrust-pc-sdk.ot-addtl-vendors .ot-sel-all-chkbox{float:right}#onetrust-pc-sdk.ot-addtl-vendors .ot-plus-minus~.ot-sel-all-chkbox{right:34px}#onetrust-pc-sdk.ot-addtl-vendors #ot-ven-lst:first-child{border-top:none}#onetrust-pc-sdk .ot-acc-cntr{position:relative;border-left:1px solid #e2e2e2;border-right:1px solid #e2e2e2;border-bottom:1px solid #e2e2e2}#onetrust-pc-sdk .ot-acc-cntr input{z-index:1}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr{background-color:#f9f9fc;padding:5px 0 5px 15px;width:auto}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr .ot-plus-minus{vertical-align:middle;top:auto}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr .ot-arw-cntr{right:10px}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr input{z-index:2}#onetrust-pc-sdk .ot-acc-cntr.ot-add-tech .ot-acc-hdr{padding:10px 0 10px 15px}#onetrust-pc-sdk .ot-acc-cntr>input[type=checkbox]:checked~.ot-acc-hdr{border-bottom:1px solid #e2e2e2}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-txt{padding-left:10px;padding-right:10px}#onetrust-pc-sdk .ot-acc-cntr button[aria-expanded=true]~.ot-acc-txt{width:auto}#onetrust-pc-sdk .ot-acc-cntr .ot-addtl-venbox{display:none}#onetrust-pc-sdk .ot-vlst-cntr{margin-bottom:0;width:100%}#onetrust-pc-sdk .ot-vensec-title{font-size:.813em;vertical-align:middle;display:inline-block}#onetrust-pc-sdk .category-vendors-list-handler,#onetrust-pc-sdk .category-vendors-list-handler+a{margin-left:0;margin-top:10px}#onetrust-pc-sdk #ot-selall-vencntr.line-through label::after,#onetrust-pc-sdk #ot-selall-adtlvencntr.line-through label::after,#onetrust-pc-sdk #ot-selall-licntr.line-through label::after,#onetrust-pc-sdk #ot-selall-hostcntr.line-through label::after,#onetrust-pc-sdk #ot-selall-gnvencntr.line-through label::after{height:auto;border-left:0;transform:none;-o-transform:none;-ms-transform:none;-webkit-transform:none;left:9px;top:12px}#onetrust-pc-sdk #ot-category-title{float:left;padding-bottom:10px;font-size:1em;width:100%}#onetrust-pc-sdk .ot-cat-grp{margin-top:10px}#onetrust-pc-sdk .ot-cat-item{line-height:1.1;margin-top:10px;display:inline-block;width:100%}#onetrust-pc-sdk .ot-btn-container{text-align:right}#onetrust-pc-sdk .ot-btn-container button{display:inline-block;font-size:.75em;margin-top:15px;min-height:40px}#onetrust-pc-sdk .ot-btn-container.ot-button-order-container{display:flex;flex-wrap:wrap;justify-content:flex-end}#onetrust-pc-sdk .ot-btn-container.ot-button-order-container *[class*=ot-button-order-]:nth-of-type(1){margin-right:auto;margin-left:15px}#onetrust-pc-sdk .ot-btn-container.ot-button-order-container *[class*=ot-button-order-]:nth-of-type(2),#onetrust-pc-sdk .ot-btn-container.ot-button-order-container *[class*=ot-button-order-]:nth-of-type(3){margin-right:15px}#onetrust-pc-sdk .ot-btn-container.ot-button-order-container.ot-stack-buttons{flex:1;width:auto;gap:.5rem;height:100%;padding:0 30px;flex-wrap:nowrap;margin-top:.75rem;align-items:center;margin-bottom:.75rem;flex-direction:column;justify-content:space-around}#onetrust-pc-sdk .ot-btn-container.ot-button-order-container.ot-stack-buttons .ot-pc-refuse-all-handler,#onetrust-pc-sdk .ot-btn-container.ot-button-order-container.ot-stack-buttons .save-preference-btn-handler,#onetrust-pc-sdk .ot-btn-container.ot-button-order-container.ot-stack-buttons #accept-recommended-btn-handler{width:100%;margin:0 !important}#onetrust-pc-sdk #close-pc-btn-handler.ot-close-icon{position:absolute;top:10px;right:0;z-index:1;padding:0;background-color:rgba(0,0,0,0);border:none}#onetrust-pc-sdk #close-pc-btn-handler.ot-close-icon svg{display:block;height:10px;width:10px}#onetrust-pc-sdk #clear-filters-handler{margin-top:20px;margin-bottom:10px;float:right;max-width:200px;text-decoration:none;color:#3860be;font-size:.9em;font-weight:bold;background-color:rgba(0,0,0,0);border-color:rgba(0,0,0,0);padding:1px}#onetrust-pc-sdk #clear-filters-handler:hover{color:#2285f7}#onetrust-pc-sdk #clear-filters-handler:focus{outline:#000 solid 1px}#onetrust-pc-sdk .ot-enbl-chr h4~.ot-tgl,#onetrust-pc-sdk .ot-enbl-chr h4~.ot-always-active{right:45px}#onetrust-pc-sdk .ot-enbl-chr h4~.ot-tgl+.ot-tgl{right:120px}#onetrust-pc-sdk .ot-enbl-chr .ot-pli-hdr.ot-leg-border-color span:first-child{width:90px}#onetrust-pc-sdk .ot-enbl-chr li.ot-subgrp>h5+.ot-tgl-cntr,#onetrust-pc-sdk .ot-enbl-chr li.ot-subgrp>p[aria-level="5"]+.ot-tgl-cntr{padding-right:25px}#onetrust-pc-sdk .ot-plus-minus{width:20px;height:20px;font-size:1.5em;position:relative;display:inline-block;margin-right:5px;top:3px}#onetrust-pc-sdk .ot-plus-minus span{position:absolute;background:#27455c;border-radius:1px}#onetrust-pc-sdk .ot-plus-minus span:first-of-type{top:25%;bottom:25%;width:10%;left:45%}#onetrust-pc-sdk .ot-plus-minus span:last-of-type{left:25%;right:25%;height:10%;top:45%}#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-arw,#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-plus-minus span:first-of-type,#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-plus-minus span:last-of-type{transform:rotate(90deg)}#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-plus-minus span:last-of-type{left:50%;right:50%}#onetrust-pc-sdk #ot-selall-vencntr label,#onetrust-pc-sdk #ot-selall-adtlvencntr label,#onetrust-pc-sdk #ot-selall-hostcntr label,#onetrust-pc-sdk #ot-selall-licntr label{position:relative;display:inline-block;width:20px;height:20px}#onetrust-pc-sdk .ot-host-item .ot-plus-minus,#onetrust-pc-sdk .ot-ven-item .ot-plus-minus{float:left;margin-right:8px;top:10px}#onetrust-pc-sdk .ot-ven-item ul{list-style:none inside;font-size:100%;margin:0}#onetrust-pc-sdk .ot-ven-item ul li{margin:0 !important;padding:0;border:none !important}#onetrust-pc-sdk .ot-pli-hdr{color:#77808e;overflow:hidden;padding-top:7.5px;padding-bottom:7.5px;width:calc(100% - 2px);border-top-left-radius:3px;border-top-right-radius:3px}#onetrust-pc-sdk .ot-pli-hdr span:first-child{top:50%;transform:translateY(50%);max-width:90px}#onetrust-pc-sdk .ot-pli-hdr span:last-child{padding-right:10px;max-width:95px;text-align:center}#onetrust-pc-sdk .ot-li-title{float:right;font-size:.813em}#onetrust-pc-sdk .ot-pli-hdr.ot-leg-border-color{background-color:#f4f4f4;border:1px solid #d8d8d8}#onetrust-pc-sdk .ot-pli-hdr.ot-leg-border-color span:first-child{text-align:left;width:70px}#onetrust-pc-sdk li.ot-subgrp>h5,#onetrust-pc-sdk li.ot-subgrp>p[aria-level="5"],#onetrust-pc-sdk .ot-cat-header{width:calc(100% - 130px)}#onetrust-pc-sdk li.ot-subgrp>h5+.ot-tgl-cntr{padding-left:13px}#onetrust-pc-sdk .ot-acc-grpcntr .ot-acc-grpdesc{margin-bottom:5px}#onetrust-pc-sdk .ot-acc-grpcntr .ot-subgrp-cntr{border-top:1px solid #d8d8d8}#onetrust-pc-sdk .ot-acc-grpcntr .ot-vlst-cntr+.ot-subgrp-cntr{border-top:none}#onetrust-pc-sdk .ot-acc-hdr .ot-arw-cntr+.ot-tgl-cntr,#onetrust-pc-sdk .ot-acc-txt h4+.ot-tgl-cntr,#onetrust-pc-sdk .ot-acc-txt p[aria-level="4"]+.ot-tgl-cntr{padding-left:13px}#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item .ot-subgrp>h5,#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item .ot-subgrp>p[aria-level="5"],#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item .ot-cat-header{width:calc(100% - 145px)}#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item h5+.ot-tgl-cntr,#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item .ot-cat-header+.ot-tgl{padding-left:28px}#onetrust-pc-sdk .ot-sel-all-hdr,#onetrust-pc-sdk .ot-sel-all-chkbox{display:inline-block;width:100%;position:relative}#onetrust-pc-sdk .ot-sel-all-chkbox{z-index:1}#onetrust-pc-sdk .ot-sel-all{margin:0;position:relative;padding-right:23px;float:right}#onetrust-pc-sdk .ot-consent-hdr,#onetrust-pc-sdk .ot-li-hdr{float:right;font-size:.812em;line-height:normal;text-align:center;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-li-hdr{max-width:100px;padding-right:10px}#onetrust-pc-sdk .ot-consent-hdr{max-width:55px}#onetrust-pc-sdk #ot-selall-licntr{display:block;width:21px;height:auto;float:right;position:relative;right:80px}#onetrust-pc-sdk #ot-selall-licntr label{position:absolute}#onetrust-pc-sdk .ot-ven-ctgl{margin-left:66px}#onetrust-pc-sdk .ot-ven-litgl+.ot-arw-cntr{margin-left:81px}#onetrust-pc-sdk .ot-enbl-chr .ot-host-cnt .ot-tgl-cntr{width:auto}#onetrust-pc-sdk #ot-lst-cnt:not(.ot-host-cnt) .ot-tgl-cntr{width:auto;top:auto;height:20px}#onetrust-pc-sdk #ot-lst-cnt .ot-chkbox{position:relative;display:inline-block;width:28px;height:28px}#onetrust-pc-sdk #ot-lst-cnt .ot-chkbox label{position:absolute;padding:0;width:28px;height:28px}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info-cntr{border:1px solid #d8d8d8;padding:.75rem 2rem;padding-bottom:0;width:auto;margin-top:.5rem}#onetrust-pc-sdk .ot-acc-grpdesc+.ot-leg-btn-container{padding-left:20px;padding-right:20px;width:calc(100% - 40px);margin-bottom:5px}#onetrust-pc-sdk .ot-subgrp .ot-leg-btn-container{margin-bottom:5px}#onetrust-pc-sdk #ot-ven-lst .ot-leg-btn-container{margin-top:10px}#onetrust-pc-sdk .ot-leg-btn-container{display:inline-block;width:100%;margin-bottom:10px}#onetrust-pc-sdk .ot-leg-btn-container button{height:auto;padding:6.5px 8px;margin-bottom:0;letter-spacing:0;font-size:.75em;line-height:normal}#onetrust-pc-sdk .ot-leg-btn-container svg{display:none;height:14px;width:14px;padding-right:5px;vertical-align:sub}#onetrust-pc-sdk .ot-active-leg-btn{cursor:default;pointer-events:none}#onetrust-pc-sdk .ot-active-leg-btn svg{display:inline-block}#onetrust-pc-sdk .ot-remove-objection-handler{text-decoration:underline;padding:0;font-size:.75em;font-weight:600;line-height:1;padding-left:10px}#onetrust-pc-sdk .ot-obj-leg-btn-handler span{font-weight:bold;text-align:center;font-size:inherit;line-height:1.5}#onetrust-pc-sdk.ot-close-btn-link #close-pc-btn-handler{border:none;height:auto;line-height:1.5;text-decoration:underline;font-size:.69em;background:none;right:15px;top:15px;width:auto;font-weight:normal}#onetrust-pc-sdk .ot-pgph-link{font-size:.813em !important;margin-top:10px;position:relative}#onetrust-pc-sdk .ot-pgph-link.ot-pgph-link-subgroup{margin-bottom:1rem}#onetrust-pc-sdk .ot-accordion-layout .ot-pgph-link{margin-top:5px}#onetrust-pc-sdk .ot-pgph-contr{margin:0 2.5rem}#onetrust-pc-sdk .ot-pgph-title{font-size:1.18rem;margin-bottom:2rem}#onetrust-pc-sdk .ot-pgph-desc{font-size:1rem;font-weight:400;margin-bottom:2rem;line-height:1.5rem}#onetrust-pc-sdk .ot-pgph-desc:not(:last-child):after{content:"";width:96%;display:block;margin:0 auto;padding-bottom:2rem;border-bottom:1px solid #e9e9e9}#onetrust-pc-sdk .ot-cat-header{float:left;font-weight:600;font-size:.875em;line-height:1.5;max-width:90%;vertical-align:middle}#onetrust-pc-sdk .ot-vnd-item>button:focus{outline:#000 solid 2px}#onetrust-pc-sdk .ot-vnd-item>button{position:absolute;cursor:pointer;width:100%;height:100%;margin:0;top:0;left:0;z-index:1;max-width:none;border:none}#onetrust-pc-sdk .ot-vnd-item>button[aria-expanded=false]~.ot-acc-txt{margin-top:0;max-height:0;opacity:0;overflow:hidden;width:100%;transition:.25s ease-out;display:none}#onetrust-pc-sdk .ot-vnd-item>button[aria-expanded=true]~.ot-acc-txt{transition:.1s ease-in;margin-top:10px;width:100%;overflow:auto;display:block}#onetrust-pc-sdk .ot-vnd-item>button[aria-expanded=true]~.ot-acc-grpcntr{width:auto;margin-top:0px;padding-bottom:10px}#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item{position:relative;border-radius:2px;margin:0;padding:0;border:1px solid #d8d8d8;border-top:none;width:calc(100% - 2px);float:left}#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item:first-of-type{margin-top:10px;border-top:1px solid #d8d8d8}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpdesc{padding-left:20px;padding-right:20px;width:calc(100% - 40px);font-size:.812em;margin-bottom:10px;margin-top:15px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpdesc>ul{padding-top:10px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpdesc>ul li{padding-top:0;line-height:1.5;padding-bottom:10px}#onetrust-pc-sdk .ot-accordion-layout div+.ot-acc-grpdesc{margin-top:5px}#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr:first-child{margin-top:10px}#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr:last-child,#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr:last-child{margin-bottom:5px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{padding-top:11.5px;padding-bottom:11.5px;padding-left:20px;padding-right:20px;width:calc(100% - 40px);display:inline-block}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-txt{width:100%;padding:0}#onetrust-pc-sdk .ot-accordion-layout .ot-subgrp-cntr{padding-left:20px;padding-right:15px;padding-bottom:0;width:calc(100% - 35px)}#onetrust-pc-sdk .ot-accordion-layout .ot-subgrp{padding-right:5px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpcntr{z-index:1;position:relative}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header+.ot-arw-cntr{position:absolute;top:50%;transform:translateY(-50%);right:20px;margin-top:-2px}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header+.ot-arw-cntr .ot-arw{width:15px;height:20px;margin-left:5px;color:dimgray}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{float:none;color:#2e3644;margin:0;display:inline-block;height:auto;word-wrap:break-word;min-height:inherit}#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr,#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr{padding-left:20px;width:calc(100% - 20px);display:inline-block;margin-top:0;padding-bottom:2px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{position:relative;min-height:25px}#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl,#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active{position:absolute;top:50%;transform:translateY(-50%);right:20px}#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl+.ot-tgl{right:95px}#onetrust-pc-sdk .ot-accordion-layout .category-vendors-list-handler,#onetrust-pc-sdk .ot-accordion-layout .category-vendors-list-handler+a{margin-top:5px}#onetrust-pc-sdk #ot-pc-lst{display:flex;flex-direction:column}#onetrust-pc-sdk #ot-lst-cnt{margin-top:1rem;max-height:calc(100% - 100px)}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info-cntr{border:1px solid #d8d8d8;padding:.75rem 2rem;padding-bottom:0;width:auto;margin-top:.5rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info{margin-bottom:1rem;padding-left:.75rem;padding-right:.75rem;display:flex;flex-direction:column}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info>div{display:flex}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info[data-vnd-info-key*=DPOEmail]{border-top:1px solid #d8d8d8;padding-top:1rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info[data-vnd-info-key*=DPOLink]{border-bottom:1px solid #d8d8d8;padding-bottom:1rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info .ot-vnd-lbl{font-weight:bold;font-size:.85em;margin-bottom:.5rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info .ot-vnd-cnt{margin-left:.5rem;font-weight:500;font-size:.85rem}#onetrust-pc-sdk .ot-vs-list,#onetrust-pc-sdk .ot-vnd-serv{width:auto;padding:1rem 1.25rem;padding-bottom:0}#onetrust-pc-sdk .ot-vs-list .ot-vnd-serv-hdr-cntr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-serv-hdr-cntr{padding-bottom:.75rem;border-bottom:1px solid #d8d8d8}#onetrust-pc-sdk .ot-vs-list .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr{font-weight:600;font-size:.95em;line-height:2;margin-left:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item{border:none;margin:0;padding:0}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item button,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item button{outline:none;border-bottom:1px solid #d8d8d8}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item button[aria-expanded=true],#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item button[aria-expanded=true]{border-bottom:none}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item:first-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item:first-child{margin-top:.25rem;border-top:unset}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item:last-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item:last-child{margin-bottom:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item:last-child button,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item:last-child button{border-bottom:none}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info-cntr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info-cntr{border:1px solid #d8d8d8;padding:.75rem 1.75rem;padding-bottom:0;width:auto;margin-top:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info{margin-bottom:1rem;padding-left:.75rem;padding-right:.75rem;display:flex;flex-direction:column}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info>div,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info>div{display:flex}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOEmail],#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOEmail]{border-top:1px solid #d8d8d8;padding-top:1rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOLink],#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOLink]{border-bottom:1px solid #d8d8d8;padding-bottom:1rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info .ot-vnd-lbl,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info .ot-vnd-lbl{font-weight:bold;font-size:.85em;margin-bottom:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info .ot-vnd-cnt,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info .ot-vnd-cnt{margin-left:.5rem;font-weight:500;font-size:.85rem}#onetrust-pc-sdk .ot-vs-list.ot-vnd-subgrp-cnt,#onetrust-pc-sdk .ot-vnd-serv.ot-vnd-subgrp-cnt{padding-left:40px}#onetrust-pc-sdk .ot-vs-list.ot-vnd-subgrp-cnt .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr,#onetrust-pc-sdk .ot-vnd-serv.ot-vnd-subgrp-cnt .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr{font-size:.8em}#onetrust-pc-sdk .ot-vs-list.ot-vnd-subgrp-cnt .ot-cat-header,#onetrust-pc-sdk .ot-vnd-serv.ot-vnd-subgrp-cnt .ot-cat-header{font-size:.8em}#onetrust-pc-sdk .ot-subgrp-cntr .ot-vnd-serv{margin-bottom:1rem;padding:1rem .95rem}#onetrust-pc-sdk .ot-subgrp-cntr .ot-vnd-serv .ot-vnd-serv-hdr-cntr{padding-bottom:.75rem;border-bottom:1px solid #d8d8d8}#onetrust-pc-sdk .ot-subgrp-cntr .ot-vnd-serv .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr{font-weight:700;font-size:.8em;line-height:20px;margin-left:.82rem}#onetrust-pc-sdk .ot-subgrp-cntr .ot-cat-header{font-weight:700;font-size:.8em;line-height:20px}#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-vnd-serv .ot-vnd-lst-cont .ot-accordion-layout .ot-acc-hdr div.ot-chkbox{margin-left:.82rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr{padding:.7rem 0;margin:0;display:flex;width:100%;align-items:center;justify-content:space-between}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr div:first-child,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr div:first-child,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr div:first-child{margin-left:.5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr div:last-child,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr div:last-child,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr div:last-child{margin-right:.5rem;margin-left:.5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-always-active{position:relative;right:unset;top:unset;transform:unset}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-plus-minus{top:0}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-arw-cntr{float:none;top:unset;right:unset;transform:unset;margin-top:-2px;position:relative}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-cat-header{flex:1;margin:0 .5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-tgl{position:relative;transform:none;right:0;top:0;float:none}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox{position:relative;margin:0 .5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox label{padding:0}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox label::before{position:relative}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox input{position:absolute;cursor:pointer;width:100%;height:100%;opacity:0;margin:0;top:0;left:0;z-index:1}#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp .ot-acc-hdr h5.ot-cat-header,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp .ot-acc-hdr h4.ot-cat-header,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp .ot-acc-hdr p[aria-level="5"].ot-cat-header,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp .ot-acc-hdr p[aria-level="4"].ot-cat-header{margin:0}#onetrust-pc-sdk .ot-vs-config .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp h5,#onetrust-pc-sdk .ot-vs-config .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp p[aria-level="5"]{top:0;line-height:20px}#onetrust-pc-sdk .ot-vs-list{display:flex;flex-direction:column;padding:0;margin:.5rem 4px}#onetrust-pc-sdk .ot-vs-selc-all{display:flex;padding:0;float:unset;align-items:center;justify-content:flex-start}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf{justify-content:flex-end}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf.ot-caret-conf .ot-sel-all-chkbox{margin-right:48px}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf .ot-sel-all-chkbox{margin:0;padding:0;margin-right:14px;justify-content:flex-end}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf #ot-selall-vencntr.ot-chkbox,#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf #ot-selall-vencntr.ot-tgl{display:inline-block;right:unset;width:auto;height:auto;float:none}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf #ot-selall-vencntr label{width:45px;height:25px}#onetrust-pc-sdk .ot-vs-selc-all .ot-sel-all-chkbox{margin-right:11px;margin-left:.75rem;display:flex;align-items:center}#onetrust-pc-sdk .ot-vs-selc-all .sel-all-hdr{margin:0 1.25rem;font-size:.812em;line-height:normal;text-align:center;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-vnd-list-cnt #ot-selall-vencntr.ot-chkbox{float:unset;right:0}#onetrust-pc-sdk[dir=rtl] #ot-back-arw,#onetrust-pc-sdk[dir=rtl] input~.ot-acc-hdr .ot-arw{transform:rotate(180deg);-o-transform:rotate(180deg);-ms-transform:rotate(180deg);-webkit-transform:rotate(180deg)}#onetrust-pc-sdk[dir=rtl] input:checked~.ot-acc-hdr .ot-arw{transform:rotate(270deg);-o-transform:rotate(270deg);-ms-transform:rotate(270deg);-webkit-transform:rotate(270deg)}#onetrust-pc-sdk[dir=rtl] .ot-chkbox label::after{transform:rotate(45deg);-webkit-transform:rotate(45deg);-o-transform:rotate(45deg);-ms-transform:rotate(45deg);border-left:0;border-right:3px solid}#onetrust-pc-sdk[dir=rtl] .ot-search-cntr>svg{right:0}@media only screen and (max-width: 600px){#onetrust-pc-sdk.otPcCenter{left:0;min-width:100%;height:100% !important;top:0;border-radius:0}#onetrust-pc-sdk #ot-pc-content.ot-button-order-container,#onetrust-pc-sdk.ot-ftr-stacked .ot-btn-container.ot-button-order-container{margin:0;width:100%;padding:.5em 1em;gap:.5rem;flex-wrap:nowrap;align-items:center;flex-direction:column;box-sizing:border-box;height:calc(100% - 30px);justify-content:space-around}#onetrust-pc-sdk #ot-pc-content.ot-button-order-container *[class*=ot-button-order-],#onetrust-pc-sdk.ot-ftr-stacked .ot-btn-container.ot-button-order-container *[class*=ot-button-order-]{margin:0 !important}#onetrust-pc-sdk .ot-btn-container button{max-width:none;letter-spacing:.01em}#onetrust-pc-sdk #close-pc-btn-handler{top:10px;right:17px}#onetrust-pc-sdk p{font-size:.7em}#onetrust-pc-sdk #ot-pc-hdr{margin:10px 10px 0 5px;width:calc(100% - 15px)}#onetrust-pc-sdk .vendor-search-handler{font-size:1em}#onetrust-pc-sdk #ot-back-arw{margin-left:12px}#onetrust-pc-sdk #ot-lst-cnt{margin:0;padding:0 5px 0 10px;min-width:95%}#onetrust-pc-sdk .switch+p{max-width:80%}#onetrust-pc-sdk .ot-ftr-stacked button{width:100%}#onetrust-pc-sdk #ot-fltr-cnt{max-width:320px;width:90%;border-top-right-radius:0;border-bottom-right-radius:0;margin:0;margin-left:15px;left:auto;right:40px;top:85px}#onetrust-pc-sdk .ot-fltr-opt{margin-left:25px;margin-bottom:10px}#onetrust-pc-sdk #ot-fltr-cnt{right:40px}}@media only screen and (max-width: 500px){#onetrust-pc-sdk .ot-fltr-cntr,#onetrust-pc-sdk #ot-fltr-cnt{right:10px}#onetrust-pc-sdk #ot-anchor{right:25px}#onetrust-pc-sdk button{width:100%}#onetrust-pc-sdk:not(.ot-addtl-vendors) #ot-pc-lst:not(.ot-enbl-chr) .ot-sel-all{padding-right:9px}#onetrust-pc-sdk:not(.ot-addtl-vendors) #ot-pc-lst:not(.ot-enbl-chr) .ot-tgl-cntr{right:0}#onetrust-pc-sdk .ot-btn-container.ot-button-order-container .ot-pc-refuse-all-handler,#onetrust-pc-sdk .ot-btn-container.ot-button-order-container .save-preference-btn-handler,#onetrust-pc-sdk .ot-btn-container.ot-button-order-container #accept-recommended-btn-handler{width:100%}}@media(min-width: 768px){#onetrust-pc-sdk.ot-tgl-with-label .ot-label-status{display:inline}#onetrust-pc-sdk.ot-tgl-with-label #ot-pc-lst .ot-label-status{display:none}}@media only screen and (max-width: 896px)and (max-height: 425px)and (orientation: landscape){#onetrust-pc-sdk.otPcCenter{left:0;top:0;min-width:100%;height:100%;border-radius:0}#onetrust-pc-sdk .ot-pc-header{height:auto;min-height:20px}#onetrust-pc-sdk .ot-pc-header .ot-pc-logo{max-height:30px}#onetrust-pc-sdk .ot-pc-footer{max-height:90px;height:100% !important}#onetrust-pc-sdk .ot-pc-footer .ot-btn-container{overflow-y:auto;overflow-x:hidden;max-height:calc(100% - 30px)}#onetrust-pc-sdk #ot-pc-content,#onetrust-pc-sdk #ot-pc-lst{bottom:100px}#onetrust-pc-sdk.ot-ftr-stacked #ot-pc-content,#onetrust-pc-sdk.ot-ftr-stacked #ot-pc-lst{bottom:100px}#onetrust-pc-sdk #ot-anchor{left:initial;right:50px}#onetrust-pc-sdk #ot-lst-title{margin-top:12px}#onetrust-pc-sdk #ot-lst-title *{font-size:inherit}#onetrust-pc-sdk #ot-pc-hdr input{margin-right:0;padding-right:45px}#onetrust-pc-sdk .switch+p{max-width:85%}#onetrust-pc-sdk #ot-sel-blk{position:static}#onetrust-pc-sdk #ot-pc-lst{overflow:auto}#onetrust-pc-sdk #ot-lst-cnt{max-height:none;overflow:initial}#onetrust-pc-sdk #ot-lst-cnt.no-results{height:auto}#onetrust-pc-sdk input{font-size:1em !important}#onetrust-pc-sdk p{font-size:.6em}#onetrust-pc-sdk #ot-fltr-modal{width:100%;top:0}#onetrust-pc-sdk ul li p,#onetrust-pc-sdk .category-vendors-list-handler,#onetrust-pc-sdk .category-vendors-list-handler+a,#onetrust-pc-sdk .category-host-list-handler{font-size:.6em}#onetrust-pc-sdk.ot-shw-fltr #ot-anchor{display:none !important}#onetrust-pc-sdk.ot-shw-fltr #ot-pc-lst{height:100% !important;overflow:hidden;top:0px}#onetrust-pc-sdk.ot-shw-fltr #ot-fltr-cnt{margin:0;height:100%;max-height:none;padding:10px;top:0;width:calc(100% - 20px);position:absolute;right:0;left:0;max-width:none}#onetrust-pc-sdk.ot-shw-fltr .ot-fltr-scrlcnt{max-height:calc(100% - 65px)}}
            #onetrust-consent-sdk #onetrust-pc-sdk,
                #onetrust-consent-sdk #ot-search-cntr,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-switch.ot-toggle,
                #onetrust-consent-sdk #onetrust-pc-sdk ot-grp-hdr1 .checkbox,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-title:after
                ,#onetrust-consent-sdk #onetrust-pc-sdk #ot-sel-blk,
                        #onetrust-consent-sdk #onetrust-pc-sdk #ot-fltr-cnt,
                        #onetrust-consent-sdk #onetrust-pc-sdk #ot-anchor {
                    background-color: #FFFFFF;
                }
               
            #onetrust-consent-sdk #onetrust-pc-sdk h3,
                #onetrust-consent-sdk #onetrust-pc-sdk h4,
                #onetrust-consent-sdk #onetrust-pc-sdk h5,
                #onetrust-consent-sdk #onetrust-pc-sdk h6,
                #onetrust-consent-sdk #onetrust-pc-sdk p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-ven-lst .ot-ven-opts p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-desc,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-title,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-li-title,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-sel-all-hdr span,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-info,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-fltr-modal #modal-header,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-checkbox label span,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst #ot-sel-blk p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst #ot-lst-title h3,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst #ot-lst-title p[aria-level="3"],
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst .back-btn-handler p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst .ot-ven-name,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst #ot-ven-lst .consent-category,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-inactive-leg-btn,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-label-status,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-chkbox label span,
                #onetrust-consent-sdk #onetrust-pc-sdk #clear-filters-handler,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-optout-signal
                {
                    color: #1A1A1A;
                }
             #onetrust-consent-sdk #onetrust-pc-sdk .privacy-notice-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-pgph-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk .category-vendors-list-handler,
                    #onetrust-consent-sdk #onetrust-pc-sdk .category-vendors-list-handler + a,
                    #onetrust-consent-sdk #onetrust-pc-sdk .category-host-list-handler,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-ven-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-ven-legclaim-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-name a,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-acc-hdr .ot-host-expand,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-info a,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-content #ot-pc-desc .ot-link-btn,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info a,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info a,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-desc a
                    {
                        color: #000000;
                    }
            #onetrust-consent-sdk #onetrust-pc-sdk .category-vendors-list-handler:hover { text-decoration: underline;}
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-grpcntr.ot-acc-txt,
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-txt .ot-subgrp-tgl .ot-switch.ot-toggle
             {
                background-color: #FFFFFF;
            }
             #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-info,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-txt .ot-ven-dets
                            {
                                background-color: #FFFFFF;
                            }
        #onetrust-consent-sdk #onetrust-pc-sdk
            button:not(#clear-filters-handler):not(.ot-close-icon):not(#filter-btn-handler):not(.ot-remove-objection-handler):not(.ot-obj-leg-btn-handler):not([aria-expanded]):not(.ot-link-btn),
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-active-leg-btn {
                background-color: #76B900;border-color: #76B900;
                color: #000000;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-active-menu {
                border-color: #76B900;
            }
            
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-remove-objection-handler{
                background-color: transparent;
                border: 1px solid transparent;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-inactive-leg-btn {
                background-color: #FFFFFF;
                color: #4D4D4D; border-color: #4D4D4D;
            }#onetrust-consent-sdk #onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob {
                background-color: #76B900;
            }
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-switch-nob {
                    background-color: #767676;
                }/** NV Extra Cautious Preference styles start **/
#onetrust-pc-sdk #ot-pc-desc a {
    margin-left: 0px;
    display: inline;
    color: inherit;
    text-decoration: underline;
    text-decoration-color: currentcolor;
    text-decoration-thickness: auto;
    -webkit-text-decoration-color: #76b900;
    text-decoration-color: #76b900;
    text-decoration-thickness: 2px;
    text-underline-offset: .3125em;

}
#onetrust-pc-sdk #ot-pc-desc a:hover {
    text-decoration-color: #000000;
}
#onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-desc {
    font-size: 15px;
}

#onetrust-consent-sdk #onetrust-pc-sdk .category-host-list-handler {
     color: #000;
    padding-bottom: 2px;
    border-bottom: 2px solid #76b900;
    text-decoration: none;
    letter-spacing: normal;
    font-size: 15px;
    padding-top: 15px;
}
#onetrust-consent-sdk #onetrust-pc-sdk .category-host-list-handler:hover {
    border-bottom: 2px solid #000;
}
#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob {
    background-color: #76b900
}


#onetrust-pc-sdk #filter-btn-handler {
    background-color: #76b900;
    border: 1px solid #76b900;
    opacity: 1;
}
#onetrust-pc-sdk #filter-btn-handler:hover {
    background-color: #91c733;
    border: 1px solid #91c733;
}

#onetrust-pc-sdk .ot-pc-footer .ot-btn-container button.ot-pc-refuse-all-handler,
#onetrust-pc-sdk .ot-pc-footer .ot-btn-container  button.save-preference-btn-handler {
    background-color: #fff !important;
    border: 2px solid #76B900;
    font-size: 16px;
    padding: 11px 13px !important;
    letter-spacing: inherit;
}

#onetrust-pc-sdk .ot-pc-footer .ot-btn-container button.ot-pc-refuse-all-handler:hover,
#onetrust-pc-sdk .ot-pc-footer .ot-btn-container  button.save-preference-btn-handler:hover {
    border: 2px solid #000 !important;
    opacity: 1;
}

#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item .ot-category-desc,
#onetrust-consent-sdk #onetrust-pc-sdk h4.ot-cat-header,
#onetrust-pc-sdk .ot-host-opt li>div div,
#onetrust-pc-sdk #ot-host-lst .ot-host-name, 
#onetrust-pc-sdk #ot-host-lst .ot-host-name a {
    font-size: 15px;   
    color: #000000;
}

#onetrust-consent-sdk #onetrust-pc-sdk *:focus,
#onetrust-consent-sdk #onetrust-pc-sdk .ot-vlst-cntr > a:focus {
    outline-width: 0px;  
}

#onetrust-banner-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):hover,
#onetrust-banner-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):focus,
#onetrust-pc-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):focus,
#ot-sdk-cookie-policy :not(.ot-leg-btn-container)>button:not(.ot-link-btn):focus{
    border: 0;
}
#onetrust-pc-sdk .ot-cat-grp .ot-always-active {
    color: #000000;
}
#ot-fltr-cnt .ot-fltr-btns #filter-apply-handler,
#ot-fltr-cnt .ot-fltr-btns #filter-cancel-handler{
    border: 2px solid #76b900 !important;
    background-color: transparent !important;
}
#ot-fltr-cnt .ot-fltr-btns #filter-apply-handler:hover,
#ot-fltr-cnt .ot-fltr-btns #filter-cancel-handler:hover {
    border: 2px solid #000000 !important;
    background-color: transparent !important;
}
#onetrust-pc-sdk .ot-chkbox input:checked ~ label::before,
#onetrust-pc-sdk #filter-btn-handler {
  background-color: #76b900;
}
#onetrust-pc-sdk .ot-chkbox label::before {
border: 2px solid #76b900;
}
#onetrust-pc-sdk #ot-pc-lst {
   bottom: 165px;
}
/** NV Extra Cautious Preference styles start **/.ot-sdk-cookie-policy{font-family:inherit;font-size:16px}.ot-sdk-cookie-policy.otRelFont{font-size:1rem}.ot-sdk-cookie-policy h3,.ot-sdk-cookie-policy h4,.ot-sdk-cookie-policy h6,.ot-sdk-cookie-policy p,.ot-sdk-cookie-policy li,.ot-sdk-cookie-policy a,.ot-sdk-cookie-policy th,.ot-sdk-cookie-policy #cookie-policy-description,.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group,.ot-sdk-cookie-policy #cookie-policy-title{color:dimgray}.ot-sdk-cookie-policy #cookie-policy-description{margin-bottom:1em}.ot-sdk-cookie-policy h4{font-size:1.2em}.ot-sdk-cookie-policy h6{font-size:1em;margin-top:2em}.ot-sdk-cookie-policy th{min-width:75px}.ot-sdk-cookie-policy a,.ot-sdk-cookie-policy a:hover{background:#fff}.ot-sdk-cookie-policy thead{background-color:#f6f6f4;font-weight:bold}.ot-sdk-cookie-policy .ot-mobile-border{display:none}.ot-sdk-cookie-policy section{margin-bottom:2em}.ot-sdk-cookie-policy table{border-collapse:inherit}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy{font-family:inherit;font-size:1rem}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h3,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h4,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h6,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy p,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy li,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-title{color:dimgray}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description{margin-bottom:1em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-subgroup{margin-left:1.5em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group-desc,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-table-header,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy span,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td{font-size:.9em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td span,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td a{font-size:inherit}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group{font-size:1em;margin-bottom:.6em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-title{margin-bottom:1.2em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy>section{margin-bottom:1em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th{min-width:75px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a:hover{background:#fff}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy thead{background-color:#f6f6f4;font-weight:bold}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-mobile-border{display:none}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy section{margin-bottom:2em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-subgroup ul li{list-style:disc;margin-left:1.5em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-subgroup ul li h4{display:inline-block}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table{border-collapse:inherit;margin:auto;border:1px solid #d7d7d7;border-radius:5px;border-spacing:initial;width:100%;overflow:hidden}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table th,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table td{border-bottom:1px solid #d7d7d7;border-right:1px solid #d7d7d7}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr:last-child td{border-bottom:0px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr th:last-child,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr td:last-child{border-right:0px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-host,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-cookies-type{width:25%}.ot-sdk-cookie-policy[dir=rtl]{text-align:left}#ot-sdk-cookie-policy h3{font-size:1.5em}@media only screen and (max-width: 530px){.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) table,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) thead,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tbody,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) th,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr{display:block}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) thead tr{position:absolute;top:-9999px;left:-9999px}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr{margin:0 0 1em 0}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr:nth-child(odd),.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr:nth-child(odd) a{background:#f6f6f4}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td{border:none;border-bottom:1px solid #eee;position:relative;padding-left:50%}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td:before{position:absolute;height:100%;left:6px;width:40%;padding-right:10px}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) .ot-mobile-border{display:inline-block;background-color:#e4e4e4;position:absolute;height:100%;top:0;left:45%;width:2px}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td:before{content:attr(data-label);font-weight:bold}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) li{word-break:break-word;word-wrap:break-word}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table{overflow:hidden}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table td{border:none;border-bottom:1px solid #d7d7d7}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy thead,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy tbody,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy tr{display:block}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-host,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-cookies-type{width:auto}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy tr{margin:0 0 1em 0}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td:before{height:100%;width:40%;padding-right:10px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td:before{content:attr(data-label);font-weight:bold}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy li{word-break:break-word;word-wrap:break-word}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy thead tr{position:absolute;top:-9999px;left:-9999px;z-index:-9999}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr:last-child td{border-bottom:1px solid #d7d7d7;border-right:0px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr:last-child td:last-child{border-bottom:0px}}
                
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h5,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h6,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy li,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy p,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy span,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description {
                        color: #000000;
                    }
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th {
                        color: #FFFFFF;
                    }
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group {
                        color: #000000;
                    }
                    
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-title {
                            color: #000000;
                        }
                    
            
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table th {
                            background-color: #666666;
                        }
                    
            #ot-sdk-cookie-policy-v2 section table tbody tr:nth-child(2n+1) {
background-color: #EEEEEE;
}
#ot-sdk-cookie-policy-v2 section table tbody tr:nth-child(2n+2) {
background-color: #f7f7f7;
}.ot-floating-button__front{background-image:url('https://cdn.cookielaw.org/logos/static/ot_persistent_cookie.png')}</style><link rel="preload" as="image" fetchpriority="high" imagesrcset="/_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fnemo-data-designer.jpg&amp;w=640&amp;q=75 640w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fnemo-data-designer.jpg&amp;w=750&amp;q=75 750w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fnemo-data-designer.jpg&amp;w=828&amp;q=75 828w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fnemo-data-designer.jpg&amp;w=1080&amp;q=75 1080w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fnemo-data-designer.jpg&amp;w=1200&amp;q=75 1200w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fnemo-data-designer.jpg&amp;w=1920&amp;q=75 1920w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fnemo-data-designer.jpg&amp;w=2048&amp;q=75 2048w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fnemo-data-designer.jpg&amp;w=3840&amp;q=75 3840w" imagesizes="100vw"><script async="" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/hotjar-3655182.js.下载"></script><script async="" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/hotjar-3655182.js.下载"></script><link rel="preload" as="image" fetchpriority="high" imagesrcset="/_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-4b-instruct.jpg&amp;w=640&amp;q=75 640w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-4b-instruct.jpg&amp;w=750&amp;q=75 750w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-4b-instruct.jpg&amp;w=828&amp;q=75 828w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-4b-instruct.jpg&amp;w=1080&amp;q=75 1080w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-4b-instruct.jpg&amp;w=1200&amp;q=75 1200w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-4b-instruct.jpg&amp;w=1920&amp;q=75 1920w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-4b-instruct.jpg&amp;w=2048&amp;q=75 2048w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-4b-instruct.jpg&amp;w=3840&amp;q=75 3840w" imagesizes="100vw"><script async="" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/hotjar-3655182.js.下载"></script><script async="" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/hotjar-3655182.js.下载"></script><script async="" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/hotjar-3655182.js.下载"></script><script async="" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/hotjar-3655182.js.下载"></script><script async="" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/hotjar-3655182.js.下载"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>Explore Speech Models | Try NVIDIA NIM APIs</title><meta name="description" content="Experience the leading models to build enterprise generative AI apps now."><link rel="canonical" href="https://build.nvidia.com/explore/speech"><meta property="og:title" content="Explore Speech Models | Try NVIDIA NIM APIs"><meta property="og:description" content="Experience the leading models to build enterprise generative AI apps now."><meta property="og:image:type" content="image/jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="og:image" content="https://build.nvidia.com/opengraph-image.jpg?6ec102a0470b935b"><meta property="og:type" content="website"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="Explore Speech Models | Try NVIDIA NIM APIs"><meta name="twitter:description" content="Experience the leading models to build enterprise generative AI apps now."><meta name="twitter:image:type" content="image/jpeg"><meta name="twitter:image:width" content="1200"><meta name="twitter:image:height" content="630"><meta name="twitter:image" content="https://build.nvidia.com/twitter-image.jpg?6ec102a0470b935b"><link rel="icon" href="https://build.nvidia.com/favicon.ico" type="image/x-icon" sizes="48x48"><script async="" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/hotjar-3655182.js.下载"></script></head><body class="bg-manitoulinLightBlack" data-scroll-lock="false"><main><!--$--><section aria-label="Notifications alt+T" tabindex="-1" aria-live="polite" aria-relevant="additions text" aria-atomic="false"></section><div class="group sticky top-0 z-50 w-full transition-colors duration-300 border-b border-n700 bg-manitoulinDarkBlack" id="app-bar"><div class="relative flex min-h-12 gap-sm px-4"><div class="flex items-center"><button aria-label="Open Menu" class="gap-2 text-center font-sans font-bold leading-text flex-row btn-tertiary btn-md btn-rounded btn-icon-only mr-4 flex w-8 items-center justify-center p-0 text-manitoulinLightWhite lg:hidden [&amp;_[data-icon-name]]:size-6"><svg data-src="https://brand-assets.cne.ngc.nvidia.com/assets/icons/3.8.0/fill/menu.svg" height="1em" width="1em" display="inline-block" data-icon-name="menu" data-cache="disabled" class="btn-icon" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" data-id="svg-loader_1"><symbol id="menu_1" viewBox="0 0 16 16"><path fill="currentColor" d="M13,4h-10v-1h10zM13,8h-10v-1h10zM13,12h-10v-1h10z"></path></symbol><use href="#menu_1"></use></svg></button><a class="w-[84px]" href="https://build.nvidia.com/"><img alt="NVIDIA" loading="lazy" width="256" height="47" decoding="async" data-nimg="1" class="object-cover" style="color:transparent" srcset="/_next/image?url=%2Fnvidia-logo.png&amp;w=300&amp;q=75 1x, /_next/image?url=%2Fnvidia-logo.png&amp;w=600&amp;q=75 2x" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/nvidia-logo.webp"></a></div><nav class="ml-8 mr-2 hidden items-center justify-start lg:flex"><div class="hidden items-center lg:flex"><a aria-selected="true" class="group whitespace-nowrap text-ms font-normal text-tw600 hover:text-n000 px-4 aria-selected:font-medium aria-selected:text-manitoulinLightWhite aria-expanded:text-n000 [[role=menuitem]]:flex [[role=menuitem]]:justify-start" href="https://build.nvidia.com/explore/discover"><div class="relative inline-flex min-h-8 items-center gap-xs text-ms">Explore<!-- --> <div class="group-aria-selected:bg-brand absolute bottom-0 h-0.5 w-full bg-transparent"></div></div></a><a aria-selected="false" class="group whitespace-nowrap text-ms font-normal text-tw600 hover:text-n000 px-4 aria-selected:font-medium aria-selected:text-manitoulinLightWhite aria-expanded:text-n000 [[role=menuitem]]:flex [[role=menuitem]]:justify-start" href="https://build.nvidia.com/models"><div class="relative inline-flex min-h-8 items-center gap-xs text-ms">Models<!-- --> <div class="group-aria-selected:bg-brand absolute bottom-0 h-0.5 w-full bg-transparent"></div></div></a><a aria-selected="false" class="group whitespace-nowrap text-ms font-normal text-tw600 hover:text-n000 px-4 aria-selected:font-medium aria-selected:text-manitoulinLightWhite aria-expanded:text-n000 [[role=menuitem]]:flex [[role=menuitem]]:justify-start" href="https://build.nvidia.com/blueprints"><div class="relative inline-flex min-h-8 items-center gap-xs text-ms">Blueprints<!-- --> <div class="group-aria-selected:bg-brand absolute bottom-0 h-0.5 w-full bg-transparent"></div></div></a><a aria-selected="false" class="group whitespace-nowrap text-ms font-normal text-tw600 hover:text-n000 px-4 aria-selected:font-medium aria-selected:text-manitoulinLightWhite aria-expanded:text-n000 [[role=menuitem]]:flex [[role=menuitem]]:justify-start" href="https://brev.nvidia.com/environment/new/public"><div class="relative inline-flex min-h-8 items-center gap-xs text-ms">GPUs<!-- --> <div class="group-aria-selected:bg-brand absolute bottom-0 h-0.5 w-full bg-transparent"></div></div></a><a aria-selected="false" class="group whitespace-nowrap text-ms font-normal text-tw600 hover:text-n000 px-4 aria-selected:font-medium aria-selected:text-manitoulinLightWhite aria-expanded:text-n000 [[role=menuitem]]:flex [[role=menuitem]]:justify-start" target="_blank" href="https://docs.api.nvidia.com/"><div class="relative inline-flex min-h-8 items-center gap-xs text-ms">Docs<!-- --> <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 16" width="1em" height="1em" display="inline-block" data-icon-name="open-external"><path fill="currentColor" d="M14 7h-1V3.707L7.354 9.354l-.708-.708L12.293 3H9V2h5z"></path><path fill="currentColor" d="M7 4H3v9h9V9h1v5H2V3h5z"></path></svg><div class="group-aria-selected:bg-brand absolute bottom-0 h-0.5 w-full bg-transparent"></div></div></a></div></nav><div class="flex w-full items-center justify-end gap-sm"><div class="relative flex h-full flex-1 items-center justify-end pl-2 lg:max-w-[320px] max-lg:hidden"><div class="nv-input nv-input--size-small nv-input--kind-flat nv-text-input-root group w-full" data-placeholder="true" data-state="closed" data-testid="nv-text-input-root" title="Search"><div class="nv-input-content"><input class="nv-text-input-element" data-testid="nv-text-input-element" type="text" autocomplete="off" placeholder="Search" value=""></div><kbd class="font-sans text-sm font-medium text-input-placeholder group-focus-within:hidden pointer-coarse:hidden"><span class="hidden os-mac:contents">⌘K</span><span class="contents os-mac:hidden">Ctrl+K</span></kbd><div tabindex="0" role="button" class="nv-input-dismiss-button" aria-label="Clear" data-dismiss="true"><i class="nv-icon nv-icon-close" role="img" aria-hidden="true"></i></div></div></div><div class="flex items-center gap-sm"><button aria-label="Search" class="items-center justify-center gap-2 text-center font-sans font-bold leading-text flex-row btn-plain btn-md btn-rounded block py-0 lg:hidden"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 16" width="24" height="24" display="inline-block" data-icon-name="magnifying-glass"><path fill="currentColor" d="M6.5 3a3.5 3.5 0 1 0 0 7 3.5 3.5 0 0 0 0-7M2 6.5a4.5 4.5 0 1 1 8.016 2.809l4.338 4.338-.707.707-4.338-4.338A4.5 4.5 0 0 1 2 6.5"></path></svg></button><button aria-label="Help" type="button" id="radix-:Rlrefhkq:" aria-haspopup="menu" aria-expanded="false" data-state="closed" data-testid="nv-dropdown-trigger" class="inline-flex items-center justify-center gap-2 text-center font-sans flex-row btn-tertiary btn-md btn-rounded btn-inverse btn-icon-only nv-dropdown-trigger py-0 text-ms font-bold">?</button><div class="flex items-center"><button data-nvtrack="Login menu click" data-nvtrack-adobe-type="ctaButtonClick" data-nvtrack-location="header" class="inline-flex items-center justify-center gap-2 text-center font-sans font-bold leading-text flex-row btn-tertiary btn-sm btn-rounded btn-inverse">Login</button></div></div></div></div></div><div class="flex min-h-[calc(100dvh-var(--app-bar-height))] flex-col items-center"><div class="w-full flex-1"><div class="mb-4 border-b border-n700 p-4 lg:p-8"><div class="mx-auto flex max-w-[1600px] flex-col gap-4"><div class="flex flex-col gap-4 min-[400px]:flex-row"><div class="flex flex-1 flex-col justify-between gap-4 rounded-2 border border-gray-700 bg-n900 p-4 shadow-[0_4px_6px_0_rgba(0,0,0,0.12)]"><div class="flex justify-between gap-2"><div class="flex flex-col flex-wrap gap-2 self-start xs:flex-row xs:items-center"><h2 class="tracking-less flex items-center gap-1 text-md font-medium leading-text text-primary xs:gap-2"><span class="-mb-1 inline-block font-bold leading-heading max-lg:text-sm">Deploy Models Now with NVIDIA NIM</span></h2><span class="text-sm font-normal leading-body">Optimized inference for the world’s leading models</span></div><button data-nvtrack="Get API Key" data-nvtrack-model="" data-nvtrack-adobe-type="modelEngagementClick" class="inline-flex items-center justify-center gap-2 text-center font-sans font-bold leading-text flex-row btn-primary btn-md btn-rounded self-start text-nowrap">Get API Key</button></div><div class="nv-flex nv-flex--align-stretch nv-flex--direction-col nv-flex--justify-start nv-flex--wrap-nowrap min-h-8 flex-1 flex-row gap-2 rounded-2 bg-black/50 px-2 py-1.5 text-sm leading-body xs:px-4 max-[1056px]:flex-col max-[1056px]:p-2" data-testid="nv-flex" role="list"><div class="flex items-center justify-center self-stretch flex-initial flex-wrap max-[1056px]:justify-start"><div class="mr-2 flex gap-2 text-nowrap" role="listitem"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 16" width="16" height="16" display="inline-block" data-icon-name="check" class="mt-px text-brand"><path fill="currentColor" d="M14.354 4.354 6 12.707 1.646 8.354l.708-.708L6 11.293l7.647-7.647z"></path></svg><div class="flex flex-wrap items-center gap-2">Free serverless APIs for development<div class="relative isolate w-fit rounded-full" style="--border-width: 1px;"><a target="_blank" class="nv-tag nv-tag--color-yellow nv-density-compact nv-tag--kind-outline relative h-full w-full rounded-full whitespace-nowrap border-transparent text-[#EBE4BB]" data-testid="nv-tag-root" href="https://www.nvidia.com/en-us/data-center/dgx-cloud/"><svg class="absolute" height="0" width="0"><defs><lineargradient id=":r59:" x1="0%" x2="100%" y1="0%" y2="0%"><stop offset="0%" stop-color="#EBE4BB"></stop><stop offset="100%" stop-color="#958146"></stop></lineargradient></defs></svg><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 16" width="1em" height="1em" display="inline-block" data-icon-name="cloud"><path fill="url(#:r59:)" d="M8 2a4.5 4.5 0 0 1 4.14 2.732A4.25 4.25 0 0 1 10.75 13H4.5a3.501 3.501 0 0 1-.986-6.86A4.5 4.5 0 0 1 8 2"></path></svg><span class="bg-gradient-to-r from-[#EBE4BB] to-[#958146] bg-clip-text text-transparent">Accelerated by DGX Cloud</span></a><div class="style_borderGradient__89Jrj bg-linear-to-r from-[#EBE4BB] to-[#958146]" data-gradient="true"></div></div></div></div></div><div aria-orientation="vertical" class="bg-n700 w-px mx-2 max-[1056px]:hidden" role="separator"></div><div class="flex flex-1 items-center justify-center self-stretch max-[1056px]:justify-start"><div class="flex gap-2 text-nowrap" role="listitem"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 16" width="16" height="16" display="inline-block" data-icon-name="check" class="mt-px text-brand"><path fill="currentColor" d="M14.354 4.354 6 12.707 1.646 8.354l.708-.708L6 11.293l7.647-7.647z"></path></svg>Self-Host on your GPU infrastructure</div></div><div aria-orientation="vertical" class="bg-n700 w-px mx-2 max-[1056px]:hidden" role="separator"></div><div class="flex flex-1 items-center justify-center self-stretch max-[1056px]:justify-start"><div class="flex gap-2 text-nowrap" role="listitem"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 16" width="16" height="16" display="inline-block" data-icon-name="check" class="mt-px text-brand"><path fill="currentColor" d="M14.354 4.354 6 12.707 1.646 8.354l.708-.708L6 11.293l7.647-7.647z"></path></svg>Continuous vulnerability fixes</div></div></div></div></div></div></div><div class="p-4 lg:p-8"><div class="mx-auto flex max-w-[1600px]"><div class="hidden w-[220px] lg:block"><div class="w-full"><nav class="mb-6"><a class="relative flex items-center justify-between px-4 py-2.5 hover:bg-white/10 hover:text-primary text-sm font-bold uppercase tracking-more text-primary" data-nvtrack="Left Sidebar click" data-nvtrack-category-name="/explore/discover" data-nvtrack-adobe-type="leftNavBarClick" href="https://build.nvidia.com/explore/discover">Discover</a><a class="relative flex items-center justify-between px-4 py-2.5 hover:bg-white/10 hover:text-primary text-sm font-bold uppercase tracking-more text-primary lg:hidden" data-nvtrack="Left Sidebar click" data-nvtrack-category-name="/models" data-nvtrack-adobe-type="leftNavBarClick" href="https://build.nvidia.com/models">Models</a><a class="relative flex items-center justify-between px-4 py-2.5 hover:bg-white/10 hover:text-primary text-sm font-bold uppercase tracking-more text-primary lg:hidden" data-nvtrack="Left Sidebar click" data-nvtrack-category-name="/blueprints" data-nvtrack-adobe-type="leftNavBarClick" href="https://build.nvidia.com/blueprints">Blueprints</a><a class="relative flex items-center justify-between px-4 py-2.5 hover:bg-white/10 hover:text-primary text-sm font-bold uppercase tracking-more text-primary lg:hidden" data-nvtrack="Left Sidebar click" data-nvtrack-category-name="https://brev.nvidia.com/environment/new/public" data-nvtrack-adobe-type="leftNavBarClick" href="https://brev.nvidia.com/environment/new/public">GPUs</a><a class="relative flex items-center justify-between px-4 py-2.5 hover:bg-white/10 hover:text-primary text-sm font-bold uppercase tracking-more text-primary lg:hidden" target="_blank" data-nvtrack="Left Sidebar click" data-nvtrack-category-name="https://docs.api.nvidia.com/" data-nvtrack-adobe-type="leftNavBarClick" href="https://docs.api.nvidia.com/">Docs<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 16" width="1em" height="1em" display="inline-block" data-icon-name="open-external" class="text-current"><path fill="currentColor" d="M14 7h-1V3.707L7.354 9.354l-.708-.708L12.293 3H9V2h5z"></path><path fill="currentColor" d="M7 4H3v9h9V9h1v5H2V3h5z"></path></svg></a><a class="relative flex items-center justify-between px-4 py-2.5 hover:bg-white/10 hover:text-primary text-sm font-bold uppercase tracking-more text-primary lg:hidden" target="_blank" data-nvtrack="Docs click" data-nvtrack-category-name="https://forums.developer.nvidia.com/c/ai-data-science/nvidia-nim/678" data-nvtrack-adobe-type="ctaButtonClick" href="https://forums.developer.nvidia.com/c/ai-data-science/nvidia-nim/678">Forums<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 16" width="1em" height="1em" display="inline-block" data-icon-name="open-external" class="text-current"><path fill="currentColor" d="M14 7h-1V3.707L7.354 9.354l-.708-.708L12.293 3H9V2h5z"></path><path fill="currentColor" d="M7 4H3v9h9V9h1v5H2V3h5z"></path></svg></a></nav><div class="flex flex-col gap-6"><nav aria-labelledby=":r5a:" pathname="/explore/speech" class="flex flex-col"><h2 class="px-4 py-5 text-md font-normal capitalize leading-text text-n500" id=":r5a:"><div class="flex items-center gap-2">workstations</div></h2><ul class="flex flex-col"><li><a class="relative flex items-center justify-between px-4 py-2.5 text-ms text-manitoulinLightWhite hover:bg-white/10 hover:text-primary" data-nvtrack="Left Sidebar click" data-nvtrack-category-name="/explore/run-on-rtx" data-nvtrack-adobe-type="leftNavBarClick" href="https://build.nvidia.com/explore/run-on-rtx">Run on RTX</a></li><li><a class="relative flex items-center justify-between px-4 py-2.5 text-ms text-manitoulinLightWhite hover:bg-white/10 hover:text-primary" data-nvtrack="Left Sidebar click" data-nvtrack-category-name="/spark" data-nvtrack-adobe-type="leftNavBarClick" href="https://build.nvidia.com/spark">Run on Spark</a></li></ul></nav><nav aria-labelledby=":r5b:" pathname="/explore/speech" class="flex flex-col"><h2 class="px-4 py-5 text-md font-normal capitalize leading-text text-n500" id=":r5b:"><div class="flex items-center gap-2">models</div></h2><ul class="flex flex-col"><li><a class="relative flex items-center justify-between px-4 py-2.5 text-ms text-manitoulinLightWhite hover:bg-white/10 hover:text-primary" data-nvtrack="Left Sidebar click" data-nvtrack-category-name="/explore/reasoning" data-nvtrack-adobe-type="leftNavBarClick" href="https://build.nvidia.com/explore/reasoning">Reasoning</a></li><li><a class="relative flex items-center justify-between px-4 py-2.5 text-ms text-manitoulinLightWhite hover:bg-white/10 hover:text-primary" data-nvtrack="Left Sidebar click" data-nvtrack-category-name="/explore/vision" data-nvtrack-adobe-type="leftNavBarClick" href="https://build.nvidia.com/explore/vision">Vision</a></li><li><a class="relative flex items-center justify-between px-4 py-2.5 text-ms text-manitoulinLightWhite hover:bg-white/10 hover:text-primary" data-nvtrack="Left Sidebar click" data-nvtrack-category-name="/explore/visual-design" data-nvtrack-adobe-type="leftNavBarClick" href="https://build.nvidia.com/explore/visual-design">Visual Design</a></li><li><a class="relative flex items-center justify-between px-4 py-2.5 text-ms text-manitoulinLightWhite hover:bg-white/10 hover:text-primary" data-nvtrack="Left Sidebar click" data-nvtrack-category-name="/explore/retrieval" data-nvtrack-adobe-type="leftNavBarClick" href="https://build.nvidia.com/explore/retrieval">Retrieval</a></li><li><a aria-current="page" class="relative flex items-center justify-between px-4 py-2.5 text-ms text-manitoulinLightWhite hover:bg-white/10 hover:text-primary cursor-default bg-gradient-to-r from-transparent to-brand/25 bg-[length:50%] bg-right bg-no-repeat after:absolute after:right-[0] after:top-[0] after:block after:h-full after:w-0.5 after:bg-gradient-to-t after:from-brand after:to-g200" data-nvtrack="Left Sidebar click" data-nvtrack-category-name="/explore/speech" data-nvtrack-adobe-type="leftNavBarClick" href="https://build.nvidia.com/explore/speech">Speech</a></li><li><a class="relative flex items-center justify-between px-4 py-2.5 text-ms text-manitoulinLightWhite hover:bg-white/10 hover:text-primary" data-nvtrack="Left Sidebar click" data-nvtrack-category-name="/explore/biology" data-nvtrack-adobe-type="leftNavBarClick" href="https://build.nvidia.com/explore/biology">Biology</a></li><li><a class="relative flex items-center justify-between px-4 py-2.5 text-ms text-manitoulinLightWhite hover:bg-white/10 hover:text-primary" data-nvtrack="Left Sidebar click" data-nvtrack-category-name="/explore/simulation" data-nvtrack-adobe-type="leftNavBarClick" href="https://build.nvidia.com/explore/simulation">Simulation</a></li><li><a class="relative flex items-center justify-between px-4 py-2.5 text-ms text-manitoulinLightWhite hover:bg-white/10 hover:text-primary" data-nvtrack="Left Sidebar click" data-nvtrack-category-name="/explore/climate-weather" data-nvtrack-adobe-type="leftNavBarClick" href="https://build.nvidia.com/explore/climate-weather">Climate &amp; Weather</a></li><li><a class="relative flex items-center justify-between px-4 py-2.5 text-ms text-manitoulinLightWhite hover:bg-white/10 hover:text-primary" data-nvtrack="Left Sidebar click" data-nvtrack-category-name="/explore/safety-moderation" data-nvtrack-adobe-type="leftNavBarClick" href="https://build.nvidia.com/explore/safety-moderation">Safety &amp; Moderation</a></li></ul></nav><nav aria-labelledby=":r5c:" pathname="/explore/speech" class="flex flex-col"><h2 class="px-4 py-5 text-md font-normal capitalize leading-text text-n500" id=":r5c:"><div class="flex items-center gap-2">industries</div></h2><ul class="flex flex-col"><li><a class="relative flex items-center justify-between px-4 py-2.5 text-ms text-manitoulinLightWhite hover:bg-white/10 hover:text-primary" data-nvtrack="Left Sidebar click" data-nvtrack-category-name="/explore/automotive" data-nvtrack-adobe-type="leftNavBarClick" href="https://build.nvidia.com/explore/automotive">Automotive</a></li><li><a class="relative flex items-center justify-between px-4 py-2.5 text-ms text-manitoulinLightWhite hover:bg-white/10 hover:text-primary" data-nvtrack="Left Sidebar click" data-nvtrack-category-name="/explore/financial-services" data-nvtrack-adobe-type="leftNavBarClick" href="https://build.nvidia.com/explore/financial-services">Financial Services</a></li><li><a class="relative flex items-center justify-between px-4 py-2.5 text-ms text-manitoulinLightWhite hover:bg-white/10 hover:text-primary" data-nvtrack="Left Sidebar click" data-nvtrack-category-name="/explore/gaming" data-nvtrack-adobe-type="leftNavBarClick" href="https://build.nvidia.com/explore/gaming">Gaming</a></li><li><a class="relative flex items-center justify-between px-4 py-2.5 text-ms text-manitoulinLightWhite hover:bg-white/10 hover:text-primary" data-nvtrack="Left Sidebar click" data-nvtrack-category-name="/explore/healthcare" data-nvtrack-adobe-type="leftNavBarClick" href="https://build.nvidia.com/explore/healthcare">Healthcare</a></li><li><a class="relative flex items-center justify-between px-4 py-2.5 text-ms text-manitoulinLightWhite hover:bg-white/10 hover:text-primary" data-nvtrack="Left Sidebar click" data-nvtrack-category-name="/explore/industrial" data-nvtrack-adobe-type="leftNavBarClick" href="https://build.nvidia.com/explore/industrial">Industrial</a></li><li><a class="relative flex items-center justify-between px-4 py-2.5 text-ms text-manitoulinLightWhite hover:bg-white/10 hover:text-primary" data-nvtrack="Left Sidebar click" data-nvtrack-category-name="/explore/robotics" data-nvtrack-adobe-type="leftNavBarClick" href="https://build.nvidia.com/explore/robotics">Robotics</a></li></ul></nav></div></div></div><div class="flex w-full min-w-0 flex-1 flex-col border-n700 lg:border-l lg:pl-8"><h2 class="tracking-less mb-8 text-lg font-medium lg:text-xl">Speech</h2><div class="nv-flex nv-flex--align-stretch nv-flex--direction-col nv-flex--justify-start nv-flex--wrap-nowrap flex flex-col gap-8" data-testid="nv-flex"><section data-testid="explore-carousel-view"><div class="relative"><div class="h-fit w-full" data-testid="carousel-view"><section class="relative"><header class="mb-2.5 flex items-end justify-between gap-2.5 text-primary"><div class="mb-2 flex items-start gap-2 max-xs:justify-between"><h2 class="text-ml font-medium leading-body tracking-less text-manitoulinLightWhite mb-0">Automatic Speech Recognition (ASR)</h2></div><p class="text-md font-normal text-manitoulinLightGray mb-0">Low Latency NVIDIA Nemotron Speech transcription models for your agentic AI workflows.</p> <div class="max-sm:hidden! ml-auto flex gap-2 justify-self-end"><button aria-label="Previous Slide" type="button" class="inline-flex items-center justify-center gap-2 text-center font-sans font-bold leading-text flex-row btn-secondary btn-md btn-pill btn-inverse btn-icon-only min-h-8 min-w-8"><svg data-src="https://brand-assets.cne.ngc.nvidia.com/assets/icons/3.8.0/fill/chevron-left.svg" height="1em" width="1em" display="inline-block" data-icon-name="chevron-left" data-cache="disabled" class="btn-icon" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" data-id="svg-loader_39"><symbol id="chevron-left_40" viewBox="0 0 16 16"><path fill="currentColor" d="M6.707,8l3.647,3.646l-0.707,0.708l-4.354,-4.354l4.354,-4.354l0.707,0.708z"></path></symbol><use href="#chevron-left_40"></use></svg></button><button aria-label="Next Slide" type="button" class="inline-flex items-center justify-center gap-2 text-center font-sans font-bold leading-text flex-row btn-secondary btn-md btn-pill btn-inverse btn-icon-only min-h-8 min-w-8"><svg data-src="https://brand-assets.cne.ngc.nvidia.com/assets/icons/3.8.0/fill/chevron-right.svg" height="1em" width="1em" display="inline-block" data-icon-name="chevron-right" data-cache="disabled" class="btn-icon" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" data-id="svg-loader_41"><symbol id="chevron-right_42" viewBox="0 0 16 16"><path fill="currentColor" d="M9.293,8l-3.647,-3.646l0.708,-0.708l4.353,4.354l-4.353,4.354l-0.708,-0.708z"></path></symbol><use href="#chevron-right_42"></use></svg></button></div></header><div class="relative overflow-x-clip" data-backup-role="null" role="tablist" data-backup-aria-live="null" aria-live="off" data-backup-aria-orientation="null" aria-orientation="horizontal" data-backup-aria-roledescription="null" aria-roledescription="Carousel" data-backup-aria-multiselectable="null" aria-multiselectable="false"><div class="backface-hidden ml-[calc(var(--spacing-4)*-1)] flex touch-pan-y" data-testid="embla-container" style="transform: translate3d(0px, 0px, 0px);"><div data-testid="carousel-item-parakeet-ctc-0_6b-zh-tw" class="min-w-0 max-w-full flex-none pl-4 *:h-full first:pl-0" data-backup-role="null" role="tabpanel" data-backup-aria-label="null" aria-label="1 of 10" data-backup-aria-roledescription="null" aria-roledescription="Slide" data-backup-aria-hidden="null" aria-hidden="false" data-backup-aria-selected="null" aria-selected="true" data-backup-tabindex="null"><div data-nvtrack="Carousel click" data-nvtrack-adobe-type="carouselItemClick" data-nvtrack-model="/nvidia/parakeet-ctc-0_6b-zh-tw" data-nvtrack-model-name="parakeet-ctc-0_6b-zh-tw" class="style_linkbox__jnr5J style_root__prJCC cursor-pointer"><div class="nv-card-root nv-card-root--interactive nv-card-root--kind-float nv-card-root--layout-vertical" data-testid="nv-card-root" endpointspec="[object Object]"><div class="nv-card-media" data-testid="nv-card-media"><img alt="" loading="lazy" decoding="async" data-nimg="fill" class="object-cover object-center" sizes="(max-width: 768px) 150px, 300px" srcset="/_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-tw.jpg&amp;w=150&amp;q=90 150w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-tw.jpg&amp;w=300&amp;q=90 300w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-tw.jpg&amp;w=350&amp;q=90 350w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-tw.jpg&amp;w=400&amp;q=90 400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-tw.jpg&amp;w=600&amp;q=90 600w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-tw.jpg&amp;w=640&amp;q=90 640w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-tw.jpg&amp;w=700&amp;q=90 700w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-tw.jpg&amp;w=750&amp;q=90 750w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-tw.jpg&amp;w=828&amp;q=90 828w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-tw.jpg&amp;w=1080&amp;q=90 1080w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-tw.jpg&amp;w=1200&amp;q=90 1200w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-tw.jpg&amp;w=1400&amp;q=90 1400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-tw.jpg&amp;w=1920&amp;q=90 1920w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-tw.jpg&amp;w=2048&amp;q=90 2048w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-tw.jpg&amp;w=3840&amp;q=90 3840w" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/parakeet-ctc-0_6b-zh-tw.jpeg" style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"><div class="nv-card-media-header"><div class="flex flex-row gap-2"><span class="nv-badge nv-badge--kind-solid nv-badge--color-purple align-bottom" data-testid="nv-badge">Download Available</span></div></div></div><div class="nv-card-content" data-testid="nv-card-content"><div class="flex flex-col gap-2 overflow-hidden"><h3 class="flex w-full flex-col gap-1"><a class="self-start truncate text-sm font-bold lowercase leading-text text-n300 hover:text-primary" href="https://build.nvidia.com/nvidia" data-backup-tabindex="null">nvidia</a><a title="parakeet-ctc-0.6b-zh-tw" class="style_overlay__cLWaT truncate text-md font-medium leading-heading text-primary" data-linkbox-overlay="true" href="https://build.nvidia.com/nvidia/parakeet-ctc-0_6b-zh-tw" data-backup-tabindex="null">parakeet-ctc-0.6b-zh-tw</a></h3><p class="line-clamp-2 text-sm font-normal leading-body text-n300">Record-setting accuracy and performance for Mandarin Taiwanese English transcriptions.</p></div><div class="relative flex w-full items-center justify-start gap-2 mt-auto min-w-0 max-w-full"><div class="flex items-center gap-2 overflow-hidden"><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=ASR" data-backup-tabindex="null"><span class="inline-block truncate">ASR</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=NVIDIA+NIM" data-backup-tabindex="null"><span class="inline-block truncate">NVIDIA NIM</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=Streaming" data-backup-tabindex="null"><span class="inline-block truncate">Streaming</span></a></div><button data-testid="nv-popover-trigger" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:r5d:" data-state="closed" class="inline-flex items-center justify-center gap-2 text-center font-sans leading-text flex-row btn-tertiary btn-xs btn-pill btn-inverse font-medium" data-backup-tabindex="null">+2</button></div></div></div></div></div><div data-testid="carousel-item-parakeet-tdt-0_6b-v2" class="min-w-0 max-w-full flex-none pl-4 *:h-full first:pl-0" data-backup-role="null" role="tabpanel" data-backup-aria-label="null" aria-label="2 of 10" data-backup-aria-roledescription="null" aria-roledescription="Slide" data-backup-aria-hidden="null" aria-hidden="false" data-backup-aria-selected="null" aria-selected="true" data-backup-tabindex="null"><div data-nvtrack="Carousel click" data-nvtrack-adobe-type="carouselItemClick" data-nvtrack-model="/nvidia/parakeet-tdt-0_6b-v2" data-nvtrack-model-name="parakeet-tdt-0_6b-v2" class="style_linkbox__jnr5J style_root__prJCC cursor-pointer"><div class="nv-card-root nv-card-root--interactive nv-card-root--kind-float nv-card-root--layout-vertical" data-testid="nv-card-root" endpointspec="[object Object]"><div class="nv-card-media" data-testid="nv-card-media"><img alt="" loading="lazy" decoding="async" data-nimg="fill" class="object-cover object-center" sizes="(max-width: 768px) 150px, 300px" srcset="/_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-tdt-0_6b-v2.jpg&amp;w=150&amp;q=90 150w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-tdt-0_6b-v2.jpg&amp;w=300&amp;q=90 300w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-tdt-0_6b-v2.jpg&amp;w=350&amp;q=90 350w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-tdt-0_6b-v2.jpg&amp;w=400&amp;q=90 400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-tdt-0_6b-v2.jpg&amp;w=600&amp;q=90 600w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-tdt-0_6b-v2.jpg&amp;w=640&amp;q=90 640w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-tdt-0_6b-v2.jpg&amp;w=700&amp;q=90 700w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-tdt-0_6b-v2.jpg&amp;w=750&amp;q=90 750w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-tdt-0_6b-v2.jpg&amp;w=828&amp;q=90 828w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-tdt-0_6b-v2.jpg&amp;w=1080&amp;q=90 1080w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-tdt-0_6b-v2.jpg&amp;w=1200&amp;q=90 1200w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-tdt-0_6b-v2.jpg&amp;w=1400&amp;q=90 1400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-tdt-0_6b-v2.jpg&amp;w=1920&amp;q=90 1920w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-tdt-0_6b-v2.jpg&amp;w=2048&amp;q=90 2048w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-tdt-0_6b-v2.jpg&amp;w=3840&amp;q=90 3840w" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/parakeet-tdt-0_6b-v2.jpeg" style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"><div class="nv-card-media-header"><div class="flex flex-row gap-2"><span class="nv-badge nv-badge--kind-solid nv-badge--color-purple align-bottom" data-testid="nv-badge">Download Available</span><span class="nv-badge nv-badge--kind-solid nv-badge--color-yellow align-bottom" data-testid="nv-badge"><svg data-src="https://brand-assets.cne.ngc.nvidia.com/assets/icons/3.8.0/fill/award-trophy.svg" height="1em" width="1em" display="inline-block" data-icon-name="award-trophy" data-cache="disabled" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" data-id="svg-loader_43"><symbol id="award-trophy_44" viewBox="0 0 16 16"><path fill="currentColor" d="M3.934,2h8.132l-0.25,2h2.336l-1.34,5h-1.62l-0.25,2h-1.79l0.535,2h1.313v1h-6v-1h1.312l0.536,-2h-1.79l-0.25,-2h-1.62l-1.34,-5h2.336zM4.309,5h-1.159l0.804,3h0.729zM7.883,11l-0.535,2h1.304l-0.536,-2zM11.316,8h0.729l0.803,-3h-1.157zM9.5,4h-3v1h3z"></path></symbol><use href="#award-trophy_44"></use></svg></span></div></div></div><div class="nv-card-content" data-testid="nv-card-content"><div class="flex flex-col gap-2 overflow-hidden"><h3 class="flex w-full flex-col gap-1"><a class="self-start truncate text-sm font-bold lowercase leading-text text-n300 hover:text-primary" href="https://build.nvidia.com/nvidia" data-backup-tabindex="null">nvidia</a><a title="parakeet-tdt-0.6b-v2" class="style_overlay__cLWaT truncate text-md font-medium leading-heading text-primary" data-linkbox-overlay="true" href="https://build.nvidia.com/nvidia/parakeet-tdt-0_6b-v2" data-backup-tabindex="null">parakeet-tdt-0.6b-v2</a></h3><p class="line-clamp-2 text-sm font-normal leading-body text-n300">Accurate and optimized English transcriptions with punctuation and word timestamps</p></div><div class="relative flex w-full items-center justify-start gap-2 mt-auto min-w-0 max-w-full"><div class="flex items-center gap-2 overflow-hidden"><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=ASR" data-backup-tabindex="null"><span class="inline-block truncate">ASR</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=English" data-backup-tabindex="null"><span class="inline-block truncate">English</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=NVIDIA+NIM" data-backup-tabindex="null"><span class="inline-block truncate">NVIDIA NIM</span></a></div><button data-testid="nv-popover-trigger" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:r5e:" data-state="closed" class="inline-flex items-center justify-center gap-2 text-center font-sans leading-text flex-row btn-tertiary btn-xs btn-pill btn-inverse font-medium" data-backup-tabindex="null">+2</button></div></div></div></div></div><div data-testid="carousel-item-parakeet-ctc-1_1b-asr" class="min-w-0 max-w-full flex-none pl-4 *:h-full first:pl-0" data-backup-role="null" role="tabpanel" data-backup-aria-label="null" aria-label="3 of 10" data-backup-aria-roledescription="null" aria-roledescription="Slide" data-backup-aria-hidden="null" aria-hidden="false" data-backup-aria-selected="null" aria-selected="true" data-backup-tabindex="null"><div data-nvtrack="Carousel click" data-nvtrack-adobe-type="carouselItemClick" data-nvtrack-model="/nvidia/parakeet-ctc-1_1b-asr" data-nvtrack-model-name="parakeet-ctc-1_1b-asr" class="style_linkbox__jnr5J style_root__prJCC cursor-pointer"><div class="nv-card-root nv-card-root--interactive nv-card-root--kind-float nv-card-root--layout-vertical" data-testid="nv-card-root" endpointspec="[object Object]"><div class="nv-card-media" data-testid="nv-card-media"><img alt="" loading="lazy" decoding="async" data-nimg="fill" class="object-cover object-center" sizes="(max-width: 768px) 150px, 300px" srcset="/_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-1_1b-asr.jpg&amp;w=150&amp;q=90 150w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-1_1b-asr.jpg&amp;w=300&amp;q=90 300w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-1_1b-asr.jpg&amp;w=350&amp;q=90 350w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-1_1b-asr.jpg&amp;w=400&amp;q=90 400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-1_1b-asr.jpg&amp;w=600&amp;q=90 600w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-1_1b-asr.jpg&amp;w=640&amp;q=90 640w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-1_1b-asr.jpg&amp;w=700&amp;q=90 700w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-1_1b-asr.jpg&amp;w=750&amp;q=90 750w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-1_1b-asr.jpg&amp;w=828&amp;q=90 828w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-1_1b-asr.jpg&amp;w=1080&amp;q=90 1080w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-1_1b-asr.jpg&amp;w=1200&amp;q=90 1200w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-1_1b-asr.jpg&amp;w=1400&amp;q=90 1400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-1_1b-asr.jpg&amp;w=1920&amp;q=90 1920w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-1_1b-asr.jpg&amp;w=2048&amp;q=90 2048w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-1_1b-asr.jpg&amp;w=3840&amp;q=90 3840w" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/parakeet-ctc-1_1b-asr.jpeg" style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"><div class="nv-card-media-header"><div class="flex flex-row gap-2"><span class="nv-badge nv-badge--kind-solid nv-badge--color-purple align-bottom" data-testid="nv-badge">Download Available</span></div></div></div><div class="nv-card-content" data-testid="nv-card-content"><div class="flex flex-col gap-2 overflow-hidden"><h3 class="flex w-full flex-col gap-1"><a class="self-start truncate text-sm font-bold lowercase leading-text text-n300 hover:text-primary" href="https://build.nvidia.com/nvidia" data-backup-tabindex="null">nvidia</a><a title="parakeet-ctc-1.1b-asr" class="style_overlay__cLWaT truncate text-md font-medium leading-heading text-primary" data-linkbox-overlay="true" href="https://build.nvidia.com/nvidia/parakeet-ctc-1_1b-asr" data-backup-tabindex="null">parakeet-ctc-1.1b-asr</a></h3><p class="line-clamp-2 text-sm font-normal leading-body text-n300">Record-setting accuracy and performance for English transcription.</p></div><div class="relative flex w-full items-center justify-start gap-2 mt-auto min-w-0 max-w-full"><div class="flex items-center gap-2 overflow-hidden"><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=ASR" data-backup-tabindex="null"><span class="inline-block truncate">ASR</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=English" data-backup-tabindex="null"><span class="inline-block truncate">English</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=NVIDIA+NIM" data-backup-tabindex="null"><span class="inline-block truncate">NVIDIA NIM</span></a></div><button data-testid="nv-popover-trigger" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:r5f:" data-state="closed" class="inline-flex items-center justify-center gap-2 text-center font-sans leading-text flex-row btn-tertiary btn-xs btn-pill btn-inverse font-medium" data-backup-tabindex="null">+3</button></div></div></div></div></div><div data-testid="carousel-item-parakeet-ctc-0_6b-asr" class="min-w-0 max-w-full flex-none pl-4 *:h-full first:pl-0" data-backup-role="null" role="tabpanel" data-backup-aria-label="null" aria-label="4 of 10" data-backup-aria-roledescription="null" aria-roledescription="Slide" data-backup-aria-hidden="null" aria-hidden="false" data-backup-aria-selected="null" aria-selected="true" data-backup-tabindex="null"><div data-nvtrack="Carousel click" data-nvtrack-adobe-type="carouselItemClick" data-nvtrack-model="/nvidia/parakeet-ctc-0_6b-asr" data-nvtrack-model-name="parakeet-ctc-0_6b-asr" class="style_linkbox__jnr5J style_root__prJCC cursor-pointer"><div class="nv-card-root nv-card-root--interactive nv-card-root--kind-float nv-card-root--layout-vertical" data-testid="nv-card-root" endpointspec="[object Object]"><div class="nv-card-media" data-testid="nv-card-media"><img alt="" loading="lazy" decoding="async" data-nimg="fill" class="object-cover object-center" sizes="(max-width: 768px) 150px, 300px" srcset="/_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-asr.jpg&amp;w=150&amp;q=90 150w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-asr.jpg&amp;w=300&amp;q=90 300w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-asr.jpg&amp;w=350&amp;q=90 350w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-asr.jpg&amp;w=400&amp;q=90 400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-asr.jpg&amp;w=600&amp;q=90 600w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-asr.jpg&amp;w=640&amp;q=90 640w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-asr.jpg&amp;w=700&amp;q=90 700w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-asr.jpg&amp;w=750&amp;q=90 750w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-asr.jpg&amp;w=828&amp;q=90 828w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-asr.jpg&amp;w=1080&amp;q=90 1080w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-asr.jpg&amp;w=1200&amp;q=90 1200w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-asr.jpg&amp;w=1400&amp;q=90 1400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-asr.jpg&amp;w=1920&amp;q=90 1920w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-asr.jpg&amp;w=2048&amp;q=90 2048w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-asr.jpg&amp;w=3840&amp;q=90 3840w" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/parakeet-ctc-0_6b-asr.jpeg" style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"><div class="nv-card-media-header"><div class="flex flex-row gap-2"><span class="nv-badge nv-badge--kind-solid nv-badge--color-purple align-bottom" data-testid="nv-badge">Download Available</span></div></div></div><div class="nv-card-content" data-testid="nv-card-content"><div class="flex flex-col gap-2 overflow-hidden"><h3 class="flex w-full flex-col gap-1"><a class="self-start truncate text-sm font-bold lowercase leading-text text-n300 hover:text-primary" href="https://build.nvidia.com/nvidia" data-backup-tabindex="null">nvidia</a><a title="parakeet-ctc-0.6b-asr" class="style_overlay__cLWaT truncate text-md font-medium leading-heading text-primary" data-linkbox-overlay="true" href="https://build.nvidia.com/nvidia/parakeet-ctc-0_6b-asr" data-backup-tabindex="null">parakeet-ctc-0.6b-asr</a></h3><p class="line-clamp-2 text-sm font-normal leading-body text-n300">State-of-the-art accuracy and speed for English transcriptions.</p></div><div class="relative flex w-full items-center justify-start gap-2 mt-auto min-w-0 max-w-full"><div class="flex items-center gap-2 overflow-hidden"><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=ASR" data-backup-tabindex="null"><span class="inline-block truncate">ASR</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=Batch" data-backup-tabindex="null"><span class="inline-block truncate">Batch</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=English" data-backup-tabindex="null"><span class="inline-block truncate">English</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=Fast" data-backup-tabindex="null"><span class="inline-block truncate">Fast</span></a></div><button data-testid="nv-popover-trigger" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:r5g:" data-state="closed" class="inline-flex items-center justify-center gap-2 text-center font-sans leading-text flex-row btn-tertiary btn-xs btn-pill btn-inverse font-medium" data-backup-tabindex="null">+4</button></div></div></div></div></div><div data-testid="carousel-item-canary-1b-asr" class="min-w-0 max-w-full flex-none pl-4 *:h-full first:pl-0" data-backup-role="null" role="tabpanel" data-backup-aria-label="null" aria-label="5 of 10" data-backup-aria-roledescription="null" aria-roledescription="Slide" data-backup-aria-hidden="null" aria-hidden="false" data-backup-aria-selected="null" aria-selected="true" data-backup-tabindex="null"><div data-nvtrack="Carousel click" data-nvtrack-adobe-type="carouselItemClick" data-nvtrack-model="/nvidia/canary-1b-asr" data-nvtrack-model-name="canary-1b-asr" class="style_linkbox__jnr5J style_root__prJCC cursor-pointer"><div class="nv-card-root nv-card-root--interactive nv-card-root--kind-float nv-card-root--layout-vertical" data-testid="nv-card-root" endpointspec="[object Object]"><div class="nv-card-media" data-testid="nv-card-media"><img alt="" loading="lazy" decoding="async" data-nimg="fill" class="object-cover object-center" sizes="(max-width: 768px) 150px, 300px" srcset="/_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=150&amp;q=90 150w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=300&amp;q=90 300w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=350&amp;q=90 350w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=400&amp;q=90 400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=600&amp;q=90 600w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=640&amp;q=90 640w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=700&amp;q=90 700w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=750&amp;q=90 750w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=828&amp;q=90 828w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=1080&amp;q=90 1080w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=1200&amp;q=90 1200w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=1400&amp;q=90 1400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=1920&amp;q=90 1920w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=2048&amp;q=90 2048w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=3840&amp;q=90 3840w" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/canary-1b-asr.jpeg" style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"><div class="nv-card-media-header"><div class="flex flex-row gap-2"><span class="nv-badge nv-badge--kind-solid nv-badge--color-purple align-bottom" data-testid="nv-badge">Download Available</span></div></div></div><div class="nv-card-content" data-testid="nv-card-content"><div class="flex flex-col gap-2 overflow-hidden"><h3 class="flex w-full flex-col gap-1"><a class="self-start truncate text-sm font-bold lowercase leading-text text-n300 hover:text-primary" href="https://build.nvidia.com/nvidia" data-backup-tabindex="null">nvidia</a><a title="canary-1b-asr" class="style_overlay__cLWaT truncate text-md font-medium leading-heading text-primary" data-linkbox-overlay="true" href="https://build.nvidia.com/nvidia/canary-1b-asr" data-backup-tabindex="null">canary-1b-asr</a></h3><p class="line-clamp-2 text-sm font-normal leading-body text-n300">Multi-lingual model supporting speech-to-text recognition and translation.</p></div><div class="relative flex w-full items-center justify-start gap-2 mt-auto min-w-0 max-w-full"><div class="flex items-center gap-2 overflow-hidden"><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=Automatic+Speech+Recognition" data-backup-tabindex="null"><span class="inline-block truncate">Automatic Speech Recognition</span></a></div><button data-testid="nv-popover-trigger" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:r5h:" data-state="closed" class="inline-flex items-center justify-center gap-2 text-center font-sans leading-text flex-row btn-tertiary btn-xs btn-pill btn-inverse font-medium" data-backup-tabindex="null">+3</button></div></div></div></div></div><div data-testid="carousel-item-parakeet-1_1b-rnnt-multilingual-asr" class="min-w-0 max-w-full flex-none pl-4 *:h-full first:pl-0" data-backup-role="null" role="tabpanel" data-backup-aria-label="null" aria-label="6 of 10" data-backup-aria-roledescription="null" aria-roledescription="Slide" data-backup-aria-hidden="null" aria-hidden="true" data-backup-aria-selected="null" aria-selected="false" data-backup-tabindex="null" tabindex="-1"><div data-nvtrack="Carousel click" data-nvtrack-adobe-type="carouselItemClick" data-nvtrack-model="/nvidia/parakeet-1_1b-rnnt-multilingual-asr" data-nvtrack-model-name="parakeet-1_1b-rnnt-multilingual-asr" class="style_linkbox__jnr5J style_root__prJCC cursor-pointer"><div class="nv-card-root nv-card-root--interactive nv-card-root--kind-float nv-card-root--layout-vertical" data-testid="nv-card-root" endpointspec="[object Object]"><div class="nv-card-media" data-testid="nv-card-media"><img alt="" loading="lazy" decoding="async" data-nimg="fill" class="object-cover object-center" sizes="(max-width: 768px) 150px, 300px" srcset="/_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-1_1b-rnnt-multilingual-asr.jpg&amp;w=150&amp;q=90 150w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-1_1b-rnnt-multilingual-asr.jpg&amp;w=300&amp;q=90 300w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-1_1b-rnnt-multilingual-asr.jpg&amp;w=350&amp;q=90 350w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-1_1b-rnnt-multilingual-asr.jpg&amp;w=400&amp;q=90 400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-1_1b-rnnt-multilingual-asr.jpg&amp;w=600&amp;q=90 600w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-1_1b-rnnt-multilingual-asr.jpg&amp;w=640&amp;q=90 640w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-1_1b-rnnt-multilingual-asr.jpg&amp;w=700&amp;q=90 700w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-1_1b-rnnt-multilingual-asr.jpg&amp;w=750&amp;q=90 750w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-1_1b-rnnt-multilingual-asr.jpg&amp;w=828&amp;q=90 828w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-1_1b-rnnt-multilingual-asr.jpg&amp;w=1080&amp;q=90 1080w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-1_1b-rnnt-multilingual-asr.jpg&amp;w=1200&amp;q=90 1200w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-1_1b-rnnt-multilingual-asr.jpg&amp;w=1400&amp;q=90 1400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-1_1b-rnnt-multilingual-asr.jpg&amp;w=1920&amp;q=90 1920w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-1_1b-rnnt-multilingual-asr.jpg&amp;w=2048&amp;q=90 2048w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-1_1b-rnnt-multilingual-asr.jpg&amp;w=3840&amp;q=90 3840w" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/parakeet-1_1b-rnnt-multilingual-asr.jpeg" style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"><div class="nv-card-media-header"><div class="flex flex-row gap-2"><span class="nv-badge nv-badge--kind-solid nv-badge--color-purple align-bottom" data-testid="nv-badge">Download Available</span></div></div></div><div class="nv-card-content" data-testid="nv-card-content"><div class="flex flex-col gap-2 overflow-hidden"><h3 class="flex w-full flex-col gap-1"><a class="self-start truncate text-sm font-bold lowercase leading-text text-n300 hover:text-primary" href="https://build.nvidia.com/nvidia" data-backup-tabindex="null" tabindex="-1">nvidia</a><a title="parakeet-1.1b-rnnt-multilingual-asr" class="style_overlay__cLWaT truncate text-md font-medium leading-heading text-primary" data-linkbox-overlay="true" href="https://build.nvidia.com/nvidia/parakeet-1_1b-rnnt-multilingual-asr" data-backup-tabindex="null" tabindex="-1">parakeet-1.1b-rnnt-multilingual-asr</a></h3><p class="line-clamp-2 text-sm font-normal leading-body text-n300">High accuracy and optimized performance for transcription in 25 languages</p></div><div class="relative flex w-full items-center justify-start gap-2 mt-auto min-w-0 max-w-full"><div class="flex items-center gap-2 overflow-hidden"><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=Automatic+Speech+Recognition" data-backup-tabindex="null" tabindex="-1"><span class="inline-block truncate">Automatic Speech Recognition</span></a></div><button data-testid="nv-popover-trigger" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:r5i:" data-state="closed" class="inline-flex items-center justify-center gap-2 text-center font-sans leading-text flex-row btn-tertiary btn-xs btn-pill btn-inverse font-medium" data-backup-tabindex="null" tabindex="-1">+3</button></div></div></div></div></div><div data-testid="carousel-item-whisper-large-v3" class="min-w-0 max-w-full flex-none pl-4 *:h-full first:pl-0" data-backup-role="null" role="tabpanel" data-backup-aria-label="null" aria-label="7 of 10" data-backup-aria-roledescription="null" aria-roledescription="Slide" data-backup-aria-hidden="null" aria-hidden="true" data-backup-aria-selected="null" aria-selected="false" data-backup-tabindex="null" tabindex="-1"><div data-nvtrack="Carousel click" data-nvtrack-adobe-type="carouselItemClick" data-nvtrack-model="/openai/whisper-large-v3" data-nvtrack-model-name="whisper-large-v3" class="style_linkbox__jnr5J style_root__prJCC cursor-pointer"><div class="nv-card-root nv-card-root--interactive nv-card-root--kind-float nv-card-root--layout-vertical" data-testid="nv-card-root" endpointspec="[object Object]"><div class="nv-card-media" data-testid="nv-card-media"><img alt="" loading="lazy" decoding="async" data-nimg="fill" class="object-cover object-center" sizes="(max-width: 768px) 150px, 300px" srcset="/_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=150&amp;q=90 150w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=300&amp;q=90 300w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=350&amp;q=90 350w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=400&amp;q=90 400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=600&amp;q=90 600w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=640&amp;q=90 640w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=700&amp;q=90 700w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=750&amp;q=90 750w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=828&amp;q=90 828w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=1080&amp;q=90 1080w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=1200&amp;q=90 1200w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=1400&amp;q=90 1400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=1920&amp;q=90 1920w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=2048&amp;q=90 2048w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=3840&amp;q=90 3840w" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/whisper-large-v3.jpeg" style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"><div class="nv-card-media-header"><div class="flex flex-row gap-2"><span class="nv-badge nv-badge--kind-solid nv-badge--color-purple align-bottom" data-testid="nv-badge">Download Available</span></div></div></div><div class="nv-card-content" data-testid="nv-card-content"><div class="flex flex-col gap-2 overflow-hidden"><h3 class="flex w-full flex-col gap-1"><a class="self-start truncate text-sm font-bold lowercase leading-text text-n300 hover:text-primary" href="https://build.nvidia.com/openai" data-backup-tabindex="null" tabindex="-1">openai</a><a title="whisper-large-v3" class="style_overlay__cLWaT truncate text-md font-medium leading-heading text-primary" data-linkbox-overlay="true" href="https://build.nvidia.com/openai/whisper-large-v3" data-backup-tabindex="null" tabindex="-1">whisper-large-v3</a></h3><p class="line-clamp-2 text-sm font-normal leading-body text-n300">Robust Speech Recognition via Large-Scale Weak Supervision.</p></div><div class="relative flex w-full items-center justify-start gap-2 mt-auto min-w-0 max-w-full"><div class="flex items-center gap-2 overflow-hidden"><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=ASR" data-backup-tabindex="null" tabindex="-1"><span class="inline-block truncate">ASR</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=AST" data-backup-tabindex="null" tabindex="-1"><span class="inline-block truncate">AST</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=Multilingual" data-backup-tabindex="null" tabindex="-1"><span class="inline-block truncate">Multilingual</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=NVIDIA+NIM" data-backup-tabindex="null" tabindex="-1"><span class="inline-block truncate">NVIDIA NIM</span></a></div><button data-testid="nv-popover-trigger" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:r5j:" data-state="closed" class="inline-flex items-center justify-center gap-2 text-center font-sans leading-text flex-row btn-tertiary btn-xs btn-pill btn-inverse font-medium" data-backup-tabindex="null" tabindex="-1">+5</button></div></div></div></div></div><div data-testid="carousel-item-parakeet-ctc-0_6b-vi" class="min-w-0 max-w-full flex-none pl-4 *:h-full first:pl-0" data-backup-role="null" role="tabpanel" data-backup-aria-label="null" aria-label="8 of 10" data-backup-aria-roledescription="null" aria-roledescription="Slide" data-backup-aria-hidden="null" aria-hidden="true" data-backup-aria-selected="null" aria-selected="false" data-backup-tabindex="null" tabindex="-1"><div data-nvtrack="Carousel click" data-nvtrack-adobe-type="carouselItemClick" data-nvtrack-model="/nvidia/parakeet-ctc-0_6b-vi" data-nvtrack-model-name="parakeet-ctc-0_6b-vi" class="style_linkbox__jnr5J style_root__prJCC cursor-pointer"><div class="nv-card-root nv-card-root--interactive nv-card-root--kind-float nv-card-root--layout-vertical" data-testid="nv-card-root" endpointspec="[object Object]"><div class="nv-card-media" data-testid="nv-card-media"><img alt="" loading="lazy" decoding="async" data-nimg="fill" class="object-cover object-center" sizes="(max-width: 768px) 150px, 300px" srcset="/_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-vi.jpg&amp;w=150&amp;q=90 150w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-vi.jpg&amp;w=300&amp;q=90 300w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-vi.jpg&amp;w=350&amp;q=90 350w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-vi.jpg&amp;w=400&amp;q=90 400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-vi.jpg&amp;w=600&amp;q=90 600w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-vi.jpg&amp;w=640&amp;q=90 640w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-vi.jpg&amp;w=700&amp;q=90 700w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-vi.jpg&amp;w=750&amp;q=90 750w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-vi.jpg&amp;w=828&amp;q=90 828w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-vi.jpg&amp;w=1080&amp;q=90 1080w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-vi.jpg&amp;w=1200&amp;q=90 1200w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-vi.jpg&amp;w=1400&amp;q=90 1400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-vi.jpg&amp;w=1920&amp;q=90 1920w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-vi.jpg&amp;w=2048&amp;q=90 2048w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-vi.jpg&amp;w=3840&amp;q=90 3840w" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/parakeet-ctc-0_6b-vi.jpeg" style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"><div class="nv-card-media-header"><div class="flex flex-row gap-2"><span class="nv-badge nv-badge--kind-solid nv-badge--color-purple align-bottom" data-testid="nv-badge">Download Available</span></div></div></div><div class="nv-card-content" data-testid="nv-card-content"><div class="flex flex-col gap-2 overflow-hidden"><h3 class="flex w-full flex-col gap-1"><a class="self-start truncate text-sm font-bold lowercase leading-text text-n300 hover:text-primary" href="https://build.nvidia.com/nvidia" data-backup-tabindex="null" tabindex="-1">nvidia</a><a title="parakeet-ctc-0.6b-vi" class="style_overlay__cLWaT truncate text-md font-medium leading-heading text-primary" data-linkbox-overlay="true" href="https://build.nvidia.com/nvidia/parakeet-ctc-0_6b-vi" data-backup-tabindex="null" tabindex="-1">parakeet-ctc-0.6b-vi</a></h3><p class="line-clamp-2 text-sm font-normal leading-body text-n300">Accurate and optimized Vietnamese-English transcriptions with punctuation and word timestamps.</p></div><div class="relative flex w-full items-center justify-start gap-2 mt-auto min-w-0 max-w-full"><div class="flex items-center gap-2 overflow-hidden"><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=ASR" data-backup-tabindex="null" tabindex="-1"><span class="inline-block truncate">ASR</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=NVIDIA+NIM" data-backup-tabindex="null" tabindex="-1"><span class="inline-block truncate">NVIDIA NIM</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=Streaming" data-backup-tabindex="null" tabindex="-1"><span class="inline-block truncate">Streaming</span></a></div><button data-testid="nv-popover-trigger" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:r5k:" data-state="closed" class="inline-flex items-center justify-center gap-2 text-center font-sans leading-text flex-row btn-tertiary btn-xs btn-pill btn-inverse font-medium" data-backup-tabindex="null" tabindex="-1">+2</button></div></div></div></div></div><div data-testid="carousel-item-parakeet-ctc-0_6b-zh-cn" class="min-w-0 max-w-full flex-none pl-4 *:h-full first:pl-0" data-backup-role="null" role="tabpanel" data-backup-aria-label="null" aria-label="9 of 10" data-backup-aria-roledescription="null" aria-roledescription="Slide" data-backup-aria-hidden="null" aria-hidden="true" data-backup-aria-selected="null" aria-selected="false" data-backup-tabindex="null" tabindex="-1"><div data-nvtrack="Carousel click" data-nvtrack-adobe-type="carouselItemClick" data-nvtrack-model="/nvidia/parakeet-ctc-0_6b-zh-cn" data-nvtrack-model-name="parakeet-ctc-0_6b-zh-cn" class="style_linkbox__jnr5J style_root__prJCC cursor-pointer"><div class="nv-card-root nv-card-root--interactive nv-card-root--kind-float nv-card-root--layout-vertical" data-testid="nv-card-root" endpointspec="[object Object]"><div class="nv-card-media" data-testid="nv-card-media"><img alt="" loading="lazy" decoding="async" data-nimg="fill" class="object-cover object-center" sizes="(max-width: 768px) 150px, 300px" srcset="/_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-cn.jpg&amp;w=150&amp;q=90 150w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-cn.jpg&amp;w=300&amp;q=90 300w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-cn.jpg&amp;w=350&amp;q=90 350w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-cn.jpg&amp;w=400&amp;q=90 400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-cn.jpg&amp;w=600&amp;q=90 600w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-cn.jpg&amp;w=640&amp;q=90 640w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-cn.jpg&amp;w=700&amp;q=90 700w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-cn.jpg&amp;w=750&amp;q=90 750w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-cn.jpg&amp;w=828&amp;q=90 828w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-cn.jpg&amp;w=1080&amp;q=90 1080w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-cn.jpg&amp;w=1200&amp;q=90 1200w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-cn.jpg&amp;w=1400&amp;q=90 1400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-cn.jpg&amp;w=1920&amp;q=90 1920w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-cn.jpg&amp;w=2048&amp;q=90 2048w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-zh-cn.jpg&amp;w=3840&amp;q=90 3840w" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/parakeet-ctc-0_6b-zh-cn.jpeg" style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"><div class="nv-card-media-header"><div class="flex flex-row gap-2"><span class="nv-badge nv-badge--kind-solid nv-badge--color-purple align-bottom" data-testid="nv-badge">Download Available</span></div></div></div><div class="nv-card-content" data-testid="nv-card-content"><div class="flex flex-col gap-2 overflow-hidden"><h3 class="flex w-full flex-col gap-1"><a class="self-start truncate text-sm font-bold lowercase leading-text text-n300 hover:text-primary" href="https://build.nvidia.com/nvidia" data-backup-tabindex="null" tabindex="-1">nvidia</a><a title="parakeet-ctc-0.6b-zh-cn" class="style_overlay__cLWaT truncate text-md font-medium leading-heading text-primary" data-linkbox-overlay="true" href="https://build.nvidia.com/nvidia/parakeet-ctc-0_6b-zh-cn" data-backup-tabindex="null" tabindex="-1">parakeet-ctc-0.6b-zh-cn</a></h3><p class="line-clamp-2 text-sm font-normal leading-body text-n300">Record-setting accuracy and performance for Mandarin English transcriptions.</p></div><div class="relative flex w-full items-center justify-start gap-2 mt-auto min-w-0 max-w-full"><div class="flex items-center gap-2 overflow-hidden"><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=ASR" data-backup-tabindex="null" tabindex="-1"><span class="inline-block truncate">ASR</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=Mandarin" data-backup-tabindex="null" tabindex="-1"><span class="inline-block truncate">Mandarin</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=NVIDIA+NIM" data-backup-tabindex="null" tabindex="-1"><span class="inline-block truncate">NVIDIA NIM</span></a></div><button data-testid="nv-popover-trigger" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:r5l:" data-state="closed" class="inline-flex items-center justify-center gap-2 text-center font-sans leading-text flex-row btn-tertiary btn-xs btn-pill btn-inverse font-medium" data-backup-tabindex="null" tabindex="-1">+2</button></div></div></div></div></div><div data-testid="carousel-item-parakeet-ctc-0_6b-es" class="min-w-0 max-w-full flex-none pl-4 *:h-full first:pl-0" data-backup-role="null" role="tabpanel" data-backup-aria-label="null" aria-label="10 of 10" data-backup-aria-roledescription="null" aria-roledescription="Slide" data-backup-aria-hidden="null" aria-hidden="true" data-backup-aria-selected="null" aria-selected="false" data-backup-tabindex="null" tabindex="-1"><div data-nvtrack="Carousel click" data-nvtrack-adobe-type="carouselItemClick" data-nvtrack-model="/nvidia/parakeet-ctc-0_6b-es" data-nvtrack-model-name="parakeet-ctc-0_6b-es" class="style_linkbox__jnr5J style_root__prJCC cursor-pointer"><div class="nv-card-root nv-card-root--interactive nv-card-root--kind-float nv-card-root--layout-vertical" data-testid="nv-card-root" endpointspec="[object Object]"><div class="nv-card-media" data-testid="nv-card-media"><img alt="" loading="lazy" decoding="async" data-nimg="fill" class="object-cover object-center" sizes="(max-width: 768px) 150px, 300px" srcset="/_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-es.jpg&amp;w=150&amp;q=90 150w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-es.jpg&amp;w=300&amp;q=90 300w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-es.jpg&amp;w=350&amp;q=90 350w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-es.jpg&amp;w=400&amp;q=90 400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-es.jpg&amp;w=600&amp;q=90 600w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-es.jpg&amp;w=640&amp;q=90 640w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-es.jpg&amp;w=700&amp;q=90 700w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-es.jpg&amp;w=750&amp;q=90 750w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-es.jpg&amp;w=828&amp;q=90 828w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-es.jpg&amp;w=1080&amp;q=90 1080w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-es.jpg&amp;w=1200&amp;q=90 1200w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-es.jpg&amp;w=1400&amp;q=90 1400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-es.jpg&amp;w=1920&amp;q=90 1920w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-es.jpg&amp;w=2048&amp;q=90 2048w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fparakeet-ctc-0_6b-es.jpg&amp;w=3840&amp;q=90 3840w" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/parakeet-ctc-0_6b-es.jpeg" style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"><div class="nv-card-media-header"><div class="flex flex-row gap-2"><span class="nv-badge nv-badge--kind-solid nv-badge--color-purple align-bottom" data-testid="nv-badge">Download Available</span></div></div></div><div class="nv-card-content" data-testid="nv-card-content"><div class="flex flex-col gap-2 overflow-hidden"><h3 class="flex w-full flex-col gap-1"><a class="self-start truncate text-sm font-bold lowercase leading-text text-n300 hover:text-primary" href="https://build.nvidia.com/nvidia" data-backup-tabindex="null" tabindex="-1">nvidia</a><a title="parakeet-ctc-0.6b-es" class="style_overlay__cLWaT truncate text-md font-medium leading-heading text-primary" data-linkbox-overlay="true" href="https://build.nvidia.com/nvidia/parakeet-ctc-0_6b-es" data-backup-tabindex="null" tabindex="-1">parakeet-ctc-0.6b-es</a></h3><p class="line-clamp-2 text-sm font-normal leading-body text-n300">Accurate and optimized Spanish English transcriptions with punctuation and word timestamps.</p></div><div class="relative flex w-full items-center justify-start gap-2 mt-auto min-w-0 max-w-full"><div class="flex items-center gap-2 overflow-hidden"><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=ASR" data-backup-tabindex="null" tabindex="-1"><span class="inline-block truncate">ASR</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=NVIDIA+NIM" data-backup-tabindex="null" tabindex="-1"><span class="inline-block truncate">NVIDIA NIM</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=Spanish" data-backup-tabindex="null" tabindex="-1"><span class="inline-block truncate">Spanish</span></a></div><button data-testid="nv-popover-trigger" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:r5m:" data-state="closed" class="inline-flex items-center justify-center gap-2 text-center font-sans leading-text flex-row btn-tertiary btn-xs btn-pill btn-inverse font-medium" data-backup-tabindex="null" tabindex="-1">+2</button></div></div></div></div></div></div></div></section></div></div></section><div aria-orientation="horizontal" class="bg-n700 h-px w-full" role="separator"></div><section data-testid="explore-carousel-view"><div class="relative"><div class="h-fit w-full" data-testid="carousel-view"><section class="relative"><header class="mb-2.5 flex items-end justify-between gap-2.5 text-primary"><div class="mb-2 flex items-start gap-2 max-xs:justify-between"><h2 class="text-ml font-medium leading-body tracking-less text-manitoulinLightWhite mb-0">Convert Text to Speech (TTS)</h2></div><p class="text-md font-normal text-manitoulinLightGray mb-0">Convert written text to spoken audio in multiple languages with NVIDIA Nemotron Speech models.</p> <div class="max-sm:hidden! ml-auto flex gap-2 justify-self-end"><button aria-label="Previous Slide" hidden="" type="button" class="inline-flex items-center justify-center gap-2 text-center font-sans font-bold leading-text flex-row btn-secondary btn-md btn-pill btn-inverse btn-icon-only min-h-8 min-w-8"><svg data-src="https://brand-assets.cne.ngc.nvidia.com/assets/icons/3.8.0/fill/chevron-left.svg" height="1em" width="1em" display="inline-block" data-icon-name="chevron-left" data-cache="disabled" class="btn-icon" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" data-id="svg-loader_45"><symbol id="chevron-left_46" viewBox="0 0 16 16"><path fill="currentColor" d="M6.707,8l3.647,3.646l-0.707,0.708l-4.354,-4.354l4.354,-4.354l0.707,0.708z"></path></symbol><use href="#chevron-left_46"></use></svg></button><button aria-label="Next Slide" hidden="" type="button" class="inline-flex items-center justify-center gap-2 text-center font-sans font-bold leading-text flex-row btn-secondary btn-md btn-pill btn-inverse btn-icon-only min-h-8 min-w-8"><svg data-src="https://brand-assets.cne.ngc.nvidia.com/assets/icons/3.8.0/fill/chevron-right.svg" height="1em" width="1em" display="inline-block" data-icon-name="chevron-right" data-cache="disabled" class="btn-icon" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" data-id="svg-loader_47"><symbol id="chevron-right_48" viewBox="0 0 16 16"><path fill="currentColor" d="M9.293,8l-3.647,-3.646l0.708,-0.708l4.353,4.354l-4.353,4.354l-0.708,-0.708z"></path></symbol><use href="#chevron-right_48"></use></svg></button></div></header><div class="relative overflow-x-clip" data-backup-role="null" role="tablist" data-backup-aria-live="null" aria-live="off" data-backup-aria-orientation="null" aria-orientation="horizontal" data-backup-aria-roledescription="null" aria-roledescription="Carousel" data-backup-aria-multiselectable="null" aria-multiselectable="false"><div class="backface-hidden ml-[calc(var(--spacing-4)*-1)] flex touch-pan-y" data-testid="embla-container" style="transform: translate3d(0px, 0px, 0px);"><div data-testid="carousel-item-magpie-tts-multilingual" class="min-w-0 max-w-full flex-none pl-4 *:h-full first:pl-0" data-backup-role="null" role="tabpanel" data-backup-aria-label="null" aria-label="1 of 3" data-backup-aria-roledescription="null" aria-roledescription="Slide" data-backup-aria-hidden="null" aria-hidden="false" data-backup-aria-selected="null" aria-selected="true" data-backup-tabindex="null"><div data-nvtrack="Carousel click" data-nvtrack-adobe-type="carouselItemClick" data-nvtrack-model="/nvidia/magpie-tts-multilingual" data-nvtrack-model-name="magpie-tts-multilingual" class="style_linkbox__jnr5J style_root__prJCC cursor-pointer"><div class="nv-card-root nv-card-root--interactive nv-card-root--kind-float nv-card-root--layout-vertical" data-testid="nv-card-root" endpointspec="[object Object]"><div class="nv-card-media" data-testid="nv-card-media"><img alt="" loading="lazy" decoding="async" data-nimg="fill" class="object-cover object-center" sizes="(max-width: 768px) 150px, 300px" srcset="/_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-multilingual.jpg&amp;w=150&amp;q=90 150w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-multilingual.jpg&amp;w=300&amp;q=90 300w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-multilingual.jpg&amp;w=350&amp;q=90 350w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-multilingual.jpg&amp;w=400&amp;q=90 400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-multilingual.jpg&amp;w=600&amp;q=90 600w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-multilingual.jpg&amp;w=640&amp;q=90 640w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-multilingual.jpg&amp;w=700&amp;q=90 700w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-multilingual.jpg&amp;w=750&amp;q=90 750w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-multilingual.jpg&amp;w=828&amp;q=90 828w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-multilingual.jpg&amp;w=1080&amp;q=90 1080w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-multilingual.jpg&amp;w=1200&amp;q=90 1200w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-multilingual.jpg&amp;w=1400&amp;q=90 1400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-multilingual.jpg&amp;w=1920&amp;q=90 1920w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-multilingual.jpg&amp;w=2048&amp;q=90 2048w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-multilingual.jpg&amp;w=3840&amp;q=90 3840w" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/magpie-tts-multilingual.jpeg" style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"><div class="nv-card-media-header"><div class="flex flex-row gap-2"><span class="nv-badge nv-badge--kind-solid nv-badge--color-purple align-bottom" data-testid="nv-badge">Download Available</span></div></div></div><div class="nv-card-content" data-testid="nv-card-content"><div class="flex flex-col gap-2 overflow-hidden"><h3 class="flex w-full flex-col gap-1"><a class="self-start truncate text-sm font-bold lowercase leading-text text-n300 hover:text-primary" href="https://build.nvidia.com/nvidia" data-backup-tabindex="null">nvidia</a><a title="magpie-tts-multilingual" class="style_overlay__cLWaT truncate text-md font-medium leading-heading text-primary" data-linkbox-overlay="true" href="https://build.nvidia.com/nvidia/magpie-tts-multilingual" data-backup-tabindex="null">magpie-tts-multilingual</a></h3><p class="line-clamp-2 text-sm font-normal leading-body text-n300">Natural and expressive voices in multiple languages. For voice agents and brand ambassadors.</p></div><div class="relative flex w-full items-center justify-start gap-2 mt-auto min-w-0 max-w-full"><div class="flex items-center gap-2 overflow-hidden"><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=NVIDIA+NIM" data-backup-tabindex="null"><span class="inline-block truncate">NVIDIA NIM</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=NVIDIA+Riva" data-backup-tabindex="null"><span class="inline-block truncate">NVIDIA Riva</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=TTS" data-backup-tabindex="null"><span class="inline-block truncate">TTS</span></a></div><button data-testid="nv-popover-trigger" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:r5n:" data-state="closed" class="inline-flex items-center justify-center gap-2 text-center font-sans leading-text flex-row btn-tertiary btn-xs btn-pill btn-inverse font-medium" data-backup-tabindex="null">+2</button></div></div></div></div></div><div data-testid="carousel-item-magpie-tts-zeroshot" class="min-w-0 max-w-full flex-none pl-4 *:h-full first:pl-0" data-backup-role="null" role="tabpanel" data-backup-aria-label="null" aria-label="2 of 3" data-backup-aria-roledescription="null" aria-roledescription="Slide" data-backup-aria-hidden="null" aria-hidden="false" data-backup-aria-selected="null" aria-selected="true" data-backup-tabindex="null"><div data-nvtrack="Carousel click" data-nvtrack-adobe-type="carouselItemClick" data-nvtrack-model="/nvidia/magpie-tts-zeroshot" data-nvtrack-model-name="magpie-tts-zeroshot" class="style_linkbox__jnr5J style_root__prJCC cursor-pointer"><div class="nv-card-root nv-card-root--interactive nv-card-root--kind-float nv-card-root--layout-vertical" data-testid="nv-card-root" endpointspec="[object Object]"><div class="nv-card-media" data-testid="nv-card-media"><img alt="" loading="lazy" decoding="async" data-nimg="fill" class="object-cover object-center" sizes="(max-width: 768px) 150px, 300px" srcset="/_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-zeroshot.jpg&amp;w=150&amp;q=90 150w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-zeroshot.jpg&amp;w=300&amp;q=90 300w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-zeroshot.jpg&amp;w=350&amp;q=90 350w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-zeroshot.jpg&amp;w=400&amp;q=90 400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-zeroshot.jpg&amp;w=600&amp;q=90 600w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-zeroshot.jpg&amp;w=640&amp;q=90 640w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-zeroshot.jpg&amp;w=700&amp;q=90 700w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-zeroshot.jpg&amp;w=750&amp;q=90 750w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-zeroshot.jpg&amp;w=828&amp;q=90 828w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-zeroshot.jpg&amp;w=1080&amp;q=90 1080w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-zeroshot.jpg&amp;w=1200&amp;q=90 1200w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-zeroshot.jpg&amp;w=1400&amp;q=90 1400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-zeroshot.jpg&amp;w=1920&amp;q=90 1920w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-zeroshot.jpg&amp;w=2048&amp;q=90 2048w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-zeroshot.jpg&amp;w=3840&amp;q=90 3840w" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/magpie-tts-zeroshot.jpeg" style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"><div class="nv-card-media-header"><div class="flex flex-row gap-2"><span class="nv-badge nv-badge--kind-solid nv-badge--color-purple align-bottom" data-testid="nv-badge">API Endpoint</span></div></div></div><div class="nv-card-content" data-testid="nv-card-content"><div class="flex flex-col gap-2 overflow-hidden"><h3 class="flex w-full flex-col gap-1"><a class="self-start truncate text-sm font-bold lowercase leading-text text-n300 hover:text-primary" href="https://build.nvidia.com/nvidia" data-backup-tabindex="null">nvidia</a><a title="magpie-tts-zeroshot" class="style_overlay__cLWaT truncate text-md font-medium leading-heading text-primary" data-linkbox-overlay="true" href="https://build.nvidia.com/nvidia/magpie-tts-zeroshot" data-backup-tabindex="null">magpie-tts-zeroshot</a></h3><p class="line-clamp-2 text-sm font-normal leading-body text-n300">Expressive and engaging text-to-speech, generated from a short audio sample.</p></div><div class="relative flex w-full items-center justify-start gap-2 mt-auto min-w-0 max-w-full"><div class="flex items-center gap-2 overflow-hidden"><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=NVIDIA+NIM" data-backup-tabindex="null"><span class="inline-block truncate">NVIDIA NIM</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=NVIDIA+Riva" data-backup-tabindex="null"><span class="inline-block truncate">NVIDIA Riva</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=TTS" data-backup-tabindex="null"><span class="inline-block truncate">TTS</span></a></div><button data-testid="nv-popover-trigger" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:r5o:" data-state="closed" class="inline-flex items-center justify-center gap-2 text-center font-sans leading-text flex-row btn-tertiary btn-xs btn-pill btn-inverse font-medium" data-backup-tabindex="null">+1</button></div></div></div></div></div><div data-testid="carousel-item-magpie-tts-flow" class="min-w-0 max-w-full flex-none pl-4 *:h-full first:pl-0" data-backup-role="null" role="tabpanel" data-backup-aria-label="null" aria-label="3 of 3" data-backup-aria-roledescription="null" aria-roledescription="Slide" data-backup-aria-hidden="null" aria-hidden="false" data-backup-aria-selected="null" aria-selected="true" data-backup-tabindex="null"><div data-nvtrack="Carousel click" data-nvtrack-adobe-type="carouselItemClick" data-nvtrack-model="/nvidia/magpie-tts-flow" data-nvtrack-model-name="magpie-tts-flow" class="style_linkbox__jnr5J style_root__prJCC cursor-pointer"><div class="nv-card-root nv-card-root--interactive nv-card-root--kind-float nv-card-root--layout-vertical" data-testid="nv-card-root" endpointspec="[object Object]"><div class="nv-card-media" data-testid="nv-card-media"><img alt="" loading="lazy" decoding="async" data-nimg="fill" class="object-cover object-center" sizes="(max-width: 768px) 150px, 300px" srcset="/_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-flow.jpg&amp;w=150&amp;q=90 150w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-flow.jpg&amp;w=300&amp;q=90 300w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-flow.jpg&amp;w=350&amp;q=90 350w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-flow.jpg&amp;w=400&amp;q=90 400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-flow.jpg&amp;w=600&amp;q=90 600w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-flow.jpg&amp;w=640&amp;q=90 640w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-flow.jpg&amp;w=700&amp;q=90 700w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-flow.jpg&amp;w=750&amp;q=90 750w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-flow.jpg&amp;w=828&amp;q=90 828w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-flow.jpg&amp;w=1080&amp;q=90 1080w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-flow.jpg&amp;w=1200&amp;q=90 1200w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-flow.jpg&amp;w=1400&amp;q=90 1400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-flow.jpg&amp;w=1920&amp;q=90 1920w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-flow.jpg&amp;w=2048&amp;q=90 2048w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fmagpie-tts-flow.jpg&amp;w=3840&amp;q=90 3840w" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/magpie-tts-flow.jpeg" style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"><div class="nv-card-media-header"><div class="flex flex-row gap-2"><span class="nv-badge nv-badge--kind-solid nv-badge--color-purple align-bottom" data-testid="nv-badge">API Endpoint</span></div></div></div><div class="nv-card-content" data-testid="nv-card-content"><div class="flex flex-col gap-2 overflow-hidden"><h3 class="flex w-full flex-col gap-1"><a class="self-start truncate text-sm font-bold lowercase leading-text text-n300 hover:text-primary" href="https://build.nvidia.com/nvidia" data-backup-tabindex="null">nvidia</a><a title="magpie-tts-flow" class="style_overlay__cLWaT truncate text-md font-medium leading-heading text-primary" data-linkbox-overlay="true" href="https://build.nvidia.com/nvidia/magpie-tts-flow" data-backup-tabindex="null">magpie-tts-flow</a></h3><p class="line-clamp-2 text-sm font-normal leading-body text-n300">Expressive and engaging text-to-speech, generated from a short audio sample.</p></div><div class="relative flex w-full items-center justify-start gap-2 mt-auto min-w-0 max-w-full"><div class="flex items-center gap-2 overflow-hidden"><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=NVIDIA+NIM" data-backup-tabindex="null"><span class="inline-block truncate">NVIDIA NIM</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=NVIDIA+Riva" data-backup-tabindex="null"><span class="inline-block truncate">NVIDIA Riva</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=TTS" data-backup-tabindex="null"><span class="inline-block truncate">TTS</span></a></div><button data-testid="nv-popover-trigger" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:r5p:" data-state="closed" class="inline-flex items-center justify-center gap-2 text-center font-sans leading-text flex-row btn-tertiary btn-xs btn-pill btn-inverse font-medium" data-backup-tabindex="null">+1</button></div></div></div></div></div></div></div></section></div></div></section><div aria-orientation="horizontal" class="bg-n700 h-px w-full" role="separator"></div><section data-testid="explore-carousel-view"><div class="relative"><div class="h-fit w-full" data-testid="carousel-view"><section class="relative"><header class="mb-2.5 flex items-end justify-between gap-2.5 text-primary"><div class="mb-2 flex items-start gap-2 max-xs:justify-between"><h2 class="text-ml font-medium leading-body tracking-less text-manitoulinLightWhite mb-0">Neural Machine Translation (NMT) &amp; Audio Speech Translation (AST)</h2></div><p class="text-md font-normal text-manitoulinLightGray mb-0">Enable seamless multilingual global communication across dozens of languages with NVIDIA Nemotron Speech models.</p> <div class="max-sm:hidden! ml-auto flex gap-2 justify-self-end"><button aria-label="Previous Slide" hidden="" type="button" class="inline-flex items-center justify-center gap-2 text-center font-sans font-bold leading-text flex-row btn-secondary btn-md btn-pill btn-inverse btn-icon-only min-h-8 min-w-8"><svg data-src="https://brand-assets.cne.ngc.nvidia.com/assets/icons/3.8.0/fill/chevron-left.svg" height="1em" width="1em" display="inline-block" data-icon-name="chevron-left" data-cache="disabled" class="btn-icon" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" data-id="svg-loader_49"><symbol id="chevron-left_50" viewBox="0 0 16 16"><path fill="currentColor" d="M6.707,8l3.647,3.646l-0.707,0.708l-4.354,-4.354l4.354,-4.354l0.707,0.708z"></path></symbol><use href="#chevron-left_50"></use></svg></button><button aria-label="Next Slide" hidden="" type="button" class="inline-flex items-center justify-center gap-2 text-center font-sans font-bold leading-text flex-row btn-secondary btn-md btn-pill btn-inverse btn-icon-only min-h-8 min-w-8"><svg data-src="https://brand-assets.cne.ngc.nvidia.com/assets/icons/3.8.0/fill/chevron-right.svg" height="1em" width="1em" display="inline-block" data-icon-name="chevron-right" data-cache="disabled" class="btn-icon" fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" data-id="svg-loader_51"><symbol id="chevron-right_52" viewBox="0 0 16 16"><path fill="currentColor" d="M9.293,8l-3.647,-3.646l0.708,-0.708l4.353,4.354l-4.353,4.354l-0.708,-0.708z"></path></symbol><use href="#chevron-right_52"></use></svg></button></div></header><div class="relative overflow-x-clip" data-backup-role="null" role="tablist" data-backup-aria-live="null" aria-live="off" data-backup-aria-orientation="null" aria-orientation="horizontal" data-backup-aria-roledescription="null" aria-roledescription="Carousel" data-backup-aria-multiselectable="null" aria-multiselectable="false"><div class="backface-hidden ml-[calc(var(--spacing-4)*-1)] flex touch-pan-y" data-testid="embla-container" style="transform: translate3d(0px, 0px, 0px);"><div data-testid="carousel-item-riva-translate-4b-instruct-v1_1" class="min-w-0 max-w-full flex-none pl-4 *:h-full first:pl-0" data-backup-role="null" role="tabpanel" data-backup-aria-label="null" aria-label="1 of 4" data-backup-aria-roledescription="null" aria-roledescription="Slide" data-backup-aria-hidden="null" aria-hidden="false" data-backup-aria-selected="null" aria-selected="true" data-backup-tabindex="null"><div data-nvtrack="Carousel click" data-nvtrack-adobe-type="carouselItemClick" data-nvtrack-model="/nvidia/riva-translate-4b-instruct-v1_1" data-nvtrack-model-name="riva-translate-4b-instruct-v1_1" class="style_linkbox__jnr5J style_root__prJCC cursor-pointer"><div class="nv-card-root nv-card-root--interactive nv-card-root--kind-float nv-card-root--layout-vertical" data-testid="nv-card-root" endpointspec="[object Object]"><div class="nv-card-media" data-testid="nv-card-media"><img alt="" loading="lazy" decoding="async" data-nimg="fill" class="object-cover object-center" sizes="(max-width: 768px) 150px, 300px" srcset="/_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-4b-instruct.jpg&amp;w=150&amp;q=90 150w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-4b-instruct.jpg&amp;w=300&amp;q=90 300w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-4b-instruct.jpg&amp;w=350&amp;q=90 350w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-4b-instruct.jpg&amp;w=400&amp;q=90 400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-4b-instruct.jpg&amp;w=600&amp;q=90 600w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-4b-instruct.jpg&amp;w=640&amp;q=90 640w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-4b-instruct.jpg&amp;w=700&amp;q=90 700w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-4b-instruct.jpg&amp;w=750&amp;q=90 750w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-4b-instruct.jpg&amp;w=828&amp;q=90 828w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-4b-instruct.jpg&amp;w=1080&amp;q=90 1080w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-4b-instruct.jpg&amp;w=1200&amp;q=90 1200w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-4b-instruct.jpg&amp;w=1400&amp;q=90 1400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-4b-instruct.jpg&amp;w=1920&amp;q=90 1920w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-4b-instruct.jpg&amp;w=2048&amp;q=90 2048w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-4b-instruct.jpg&amp;w=3840&amp;q=90 3840w" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/riva-translate-4b-instruct.jpeg" style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"><div class="nv-card-media-header"><div class="flex flex-row gap-2"><span class="nv-badge nv-badge--kind-solid nv-badge--color-purple align-bottom" data-testid="nv-badge">API Endpoint</span></div></div></div><div class="nv-card-content" data-testid="nv-card-content"><div class="flex flex-col gap-2 overflow-hidden"><h3 class="flex w-full flex-col gap-1"><a class="self-start truncate text-sm font-bold lowercase leading-text text-n300 hover:text-primary" href="https://build.nvidia.com/nvidia" data-backup-tabindex="null">nvidia</a><a title="riva-translate-4b-instruct-v1_1" class="style_overlay__cLWaT truncate text-md font-medium leading-heading text-primary" data-linkbox-overlay="true" href="https://build.nvidia.com/nvidia/riva-translate-4b-instruct-v1_1" data-backup-tabindex="null">riva-translate-4b-instruct-v1_1</a></h3><p class="line-clamp-2 text-sm font-normal leading-body text-n300">Translation model in 12 languages with few-shots example prompts capability.</p></div><div class="relative flex w-full items-center justify-start gap-2 mt-auto min-w-0 max-w-full"><div class="flex items-center gap-2 overflow-hidden"><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=neural+machine+translation" data-backup-tabindex="null"><span class="inline-block truncate">neural machine translation</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=nvidia+nim" data-backup-tabindex="null"><span class="inline-block truncate">nvidia nim</span></a></div><button data-testid="nv-popover-trigger" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:r5q:" data-state="closed" class="inline-flex items-center justify-center gap-2 text-center font-sans leading-text flex-row btn-tertiary btn-xs btn-pill btn-inverse font-medium" data-backup-tabindex="null">+1</button></div></div></div></div></div><div data-testid="carousel-item-riva-translate-1_6b" class="min-w-0 max-w-full flex-none pl-4 *:h-full first:pl-0" data-backup-role="null" role="tabpanel" data-backup-aria-label="null" aria-label="2 of 4" data-backup-aria-roledescription="null" aria-roledescription="Slide" data-backup-aria-hidden="null" aria-hidden="false" data-backup-aria-selected="null" aria-selected="true" data-backup-tabindex="null"><div data-nvtrack="Carousel click" data-nvtrack-adobe-type="carouselItemClick" data-nvtrack-model="/nvidia/riva-translate-1_6b" data-nvtrack-model-name="riva-translate-1_6b" class="style_linkbox__jnr5J style_root__prJCC cursor-pointer"><div class="nv-card-root nv-card-root--interactive nv-card-root--kind-float nv-card-root--layout-vertical" data-testid="nv-card-root" endpointspec="[object Object]"><div class="nv-card-media" data-testid="nv-card-media"><img alt="" loading="lazy" decoding="async" data-nimg="fill" class="object-cover object-center" sizes="(max-width: 768px) 150px, 300px" srcset="/_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-1_6b.jpg&amp;w=150&amp;q=90 150w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-1_6b.jpg&amp;w=300&amp;q=90 300w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-1_6b.jpg&amp;w=350&amp;q=90 350w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-1_6b.jpg&amp;w=400&amp;q=90 400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-1_6b.jpg&amp;w=600&amp;q=90 600w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-1_6b.jpg&amp;w=640&amp;q=90 640w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-1_6b.jpg&amp;w=700&amp;q=90 700w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-1_6b.jpg&amp;w=750&amp;q=90 750w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-1_6b.jpg&amp;w=828&amp;q=90 828w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-1_6b.jpg&amp;w=1080&amp;q=90 1080w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-1_6b.jpg&amp;w=1200&amp;q=90 1200w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-1_6b.jpg&amp;w=1400&amp;q=90 1400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-1_6b.jpg&amp;w=1920&amp;q=90 1920w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-1_6b.jpg&amp;w=2048&amp;q=90 2048w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Friva-translate-1_6b.jpg&amp;w=3840&amp;q=90 3840w" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/riva-translate-1_6b.jpeg" style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"><div class="nv-card-media-header"><div class="flex flex-row gap-2"><span class="nv-badge nv-badge--kind-solid nv-badge--color-purple align-bottom" data-testid="nv-badge">Download Available</span></div></div></div><div class="nv-card-content" data-testid="nv-card-content"><div class="flex flex-col gap-2 overflow-hidden"><h3 class="flex w-full flex-col gap-1"><a class="self-start truncate text-sm font-bold lowercase leading-text text-n300 hover:text-primary" href="https://build.nvidia.com/nvidia" data-backup-tabindex="null">nvidia</a><a title="riva-translate-1.6b" class="style_overlay__cLWaT truncate text-md font-medium leading-heading text-primary" data-linkbox-overlay="true" href="https://build.nvidia.com/nvidia/riva-translate-1_6b" data-backup-tabindex="null">riva-translate-1.6b</a></h3><p class="line-clamp-2 text-sm font-normal leading-body text-n300">Enable smooth global interactions in 36 languages.</p></div><div class="relative flex w-full items-center justify-start gap-2 mt-auto min-w-0 max-w-full"><div class="flex items-center gap-2 overflow-hidden"><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=NVIDIA+NIM" data-backup-tabindex="null"><span class="inline-block truncate">NVIDIA NIM</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=Neural+machine+translation" data-backup-tabindex="null"><span class="inline-block truncate">Neural machine translation</span></a></div><button data-testid="nv-popover-trigger" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:r5r:" data-state="closed" class="inline-flex items-center justify-center gap-2 text-center font-sans leading-text flex-row btn-tertiary btn-xs btn-pill btn-inverse font-medium" data-backup-tabindex="null">+1</button></div></div></div></div></div><div data-testid="carousel-item-canary-1b-asr" class="min-w-0 max-w-full flex-none pl-4 *:h-full first:pl-0" data-backup-role="null" role="tabpanel" data-backup-aria-label="null" aria-label="3 of 4" data-backup-aria-roledescription="null" aria-roledescription="Slide" data-backup-aria-hidden="null" aria-hidden="false" data-backup-aria-selected="null" aria-selected="true" data-backup-tabindex="null"><div data-nvtrack="Carousel click" data-nvtrack-adobe-type="carouselItemClick" data-nvtrack-model="/nvidia/canary-1b-asr" data-nvtrack-model-name="canary-1b-asr" class="style_linkbox__jnr5J style_root__prJCC cursor-pointer"><div class="nv-card-root nv-card-root--interactive nv-card-root--kind-float nv-card-root--layout-vertical" data-testid="nv-card-root" endpointspec="[object Object]"><div class="nv-card-media" data-testid="nv-card-media"><img alt="" loading="lazy" decoding="async" data-nimg="fill" class="object-cover object-center" sizes="(max-width: 768px) 150px, 300px" srcset="/_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=150&amp;q=90 150w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=300&amp;q=90 300w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=350&amp;q=90 350w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=400&amp;q=90 400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=600&amp;q=90 600w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=640&amp;q=90 640w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=700&amp;q=90 700w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=750&amp;q=90 750w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=828&amp;q=90 828w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=1080&amp;q=90 1080w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=1200&amp;q=90 1200w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=1400&amp;q=90 1400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=1920&amp;q=90 1920w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=2048&amp;q=90 2048w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fcanary-1b-asr.jpg&amp;w=3840&amp;q=90 3840w" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/canary-1b-asr.jpeg" style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"><div class="nv-card-media-header"><div class="flex flex-row gap-2"><span class="nv-badge nv-badge--kind-solid nv-badge--color-purple align-bottom" data-testid="nv-badge">Download Available</span></div></div></div><div class="nv-card-content" data-testid="nv-card-content"><div class="flex flex-col gap-2 overflow-hidden"><h3 class="flex w-full flex-col gap-1"><a class="self-start truncate text-sm font-bold lowercase leading-text text-n300 hover:text-primary" href="https://build.nvidia.com/nvidia" data-backup-tabindex="null">nvidia</a><a title="canary-1b-asr" class="style_overlay__cLWaT truncate text-md font-medium leading-heading text-primary" data-linkbox-overlay="true" href="https://build.nvidia.com/nvidia/canary-1b-asr" data-backup-tabindex="null">canary-1b-asr</a></h3><p class="line-clamp-2 text-sm font-normal leading-body text-n300">Multi-lingual model supporting speech-to-text recognition and translation.</p></div><div class="relative flex w-full items-center justify-start gap-2 mt-auto min-w-0 max-w-full"><div class="flex items-center gap-2 overflow-hidden"><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=Automatic+Speech+Recognition" data-backup-tabindex="null"><span class="inline-block truncate">Automatic Speech Recognition</span></a></div><button data-testid="nv-popover-trigger" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:r5s:" data-state="closed" class="inline-flex items-center justify-center gap-2 text-center font-sans leading-text flex-row btn-tertiary btn-xs btn-pill btn-inverse font-medium" data-backup-tabindex="null">+3</button></div></div></div></div></div><div data-testid="carousel-item-whisper-large-v3" class="min-w-0 max-w-full flex-none pl-4 *:h-full first:pl-0" data-backup-role="null" role="tabpanel" data-backup-aria-label="null" aria-label="4 of 4" data-backup-aria-roledescription="null" aria-roledescription="Slide" data-backup-aria-hidden="null" aria-hidden="false" data-backup-aria-selected="null" aria-selected="true" data-backup-tabindex="null"><div data-nvtrack="Carousel click" data-nvtrack-adobe-type="carouselItemClick" data-nvtrack-model="/openai/whisper-large-v3" data-nvtrack-model-name="whisper-large-v3" class="style_linkbox__jnr5J style_root__prJCC cursor-pointer"><div class="nv-card-root nv-card-root--interactive nv-card-root--kind-float nv-card-root--layout-vertical" data-testid="nv-card-root" endpointspec="[object Object]"><div class="nv-card-media" data-testid="nv-card-media"><img alt="" loading="lazy" decoding="async" data-nimg="fill" class="object-cover object-center" sizes="(max-width: 768px) 150px, 300px" srcset="/_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=150&amp;q=90 150w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=300&amp;q=90 300w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=350&amp;q=90 350w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=400&amp;q=90 400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=600&amp;q=90 600w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=640&amp;q=90 640w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=700&amp;q=90 700w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=750&amp;q=90 750w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=828&amp;q=90 828w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=1080&amp;q=90 1080w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=1200&amp;q=90 1200w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=1400&amp;q=90 1400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=1920&amp;q=90 1920w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=2048&amp;q=90 2048w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fwhisper-large-v3.jpg&amp;w=3840&amp;q=90 3840w" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/whisper-large-v3.jpeg" style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"><div class="nv-card-media-header"><div class="flex flex-row gap-2"><span class="nv-badge nv-badge--kind-solid nv-badge--color-purple align-bottom" data-testid="nv-badge">Download Available</span></div></div></div><div class="nv-card-content" data-testid="nv-card-content"><div class="flex flex-col gap-2 overflow-hidden"><h3 class="flex w-full flex-col gap-1"><a class="self-start truncate text-sm font-bold lowercase leading-text text-n300 hover:text-primary" href="https://build.nvidia.com/openai" data-backup-tabindex="null">openai</a><a title="whisper-large-v3" class="style_overlay__cLWaT truncate text-md font-medium leading-heading text-primary" data-linkbox-overlay="true" href="https://build.nvidia.com/openai/whisper-large-v3" data-backup-tabindex="null">whisper-large-v3</a></h3><p class="line-clamp-2 text-sm font-normal leading-body text-n300">Robust Speech Recognition via Large-Scale Weak Supervision.</p></div><div class="relative flex w-full items-center justify-start gap-2 mt-auto min-w-0 max-w-full"><div class="flex items-center gap-2 overflow-hidden"><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=ASR" data-backup-tabindex="null"><span class="inline-block truncate">ASR</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=AST" data-backup-tabindex="null"><span class="inline-block truncate">AST</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=Multilingual" data-backup-tabindex="null"><span class="inline-block truncate">Multilingual</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=NVIDIA+NIM" data-backup-tabindex="null"><span class="inline-block truncate">NVIDIA NIM</span></a></div><button data-testid="nv-popover-trigger" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:r5t:" data-state="closed" class="inline-flex items-center justify-center gap-2 text-center font-sans leading-text flex-row btn-tertiary btn-xs btn-pill btn-inverse font-medium" data-backup-tabindex="null">+5</button></div></div></div></div></div></div></div></section></div></div></section><div aria-orientation="horizontal" class="bg-n700 h-px w-full" role="separator"></div><section><div class="flex items-center justify-between"><h2 class="mb-sm text-ml font-medium leading-body tracking-less text-manitoulinLightWhite">Speech Enhancement</h2></div><p class="mb-[30px] text-md font-normal text-manitoulinLightGray">Speech enhancing AI models for common voice degradations.</p><div class="grid grid-cols-1 gap-8 overflow-hidden lg:grid-cols-2 xl:grid-cols-2"><div><div data-nvtrack="Carousel click" data-nvtrack-adobe-type="carouselItemClick" data-nvtrack-model="/nvidia/studiovoice" data-nvtrack-model-name="studiovoice" class="style_linkbox__jnr5J style_root__prJCC cursor-pointer"><div class="nv-card-root nv-card-root--interactive nv-card-root--kind-float nv-card-root--layout-horizontal" data-testid="nv-card-root"><div class="nv-card-media" data-testid="nv-card-media"><img alt="" loading="lazy" decoding="async" data-nimg="fill" class="object-cover object-center" sizes="(max-width: 768px) 150px, 300px" srcset="/_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fstudiovoice.jpg&amp;w=150&amp;q=90 150w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fstudiovoice.jpg&amp;w=300&amp;q=90 300w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fstudiovoice.jpg&amp;w=350&amp;q=90 350w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fstudiovoice.jpg&amp;w=400&amp;q=90 400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fstudiovoice.jpg&amp;w=600&amp;q=90 600w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fstudiovoice.jpg&amp;w=640&amp;q=90 640w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fstudiovoice.jpg&amp;w=700&amp;q=90 700w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fstudiovoice.jpg&amp;w=750&amp;q=90 750w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fstudiovoice.jpg&amp;w=828&amp;q=90 828w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fstudiovoice.jpg&amp;w=1080&amp;q=90 1080w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fstudiovoice.jpg&amp;w=1200&amp;q=90 1200w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fstudiovoice.jpg&amp;w=1400&amp;q=90 1400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fstudiovoice.jpg&amp;w=1920&amp;q=90 1920w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fstudiovoice.jpg&amp;w=2048&amp;q=90 2048w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fstudiovoice.jpg&amp;w=3840&amp;q=90 3840w" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/studiovoice.jpeg" style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"><div class="nv-card-media-header"><div class="flex flex-row gap-2"><span class="nv-badge nv-badge--kind-solid nv-badge--color-purple align-bottom" data-testid="nv-badge">Download Available</span></div></div></div><div class="nv-card-content" data-testid="nv-card-content"><div class="flex flex-col gap-2 overflow-hidden"><h3 class="flex w-full flex-col gap-1"><a class="self-start truncate text-sm font-bold lowercase leading-text text-n300 hover:text-primary" href="https://build.nvidia.com/nvidia">nvidia</a><a title="studiovoice" class="style_overlay__cLWaT truncate text-md font-medium leading-heading text-primary" data-linkbox-overlay="true" href="https://build.nvidia.com/nvidia/studiovoice">studiovoice</a></h3><p class="line-clamp-2 text-sm font-normal leading-body text-n300">Enhance speech by correcting common audio degradations to create studio quality speech output.</p></div><div class="relative flex w-full items-center justify-start gap-2 mt-auto min-w-0 max-w-full"><div class="flex items-center gap-2 overflow-hidden"><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=Digital+Human"><span class="inline-block truncate">Digital Human</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=Nvidia+Maxine"><span class="inline-block truncate">Nvidia Maxine</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=Run-on-RTX"><span class="inline-block truncate">Run-on-RTX</span></a></div><button data-testid="nv-popover-trigger" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:r5u:" data-state="closed" class="inline-flex items-center justify-center gap-2 text-center font-sans leading-text flex-row btn-tertiary btn-xs btn-pill btn-inverse font-medium">+2</button></div></div></div></div></div><div><div data-nvtrack="Carousel click" data-nvtrack-adobe-type="carouselItemClick" data-nvtrack-model="/nvidia/bnr" data-nvtrack-model-name="bnr" class="style_linkbox__jnr5J style_root__prJCC cursor-pointer"><div class="nv-card-root nv-card-root--interactive nv-card-root--kind-float nv-card-root--layout-horizontal" data-testid="nv-card-root"><div class="nv-card-media" data-testid="nv-card-media"><img alt="" loading="lazy" decoding="async" data-nimg="fill" class="object-cover object-center" sizes="(max-width: 768px) 150px, 300px" srcset="/_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fbnr.jpg&amp;w=150&amp;q=90 150w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fbnr.jpg&amp;w=300&amp;q=90 300w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fbnr.jpg&amp;w=350&amp;q=90 350w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fbnr.jpg&amp;w=400&amp;q=90 400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fbnr.jpg&amp;w=600&amp;q=90 600w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fbnr.jpg&amp;w=640&amp;q=90 640w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fbnr.jpg&amp;w=700&amp;q=90 700w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fbnr.jpg&amp;w=750&amp;q=90 750w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fbnr.jpg&amp;w=828&amp;q=90 828w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fbnr.jpg&amp;w=1080&amp;q=90 1080w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fbnr.jpg&amp;w=1200&amp;q=90 1200w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fbnr.jpg&amp;w=1400&amp;q=90 1400w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fbnr.jpg&amp;w=1920&amp;q=90 1920w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fbnr.jpg&amp;w=2048&amp;q=90 2048w, /_next/image?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Fimages%2Fbnr.jpg&amp;w=3840&amp;q=90 3840w" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/bnr.jpeg" style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"><div class="nv-card-media-header"><div class="flex flex-row gap-2"><span class="nv-badge nv-badge--kind-solid nv-badge--color-purple align-bottom" data-testid="nv-badge">Download Available</span></div></div></div><div class="nv-card-content" data-testid="nv-card-content"><div class="flex flex-col gap-2 overflow-hidden"><h3 class="flex w-full flex-col gap-1"><a class="self-start truncate text-sm font-bold lowercase leading-text text-n300 hover:text-primary" href="https://build.nvidia.com/nvidia">nvidia</a><a title="Background Noise Removal" class="style_overlay__cLWaT truncate text-md font-medium leading-heading text-primary" data-linkbox-overlay="true" href="https://build.nvidia.com/nvidia/bnr">Background Noise Removal</a></h3><p class="line-clamp-2 text-sm font-normal leading-body text-n300">Removes unwanted noises from audio improving speech intelligibility.</p></div><div class="relative flex w-full items-center justify-start gap-2 mt-auto min-w-0 max-w-full"><div class="flex items-center gap-2 overflow-hidden"><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=Digital+Human"><span class="inline-block truncate">Digital Human</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=Nvidia+Maxine"><span class="inline-block truncate">Nvidia Maxine</span></a><a class="nv-tag nv-tag--color-gray nv-density-compact [[readonly]]:pointer-events-none flex-row lowercase" data-testid="nv-tag-root" href="https://build.nvidia.com/search?label=Speech+Enhancement"><span class="inline-block truncate">Speech Enhancement</span></a></div><button data-testid="nv-popover-trigger" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:r5v:" data-state="closed" class="inline-flex items-center justify-center gap-2 text-center font-sans leading-text flex-row btn-tertiary btn-xs btn-pill btn-inverse font-medium">+1</button></div></div></div></div></div></div></section></div></div></div></div></div><footer class="mt-8 w-full border-t border-white/20 p-4 text-center text-n500"><div class="nv-flex nv-flex--align-stretch nv-flex--direction-row nv-flex--justify-start nv-flex--wrap-nowrap mx-auto mb-2 flex-wrap justify-center gap-x-4" data-testid="nv-flex"><a class="text-sm font-normal hover:text-n000" href="https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA_Technology_Access_TOU.pdf">Terms of Use</a><div aria-orientation="vertical" class="bg-n700 w-px" role="separator"></div><a class="text-sm font-normal hover:text-n000" href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/">Privacy Policy</a><div aria-orientation="vertical" class="bg-n700 w-px" role="separator"></div><a class="text-sm font-normal hover:text-n000" href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/">Your Privacy Choices</a><div aria-orientation="vertical" class="bg-n700 w-px" role="separator"></div><a class="text-sm font-normal hover:text-n000" href="https://developer.nvidia.com/contact">Contact</a></div><p class="text-xs">Copyright © <!-- -->2026<!-- --> NVIDIA Corporation</p></footer></div><!--/$--></main><script>(self.__next_s=self.__next_s||[]).push([0,{"type":"speculationrules","children":"{\"prerender\":[{\"where\":{\"and\":[{\"not\":{\"href_matches\":[\"/internal/agents*\",\"/internal/tools*\",\"/internal/packages*\",\"/settings*\"]}}]},\"eagerness\":\"moderate\"}]}","id":"speculationrules"}])</script><script>(self.__next_s=self.__next_s||[]).push(["https://cdn.cookielaw.org/scripttemplates/otSDKStub.js",{"data-document-language":"true","data-domain-script":"3e2b62ff-7ae7-4ac5-87c8-d5949ecafff5","type":"text/javascript"}])</script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/webpack-5957dab4d02a0e59.js.下载" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/css/f6d332f52e251a32.css\",\"style\"]\n2:HL[\"/_next/static/css/3a01ffc7bcbb63dc.css\",\"style\"]\n3:HL[\"/_next/static/css/2527a870c9602126.css\",\"style\"]\n4:HL[\"/_next/static/css/7a03bc21c9d936ab.css\",\"style\"]\n5:HL[\"/_next/static/css/7991542044add72e.css\",\"style\"]\n6:HL[\"/_next/static/css/315e2c22febd8cea.css\",\"style\"]\n7:HL[\"/_next/static/css/8eeb89dd0ee182d1.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"8:I[958554,[],\"\"]\nb:I[780466,[],\"\"]\nd:I[405071,[],\"\"]\n12:I[25146,[],\"\"]\nc:[\"category\",\"speech\",\"d\"]\n13:[]\n0:[\"$\",\"$L8\",null,{\"buildId\":\"cJFPR41EWDD8EkcS7haWV\",\"assetPrefix\":\"\",\"urlParts\":[\"\",\"explore\",\"speech\"],\"initialTree\":[\"\",{\"children\":[\"(app-bar-layout)\",{\"children\":[\"explore\",{\"children\":[[\"category\",\"speech\",\"d\"],{\"children\":[\"__PAGE__\",{}]}],\"top\":[\"__DEFAULT__\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"(app-bar-layout)\",{\"children\":[\"explore\",{\"children\":[[\"category\",\"speech\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$L9\",\"$La\",null],null],null]},[null,[\"$\",\"$Lb\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(app-bar-layout)\",\"children\",\"explore\",\"children\",\"$c\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Ld\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null],\"top\":[\"__DEFAULT__\",{},[[\"$undefined\",null,null],null],null]},[[null,\"$Le\"],null],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/3a01ffc7bcbb63dc.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/2527a870c9602126.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}],[\"$\",\"link\",\"2\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/7a03bc21c9d936ab.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}],[\"$\",\"link\",\"3\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/7991542044add72e.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}],[\"$\",\"link\",\"4\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/315e2c22febd8cea.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}],[\"$\",\"link\",\"5\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/8eeb89dd0ee182d1.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],\"$Lf\"],null],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/f6d332f52e251a32.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],\"$L10\"],null],null],\"couldBeI"])</script><script>self.__next_f.push([1,"ntercepted\":false,\"initialHead\":[null,\"$L11\"],\"globalErrorComponent\":\"$12\",\"missingSlots\":\"$W13\"}]\n"])</script><script>self.__next_f.push([1,"14:I[864952,[\"80650\",\"static/chunks/cef99e24-d6731e3d04a81d8a.js\",\"42393\",\"static/chunks/1c5dbc3d-c63152837a7859da.js\",\"71075\",\"static/chunks/a9670928-c55d94caff555700.js\",\"97049\",\"static/chunks/b794d9c1-6ad16db09e2cd0a0.js\",\"46781\",\"static/chunks/cb9c0786-c452c7cb5b6570dc.js\",\"88838\",\"static/chunks/df3b6c2d-589577bd4490795d.js\",\"33214\",\"static/chunks/33214-d9197868e7892301.js\",\"45617\",\"static/chunks/45617-cfa1b086b480deb6.js\",\"6753\",\"static/chunks/6753-e4b994d0c8f68c36.js\",\"87307\",\"static/chunks/87307-d266ed7473cbf6ae.js\",\"93690\",\"static/chunks/93690-f540e8a0b150e438.js\",\"13574\",\"static/chunks/13574-6e026190fef980e8.js\",\"98907\",\"static/chunks/98907-e60273a43439fd7b.js\",\"4854\",\"static/chunks/4854-39125d9813ba652b.js\",\"71917\",\"static/chunks/71917-5e847e6cfe5fa495.js\",\"25714\",\"static/chunks/25714-97415bf26ad42b0e.js\",\"38366\",\"static/chunks/38366-ccbd630daa1a2ff8.js\",\"1928\",\"static/chunks/1928-65e1a03bda27fa5e.js\",\"37668\",\"static/chunks/37668-74087f4c53f603ed.js\",\"78476\",\"static/chunks/78476-b8b92bea90e6bda8.js\",\"67642\",\"static/chunks/67642-8949f573d31923ee.js\",\"58739\",\"static/chunks/58739-1aab14e336a381ac.js\",\"17839\",\"static/chunks/17839-68556964a45a9cee.js\",\"68462\",\"static/chunks/68462-de6efca665b84003.js\",\"36834\",\"static/chunks/36834-f7dd14020e490870.js\",\"42274\",\"static/chunks/42274-8dcfe58020717e14.js\",\"19371\",\"static/chunks/app/(app-bar-layout)/explore/%5Bcategory%5D/page-f358e48232b974d9.js\"],\"default\"]\n16:\"$Sreact.suspense\"\n18:I[998953,[\"80650\",\"static/chunks/cef99e24-d6731e3d04a81d8a.js\",\"42393\",\"static/chunks/1c5dbc3d-c63152837a7859da.js\",\"71075\",\"static/chunks/a9670928-c55d94caff555700.js\",\"97049\",\"static/chunks/b794d9c1-6ad16db09e2cd0a0.js\",\"46781\",\"static/chunks/cb9c0786-c452c7cb5b6570dc.js\",\"88838\",\"static/chunks/df3b6c2d-589577bd4490795d.js\",\"33214\",\"static/chunks/33214-d9197868e7892301.js\",\"45617\",\"static/chunks/45617-cfa1b086b480deb6.js\",\"6753\",\"static/chunks/6753-e4b994d0c8f68c36.js\",\"87307\",\"static/chunks/87307-d266ed7473cbf6ae.js\",\"93690\",\"static/chunks/93690-f540e8a0b150e438.js\",\"13574\",\"static/c"])</script><script>self.__next_f.push([1,"hunks/13574-6e026190fef980e8.js\",\"98907\",\"static/chunks/98907-e60273a43439fd7b.js\",\"4854\",\"static/chunks/4854-39125d9813ba652b.js\",\"71917\",\"static/chunks/71917-5e847e6cfe5fa495.js\",\"25714\",\"static/chunks/25714-97415bf26ad42b0e.js\",\"38366\",\"static/chunks/38366-ccbd630daa1a2ff8.js\",\"37668\",\"static/chunks/37668-74087f4c53f603ed.js\",\"78476\",\"static/chunks/78476-b8b92bea90e6bda8.js\",\"39454\",\"static/chunks/39454-c3daaea0f5647c61.js\",\"8962\",\"static/chunks/8962-960d23709bd9a386.js\",\"13151\",\"static/chunks/13151-bf6d9cc282d1a471.js\",\"69692\",\"static/chunks/69692-c2058d2a817e04c0.js\",\"27466\",\"static/chunks/27466-c05096b2b9b5fddb.js\",\"50512\",\"static/chunks/50512-cd0d411310a39fe5.js\",\"32412\",\"static/chunks/32412-36bdc4cbdbdd62e0.js\",\"36038\",\"static/chunks/36038-505a7485b6daabcb.js\",\"80249\",\"static/chunks/80249-9d200a8a17c8b6c2.js\",\"66259\",\"static/chunks/66259-b4272c670d8e5d6b.js\",\"303\",\"static/chunks/app/(app-bar-layout)/explore/layout-2854c4f952cdfb16.js\"],\"SideBar\"]\n19:I[605829,[\"80650\",\"static/chunks/cef99e24-d6731e3d04a81d8a.js\",\"42393\",\"static/chunks/1c5dbc3d-c63152837a7859da.js\",\"71075\",\"static/chunks/a9670928-c55d94caff555700.js\",\"97049\",\"static/chunks/b794d9c1-6ad16db09e2cd0a0.js\",\"46781\",\"static/chunks/cb9c0786-c452c7cb5b6570dc.js\",\"88838\",\"static/chunks/df3b6c2d-589577bd4490795d.js\",\"33214\",\"static/chunks/33214-d9197868e7892301.js\",\"45617\",\"static/chunks/45617-cfa1b086b480deb6.js\",\"6753\",\"static/chunks/6753-e4b994d0c8f68c36.js\",\"87307\",\"static/chunks/87307-d266ed7473cbf6ae.js\",\"93690\",\"static/chunks/93690-f540e8a0b150e438.js\",\"13574\",\"static/chunks/13574-6e026190fef980e8.js\",\"98907\",\"static/chunks/98907-e60273a43439fd7b.js\",\"4854\",\"static/chunks/4854-39125d9813ba652b.js\",\"71917\",\"static/chunks/71917-5e847e6cfe5fa495.js\",\"25714\",\"static/chunks/25714-97415bf26ad42b0e.js\",\"38366\",\"static/chunks/38366-ccbd630daa1a2ff8.js\",\"1928\",\"static/chunks/1928-65e1a03bda27fa5e.js\",\"37668\",\"static/chunks/37668-74087f4c53f603ed.js\",\"78476\",\"static/chunks/78476-b8b92bea90e6bda8.js\",\"39454\",\"static/chunks/39454-c3daaea0f5647c61.js\",\"896"])</script><script>self.__next_f.push([1,"2\",\"static/chunks/8962-960d23709bd9a386.js\",\"13151\",\"static/chunks/13151-bf6d9cc282d1a471.js\",\"69692\",\"static/chunks/69692-c2058d2a817e04c0.js\",\"27466\",\"static/chunks/27466-c05096b2b9b5fddb.js\",\"37410\",\"static/chunks/37410-97fe307bf35e38e3.js\",\"68462\",\"static/chunks/68462-de6efca665b84003.js\",\"32412\",\"static/chunks/32412-36bdc4cbdbdd62e0.js\",\"36038\",\"static/chunks/36038-505a7485b6daabcb.js\",\"80249\",\"static/chunks/80249-9d200a8a17c8b6c2.js\",\"96098\",\"static/chunks/96098-ef4a8f3f80ba4955.js\",\"99369\",\"static/chunks/99369-fd7fd7c25660c530.js\",\"37373\",\"static/chunks/app/(app-bar-layout)/layout-e5d4ea770886a4c1.js\"],\"HydrationBoundary\"]\n1a:I[895755,[\"80650\",\"static/chunks/cef99e24-d6731e3d04a81d8a.js\",\"42393\",\"static/chunks/1c5dbc3d-c63152837a7859da.js\",\"71075\",\"static/chunks/a9670928-c55d94caff555700.js\",\"97049\",\"static/chunks/b794d9c1-6ad16db09e2cd0a0.js\",\"46781\",\"static/chunks/cb9c0786-c452c7cb5b6570dc.js\",\"88838\",\"static/chunks/df3b6c2d-589577bd4490795d.js\",\"33214\",\"static/chunks/33214-d9197868e7892301.js\",\"45617\",\"static/chunks/45617-cfa1b086b480deb6.js\",\"6753\",\"static/chunks/6753-e4b994d0c8f68c36.js\",\"87307\",\"static/chunks/87307-d266ed7473cbf6ae.js\",\"93690\",\"static/chunks/93690-f540e8a0b150e438.js\",\"13574\",\"static/chunks/13574-6e026190fef980e8.js\",\"98907\",\"static/chunks/98907-e60273a43439fd7b.js\",\"4854\",\"static/chunks/4854-39125d9813ba652b.js\",\"71917\",\"static/chunks/71917-5e847e6cfe5fa495.js\",\"25714\",\"static/chunks/25714-97415bf26ad42b0e.js\",\"38366\",\"static/chunks/38366-ccbd630daa1a2ff8.js\",\"1928\",\"static/chunks/1928-65e1a03bda27fa5e.js\",\"37668\",\"static/chunks/37668-74087f4c53f603ed.js\",\"78476\",\"static/chunks/78476-b8b92bea90e6bda8.js\",\"39454\",\"static/chunks/39454-c3daaea0f5647c61.js\",\"8962\",\"static/chunks/8962-960d23709bd9a386.js\",\"13151\",\"static/chunks/13151-bf6d9cc282d1a471.js\",\"69692\",\"static/chunks/69692-c2058d2a817e04c0.js\",\"27466\",\"static/chunks/27466-c05096b2b9b5fddb.js\",\"37410\",\"static/chunks/37410-97fe307bf35e38e3.js\",\"68462\",\"static/chunks/68462-de6efca665b84003.js\",\"32412\",\"static/chunks/32412-36bdc4cb"])</script><script>self.__next_f.push([1,"dbdd62e0.js\",\"36038\",\"static/chunks/36038-505a7485b6daabcb.js\",\"80249\",\"static/chunks/80249-9d200a8a17c8b6c2.js\",\"96098\",\"static/chunks/96098-ef4a8f3f80ba4955.js\",\"99369\",\"static/chunks/99369-fd7fd7c25660c530.js\",\"37373\",\"static/chunks/app/(app-bar-layout)/layout-e5d4ea770886a4c1.js\"],\"Toaster\"]\n1b:I[304860,[\"80650\",\"static/chunks/cef99e24-d6731e3d04a81d8a.js\",\"42393\",\"static/chunks/1c5dbc3d-c63152837a7859da.js\",\"71075\",\"static/chunks/a9670928-c55d94caff555700.js\",\"97049\",\"static/chunks/b794d9c1-6ad16db09e2cd0a0.js\",\"46781\",\"static/chunks/cb9c0786-c452c7cb5b6570dc.js\",\"88838\",\"static/chunks/df3b6c2d-589577bd4490795d.js\",\"33214\",\"static/chunks/33214-d9197868e7892301.js\",\"45617\",\"static/chunks/45617-cfa1b086b480deb6.js\",\"6753\",\"static/chunks/6753-e4b994d0c8f68c36.js\",\"87307\",\"static/chunks/87307-d266ed7473cbf6ae.js\",\"93690\",\"static/chunks/93690-f540e8a0b150e438.js\",\"13574\",\"static/chunks/13574-6e026190fef980e8.js\",\"98907\",\"static/chunks/98907-e60273a43439fd7b.js\",\"4854\",\"static/chunks/4854-39125d9813ba652b.js\",\"71917\",\"static/chunks/71917-5e847e6cfe5fa495.js\",\"25714\",\"static/chunks/25714-97415bf26ad42b0e.js\",\"38366\",\"static/chunks/38366-ccbd630daa1a2ff8.js\",\"1928\",\"static/chunks/1928-65e1a03bda27fa5e.js\",\"37668\",\"static/chunks/37668-74087f4c53f603ed.js\",\"78476\",\"static/chunks/78476-b8b92bea90e6bda8.js\",\"39454\",\"static/chunks/39454-c3daaea0f5647c61.js\",\"8962\",\"static/chunks/8962-960d23709bd9a386.js\",\"13151\",\"static/chunks/13151-bf6d9cc282d1a471.js\",\"69692\",\"static/chunks/69692-c2058d2a817e04c0.js\",\"27466\",\"static/chunks/27466-c05096b2b9b5fddb.js\",\"37410\",\"static/chunks/37410-97fe307bf35e38e3.js\",\"68462\",\"static/chunks/68462-de6efca665b84003.js\",\"32412\",\"static/chunks/32412-36bdc4cbdbdd62e0.js\",\"36038\",\"static/chunks/36038-505a7485b6daabcb.js\",\"80249\",\"static/chunks/80249-9d200a8a17c8b6c2.js\",\"96098\",\"static/chunks/96098-ef4a8f3f80ba4955.js\",\"99369\",\"static/chunks/99369-fd7fd7c25660c530.js\",\"37373\",\"static/chunks/app/(app-bar-layout)/layout-e5d4ea770886a4c1.js\"],\"default\"]\n1c:I[881624,[\"80650\",\"static/chunks/cef99e24"])</script><script>self.__next_f.push([1,"-d6731e3d04a81d8a.js\",\"42393\",\"static/chunks/1c5dbc3d-c63152837a7859da.js\",\"71075\",\"static/chunks/a9670928-c55d94caff555700.js\",\"97049\",\"static/chunks/b794d9c1-6ad16db09e2cd0a0.js\",\"46781\",\"static/chunks/cb9c0786-c452c7cb5b6570dc.js\",\"88838\",\"static/chunks/df3b6c2d-589577bd4490795d.js\",\"33214\",\"static/chunks/33214-d9197868e7892301.js\",\"45617\",\"static/chunks/45617-cfa1b086b480deb6.js\",\"6753\",\"static/chunks/6753-e4b994d0c8f68c36.js\",\"87307\",\"static/chunks/87307-d266ed7473cbf6ae.js\",\"93690\",\"static/chunks/93690-f540e8a0b150e438.js\",\"13574\",\"static/chunks/13574-6e026190fef980e8.js\",\"98907\",\"static/chunks/98907-e60273a43439fd7b.js\",\"4854\",\"static/chunks/4854-39125d9813ba652b.js\",\"71917\",\"static/chunks/71917-5e847e6cfe5fa495.js\",\"25714\",\"static/chunks/25714-97415bf26ad42b0e.js\",\"38366\",\"static/chunks/38366-ccbd630daa1a2ff8.js\",\"1928\",\"static/chunks/1928-65e1a03bda27fa5e.js\",\"37668\",\"static/chunks/37668-74087f4c53f603ed.js\",\"78476\",\"static/chunks/78476-b8b92bea90e6bda8.js\",\"39454\",\"static/chunks/39454-c3daaea0f5647c61.js\",\"8962\",\"static/chunks/8962-960d23709bd9a386.js\",\"13151\",\"static/chunks/13151-bf6d9cc282d1a471.js\",\"69692\",\"static/chunks/69692-c2058d2a817e04c0.js\",\"27466\",\"static/chunks/27466-c05096b2b9b5fddb.js\",\"37410\",\"static/chunks/37410-97fe307bf35e38e3.js\",\"68462\",\"static/chunks/68462-de6efca665b84003.js\",\"32412\",\"static/chunks/32412-36bdc4cbdbdd62e0.js\",\"36038\",\"static/chunks/36038-505a7485b6daabcb.js\",\"80249\",\"static/chunks/80249-9d200a8a17c8b6c2.js\",\"96098\",\"static/chunks/96098-ef4a8f3f80ba4955.js\",\"99369\",\"static/chunks/99369-fd7fd7c25660c530.js\",\"37373\",\"static/chunks/app/(app-bar-layout)/layout-e5d4ea770886a4c1.js\"],\"AppBar\"]\n1d:I[260043,[\"80650\",\"static/chunks/cef99e24-d6731e3d04a81d8a.js\",\"42393\",\"static/chunks/1c5dbc3d-c63152837a7859da.js\",\"71075\",\"static/chunks/a9670928-c55d94caff555700.js\",\"97049\",\"static/chunks/b794d9c1-6ad16db09e2cd0a0.js\",\"46781\",\"static/chunks/cb9c0786-c452c7cb5b6570dc.js\",\"88838\",\"static/chunks/df3b6c2d-589577bd4490795d.js\",\"33214\",\"static/chunks/33214-d9197868e7892301.js\",\"4561"])</script><script>self.__next_f.push([1,"7\",\"static/chunks/45617-cfa1b086b480deb6.js\",\"6753\",\"static/chunks/6753-e4b994d0c8f68c36.js\",\"87307\",\"static/chunks/87307-d266ed7473cbf6ae.js\",\"93690\",\"static/chunks/93690-f540e8a0b150e438.js\",\"13574\",\"static/chunks/13574-6e026190fef980e8.js\",\"98907\",\"static/chunks/98907-e60273a43439fd7b.js\",\"4854\",\"static/chunks/4854-39125d9813ba652b.js\",\"71917\",\"static/chunks/71917-5e847e6cfe5fa495.js\",\"25714\",\"static/chunks/25714-97415bf26ad42b0e.js\",\"38366\",\"static/chunks/38366-ccbd630daa1a2ff8.js\",\"1928\",\"static/chunks/1928-65e1a03bda27fa5e.js\",\"37668\",\"static/chunks/37668-74087f4c53f603ed.js\",\"78476\",\"static/chunks/78476-b8b92bea90e6bda8.js\",\"39454\",\"static/chunks/39454-c3daaea0f5647c61.js\",\"8962\",\"static/chunks/8962-960d23709bd9a386.js\",\"13151\",\"static/chunks/13151-bf6d9cc282d1a471.js\",\"69692\",\"static/chunks/69692-c2058d2a817e04c0.js\",\"27466\",\"static/chunks/27466-c05096b2b9b5fddb.js\",\"37410\",\"static/chunks/37410-97fe307bf35e38e3.js\",\"68462\",\"static/chunks/68462-de6efca665b84003.js\",\"32412\",\"static/chunks/32412-36bdc4cbdbdd62e0.js\",\"36038\",\"static/chunks/36038-505a7485b6daabcb.js\",\"80249\",\"static/chunks/80249-9d200a8a17c8b6c2.js\",\"96098\",\"static/chunks/96098-ef4a8f3f80ba4955.js\",\"99369\",\"static/chunks/99369-fd7fd7c25660c530.js\",\"37373\",\"static/chunks/app/(app-bar-layout)/layout-e5d4ea770886a4c1.js\"],\"default\"]\n1e:I[745617,[\"80650\",\"static/chunks/cef99e24-d6731e3d04a81d8a.js\",\"42393\",\"static/chunks/1c5dbc3d-c63152837a7859da.js\",\"71075\",\"static/chunks/a9670928-c55d94caff555700.js\",\"97049\",\"static/chunks/b794d9c1-6ad16db09e2cd0a0.js\",\"46781\",\"static/chunks/cb9c0786-c452c7cb5b6570dc.js\",\"88838\",\"static/chunks/df3b6c2d-589577bd4490795d.js\",\"33214\",\"static/chunks/33214-d9197868e7892301.js\",\"45617\",\"static/chunks/45617-cfa1b086b480deb6.js\",\"6753\",\"static/chunks/6753-e4b994d0c8f68c36.js\",\"87307\",\"static/chunks/87307-d266ed7473cbf6ae.js\",\"93690\",\"static/chunks/93690-f540e8a0b150e438.js\",\"13574\",\"static/chunks/13574-6e026190fef980e8.js\",\"98907\",\"static/chunks/98907-e60273a43439fd7b.js\",\"4854\",\"static/chunks/4854-39125d9813ba652b.js\","])</script><script>self.__next_f.push([1,"\"71917\",\"static/chunks/71917-5e847e6cfe5fa495.js\",\"25714\",\"static/chunks/25714-97415bf26ad42b0e.js\",\"38366\",\"static/chunks/38366-ccbd630daa1a2ff8.js\",\"1928\",\"static/chunks/1928-65e1a03bda27fa5e.js\",\"37668\",\"static/chunks/37668-74087f4c53f603ed.js\",\"78476\",\"static/chunks/78476-b8b92bea90e6bda8.js\",\"67642\",\"static/chunks/67642-8949f573d31923ee.js\",\"58739\",\"static/chunks/58739-1aab14e336a381ac.js\",\"17839\",\"static/chunks/17839-68556964a45a9cee.js\",\"68462\",\"static/chunks/68462-de6efca665b84003.js\",\"36834\",\"static/chunks/36834-f7dd14020e490870.js\",\"42274\",\"static/chunks/42274-8dcfe58020717e14.js\",\"19371\",\"static/chunks/app/(app-bar-layout)/explore/%5Bcategory%5D/page-f358e48232b974d9.js\"],\"\"]\n1f:\"$Sreact.fragment\"\n21:I[180851,[\"80650\",\"static/chunks/cef99e24-d6731e3d04a81d8a.js\",\"42393\",\"static/chunks/1c5dbc3d-c63152837a7859da.js\",\"71075\",\"static/chunks/a9670928-c55d94caff555700.js\",\"97049\",\"static/chunks/b794d9c1-6ad16db09e2cd0a0.js\",\"46781\",\"static/chunks/cb9c0786-c452c7cb5b6570dc.js\",\"88838\",\"static/chunks/df3b6c2d-589577bd4490795d.js\",\"33214\",\"static/chunks/33214-d9197868e7892301.js\",\"45617\",\"static/chunks/45617-cfa1b086b480deb6.js\",\"6753\",\"static/chunks/6753-e4b994d0c8f68c36.js\",\"87307\",\"static/chunks/87307-d266ed7473cbf6ae.js\",\"93690\",\"static/chunks/93690-f540e8a0b150e438.js\",\"13574\",\"static/chunks/13574-6e026190fef980e8.js\",\"98907\",\"static/chunks/98907-e60273a43439fd7b.js\",\"4854\",\"static/chunks/4854-39125d9813ba652b.js\",\"71917\",\"static/chunks/71917-5e847e6cfe5fa495.js\",\"25714\",\"static/chunks/25714-97415bf26ad42b0e.js\",\"38366\",\"static/chunks/38366-ccbd630daa1a2ff8.js\",\"1928\",\"static/chunks/1928-65e1a03bda27fa5e.js\",\"37668\",\"static/chunks/37668-74087f4c53f603ed.js\",\"78476\",\"static/chunks/78476-b8b92bea90e6bda8.js\",\"67642\",\"static/chunks/67642-8949f573d31923ee.js\",\"58739\",\"static/chunks/58739-1aab14e336a381ac.js\",\"17839\",\"static/chunks/17839-68556964a45a9cee.js\",\"68462\",\"static/chunks/68462-de6efca665b84003.js\",\"36834\",\"static/chunks/36834-f7dd14020e490870.js\",\"42274\",\"static/chunks/42274-8dcfe58020717e14.js\",\"1937"])</script><script>self.__next_f.push([1,"1\",\"static/chunks/app/(app-bar-layout)/explore/%5Bcategory%5D/page-f358e48232b974d9.js\"],\"default\"]\n22:I[553344,[\"80650\",\"static/chunks/cef99e24-d6731e3d04a81d8a.js\",\"42393\",\"static/chunks/1c5dbc3d-c63152837a7859da.js\",\"71075\",\"static/chunks/a9670928-c55d94caff555700.js\",\"97049\",\"static/chunks/b794d9c1-6ad16db09e2cd0a0.js\",\"46781\",\"static/chunks/cb9c0786-c452c7cb5b6570dc.js\",\"88838\",\"static/chunks/df3b6c2d-589577bd4490795d.js\",\"33214\",\"static/chunks/33214-d9197868e7892301.js\",\"45617\",\"static/chunks/45617-cfa1b086b480deb6.js\",\"6753\",\"static/chunks/6753-e4b994d0c8f68c36.js\",\"87307\",\"static/chunks/87307-d266ed7473cbf6ae.js\",\"93690\",\"static/chunks/93690-f540e8a0b150e438.js\",\"13574\",\"static/chunks/13574-6e026190fef980e8.js\",\"98907\",\"static/chunks/98907-e60273a43439fd7b.js\",\"4854\",\"static/chunks/4854-39125d9813ba652b.js\",\"71917\",\"static/chunks/71917-5e847e6cfe5fa495.js\",\"25714\",\"static/chunks/25714-97415bf26ad42b0e.js\",\"38366\",\"static/chunks/38366-ccbd630daa1a2ff8.js\",\"1928\",\"static/chunks/1928-65e1a03bda27fa5e.js\",\"37668\",\"static/chunks/37668-74087f4c53f603ed.js\",\"78476\",\"static/chunks/78476-b8b92bea90e6bda8.js\",\"67642\",\"static/chunks/67642-8949f573d31923ee.js\",\"58739\",\"static/chunks/58739-1aab14e336a381ac.js\",\"17839\",\"static/chunks/17839-68556964a45a9cee.js\",\"68462\",\"static/chunks/68462-de6efca665b84003.js\",\"36834\",\"static/chunks/36834-f7dd14020e490870.js\",\"42274\",\"static/chunks/42274-8dcfe58020717e14.js\",\"19371\",\"static/chunks/app/(app-bar-layout)/explore/%5Bcategory%5D/page-f358e48232b974d9.js\"],\"AuthContextProvider\"]\n23:I[262447,[\"80650\",\"static/chunks/cef99e24-d6731e3d04a81d8a.js\",\"42393\",\"static/chunks/1c5dbc3d-c63152837a7859da.js\",\"71075\",\"static/chunks/a9670928-c55d94caff555700.js\",\"97049\",\"static/chunks/b794d9c1-6ad16db09e2cd0a0.js\",\"46781\",\"static/chunks/cb9c0786-c452c7cb5b6570dc.js\",\"88838\",\"static/chunks/df3b6c2d-589577bd4490795d.js\",\"33214\",\"static/chunks/33214-d9197868e7892301.js\",\"45617\",\"static/chunks/45617-cfa1b086b480deb6.js\",\"6753\",\"static/chunks/6753-e4b994d0c8f68c36.js\",\"87307\",\"static/chunks/8730"])</script><script>self.__next_f.push([1,"7-d266ed7473cbf6ae.js\",\"93690\",\"static/chunks/93690-f540e8a0b150e438.js\",\"13574\",\"static/chunks/13574-6e026190fef980e8.js\",\"98907\",\"static/chunks/98907-e60273a43439fd7b.js\",\"4854\",\"static/chunks/4854-39125d9813ba652b.js\",\"71917\",\"static/chunks/71917-5e847e6cfe5fa495.js\",\"25714\",\"static/chunks/25714-97415bf26ad42b0e.js\",\"38366\",\"static/chunks/38366-ccbd630daa1a2ff8.js\",\"1928\",\"static/chunks/1928-65e1a03bda27fa5e.js\",\"37668\",\"static/chunks/37668-74087f4c53f603ed.js\",\"78476\",\"static/chunks/78476-b8b92bea90e6bda8.js\",\"67642\",\"static/chunks/67642-8949f573d31923ee.js\",\"58739\",\"static/chunks/58739-1aab14e336a381ac.js\",\"17839\",\"static/chunks/17839-68556964a45a9cee.js\",\"68462\",\"static/chunks/68462-de6efca665b84003.js\",\"36834\",\"static/chunks/36834-f7dd14020e490870.js\",\"42274\",\"static/chunks/42274-8dcfe58020717e14.js\",\"19371\",\"static/chunks/app/(app-bar-layout)/explore/%5Bcategory%5D/page-f358e48232b974d9.js\"],\"SelectedModelProvider\"]\n24:I[367055,[\"80650\",\"static/chunks/cef99e24-d6731e3d04a81d8a.js\",\"42393\",\"static/chunks/1c5dbc3d-c63152837a7859da.js\",\"71075\",\"static/chunks/a9670928-c55d94caff555700.js\",\"97049\",\"static/chunks/b794d9c1-6ad16db09e2cd0a0.js\",\"46781\",\"static/chunks/cb9c0786-c452c7cb5b6570dc.js\",\"88838\",\"static/chunks/df3b6c2d-589577bd4490795d.js\",\"33214\",\"static/chunks/33214-d9197868e7892301.js\",\"45617\",\"static/chunks/45617-cfa1b086b480deb6.js\",\"6753\",\"static/chunks/6753-e4b994d0c8f68c36.js\",\"87307\",\"static/chunks/87307-d266ed7473cbf6ae.js\",\"93690\",\"static/chunks/93690-f540e8a0b150e438.js\",\"13574\",\"static/chunks/13574-6e026190fef980e8.js\",\"98907\",\"static/chunks/98907-e60273a43439fd7b.js\",\"4854\",\"static/chunks/4854-39125d9813ba652b.js\",\"71917\",\"static/chunks/71917-5e847e6cfe5fa495.js\",\"25714\",\"static/chunks/25714-97415bf26ad42b0e.js\",\"38366\",\"static/chunks/38366-ccbd630daa1a2ff8.js\",\"1928\",\"static/chunks/1928-65e1a03bda27fa5e.js\",\"37668\",\"static/chunks/37668-74087f4c53f603ed.js\",\"78476\",\"static/chunks/78476-b8b92bea90e6bda8.js\",\"67642\",\"static/chunks/67642-8949f573d31923ee.js\",\"58739\",\"static/chunks/58739-1aab14"])</script><script>self.__next_f.push([1,"e336a381ac.js\",\"17839\",\"static/chunks/17839-68556964a45a9cee.js\",\"68462\",\"static/chunks/68462-de6efca665b84003.js\",\"36834\",\"static/chunks/36834-f7dd14020e490870.js\",\"42274\",\"static/chunks/42274-8dcfe58020717e14.js\",\"19371\",\"static/chunks/app/(app-bar-layout)/explore/%5Bcategory%5D/page-f358e48232b974d9.js\"],\"default\"]\n25:I[84046,[\"33214\",\"static/chunks/33214-d9197868e7892301.js\",\"39454\",\"static/chunks/39454-c3daaea0f5647c61.js\",\"49158\",\"static/chunks/49158-fe7688694f8dea7b.js\",\"97601\",\"static/chunks/app/error-ccc96de798cf1672.js\"],\"default\"]\n26:I[743520,[\"80650\",\"static/chunks/cef99e24-d6731e3d04a81d8a.js\",\"42393\",\"static/chunks/1c5dbc3d-c63152837a7859da.js\",\"71075\",\"static/chunks/a9670928-c55d94caff555700.js\",\"97049\",\"static/chunks/b794d9c1-6ad16db09e2cd0a0.js\",\"46781\",\"static/chunks/cb9c0786-c452c7cb5b6570dc.js\",\"88838\",\"static/chunks/df3b6c2d-589577bd4490795d.js\",\"33214\",\"static/chunks/33214-d9197868e7892301.js\",\"45617\",\"static/chunks/45617-cfa1b086b480deb6.js\",\"6753\",\"static/chunks/6753-e4b994d0c8f68c36.js\",\"87307\",\"static/chunks/87307-d266ed7473cbf6ae.js\",\"93690\",\"static/chunks/93690-f540e8a0b150e438.js\",\"13574\",\"static/chunks/13574-6e026190fef980e8.js\",\"98907\",\"static/chunks/98907-e60273a43439fd7b.js\",\"4854\",\"static/chunks/4854-39125d9813ba652b.js\",\"71917\",\"static/chunks/71917-5e847e6cfe5fa495.js\",\"25714\",\"static/chunks/25714-97415bf26ad42b0e.js\",\"38366\",\"static/chunks/38366-ccbd630daa1a2ff8.js\",\"1928\",\"static/chunks/1928-65e1a03bda27fa5e.js\",\"37668\",\"static/chunks/37668-74087f4c53f603ed.js\",\"78476\",\"static/chunks/78476-b8b92bea90e6bda8.js\",\"67642\",\"static/chunks/67642-8949f573d31923ee.js\",\"58739\",\"static/chunks/58739-1aab14e336a381ac.js\",\"17839\",\"static/chunks/17839-68556964a45a9cee.js\",\"68462\",\"static/chunks/68462-de6efca665b84003.js\",\"36834\",\"static/chunks/36834-f7dd14020e490870.js\",\"42274\",\"static/chunks/42274-8dcfe58020717e14.js\",\"19371\",\"static/chunks/app/(app-bar-layout)/explore/%5Bcategory%5D/page-f358e48232b974d9.js\"],\"OsClassProvider\"]\n27:I[746292,[\"80650\",\"static/chunks/cef99e24-d6731e3d04a81d8a."])</script><script>self.__next_f.push([1,"js\",\"42393\",\"static/chunks/1c5dbc3d-c63152837a7859da.js\",\"71075\",\"static/chunks/a9670928-c55d94caff555700.js\",\"97049\",\"static/chunks/b794d9c1-6ad16db09e2cd0a0.js\",\"46781\",\"static/chunks/cb9c0786-c452c7cb5b6570dc.js\",\"88838\",\"static/chunks/df3b6c2d-589577bd4490795d.js\",\"33214\",\"static/chunks/33214-d9197868e7892301.js\",\"45617\",\"static/chunks/45617-cfa1b086b480deb6.js\",\"6753\",\"static/chunks/6753-e4b994d0c8f68c36.js\",\"87307\",\"static/chunks/87307-d266ed7473cbf6ae.js\",\"93690\",\"static/chunks/93690-f540e8a0b150e438.js\",\"13574\",\"static/chunks/13574-6e026190fef980e8.js\",\"98907\",\"static/chunks/98907-e60273a43439fd7b.js\",\"4854\",\"static/chunks/4854-39125d9813ba652b.js\",\"71917\",\"static/chunks/71917-5e847e6cfe5fa495.js\",\"25714\",\"static/chunks/25714-97415bf26ad42b0e.js\",\"38366\",\"static/chunks/38366-ccbd630daa1a2ff8.js\",\"1928\",\"static/chunks/1928-65e1a03bda27fa5e.js\",\"37668\",\"static/chunks/37668-74087f4c53f603ed.js\",\"78476\",\"static/chunks/78476-b8b92bea90e6bda8.js\",\"67642\",\"static/chunks/67642-8949f573d31923ee.js\",\"58739\",\"static/chunks/58739-1aab14e336a381ac.js\",\"17839\",\"static/chunks/17839-68556964a45a9cee.js\",\"68462\",\"static/chunks/68462-de6efca665b84003.js\",\"36834\",\"static/chunks/36834-f7dd14020e490870.js\",\"42274\",\"static/chunks/42274-8dcfe58020717e14.js\",\"19371\",\"static/chunks/app/(app-bar-layout)/explore/%5Bcategory%5D/page-f358e48232b974d9.js\"],\"CSPReporter\"]\n28:I[668531,[\"80650\",\"static/chunks/cef99e24-d6731e3d04a81d8a.js\",\"42393\",\"static/chunks/1c5dbc3d-c63152837a7859da.js\",\"71075\",\"static/chunks/a9670928-c55d94caff555700.js\",\"97049\",\"static/chunks/b794d9c1-6ad16db09e2cd0a0.js\",\"46781\",\"static/chunks/cb9c0786-c452c7cb5b6570dc.js\",\"88838\",\"static/chunks/df3b6c2d-589577bd4490795d.js\",\"33214\",\"static/chunks/33214-d9197868e7892301.js\",\"45617\",\"static/chunks/45617-cfa1b086b480deb6.js\",\"6753\",\"static/chunks/6753-e4b994d0c8f68c36.js\",\"87307\",\"static/chunks/87307-d266ed7473cbf6ae.js\",\"93690\",\"static/chunks/93690-f540e8a0b150e438.js\",\"13574\",\"static/chunks/13574-6e026190fef980e8.js\",\"98907\",\"static/chunks/98907-e60273a43439fd7b.j"])</script><script>self.__next_f.push([1,"s\",\"4854\",\"static/chunks/4854-39125d9813ba652b.js\",\"71917\",\"static/chunks/71917-5e847e6cfe5fa495.js\",\"25714\",\"static/chunks/25714-97415bf26ad42b0e.js\",\"38366\",\"static/chunks/38366-ccbd630daa1a2ff8.js\",\"1928\",\"static/chunks/1928-65e1a03bda27fa5e.js\",\"37668\",\"static/chunks/37668-74087f4c53f603ed.js\",\"78476\",\"static/chunks/78476-b8b92bea90e6bda8.js\",\"67642\",\"static/chunks/67642-8949f573d31923ee.js\",\"58739\",\"static/chunks/58739-1aab14e336a381ac.js\",\"17839\",\"static/chunks/17839-68556964a45a9cee.js\",\"68462\",\"static/chunks/68462-de6efca665b84003.js\",\"36834\",\"static/chunks/36834-f7dd14020e490870.js\",\"42274\",\"static/chunks/42274-8dcfe58020717e14.js\",\"19371\",\"static/chunks/app/(app-bar-layout)/explore/%5Bcategory%5D/page-f358e48232b974d9.js\"],\"AdobeAnalytics\"]\n29:I[119547,[\"80650\",\"static/chunks/cef99e24-d6731e3d04a81d8a.js\",\"42393\",\"static/chunks/1c5dbc3d-c63152837a7859da.js\",\"71075\",\"static/chunks/a9670928-c55d94caff555700.js\",\"97049\",\"static/chunks/b794d9c1-6ad16db09e2cd0a0.js\",\"46781\",\"static/chunks/cb9c0786-c452c7cb5b6570dc.js\",\"88838\",\"static/chunks/df3b6c2d-589577bd4490795d.js\",\"33214\",\"static/chunks/33214-d9197868e7892301.js\",\"45617\",\"static/chunks/45617-cfa1b086b480deb6.js\",\"6753\",\"static/chunks/6753-e4b994d0c8f68c36.js\",\"87307\",\"static/chunks/87307-d266ed7473cbf6ae.js\",\"93690\",\"static/chunks/93690-f540e8a0b150e438.js\",\"13574\",\"static/chunks/13574-6e026190fef980e8.js\",\"98907\",\"static/chunks/98907-e60273a43439fd7b.js\",\"4854\",\"static/chunks/4854-39125d9813ba652b.js\",\"71917\",\"static/chunks/71917-5e847e6cfe5fa495.js\",\"25714\",\"static/chunks/25714-97415bf26ad42b0e.js\",\"38366\",\"static/chunks/38366-ccbd630daa1a2ff8.js\",\"1928\",\"static/chunks/1928-65e1a03bda27fa5e.js\",\"37668\",\"static/chunks/37668-74087f4c53f603ed.js\",\"78476\",\"static/chunks/78476-b8b92bea90e6bda8.js\",\"67642\",\"static/chunks/67642-8949f573d31923ee.js\",\"58739\",\"static/chunks/58739-1aab14e336a381ac.js\",\"17839\",\"static/chunks/17839-68556964a45a9cee.js\",\"68462\",\"static/chunks/68462-de6efca665b84003.js\",\"36834\",\"static/chunks/36834-f7dd14020e490870.js\",\"42274\",\"sta"])</script><script>self.__next_f.push([1,"tic/chunks/42274-8dcfe58020717e14.js\",\"19371\",\"static/chunks/app/(app-bar-layout)/explore/%5Bcategory%5D/page-f358e48232b974d9.js\"],\"\"]\na:[[\"$\",\"$L14\",null,{\"sidebarNav\":{\"workstations\":[{\"label\":\"Run on RTX\",\"url\":\"/explore/run-on-rtx\"},{\"label\":\"Run on Spark\",\"url\":\"/spark\"}],\"models\":[{\"label\":\"Reasoning\",\"url\":\"/explore/reasoning\"},{\"label\":\"Vision\",\"url\":\"/explore/vision\"},{\"label\":\"Visual Design\",\"url\":\"/explore/visual-design\"},{\"label\":\"Retrieval\",\"url\":\"/explore/retrieval\"},{\"label\":\"Speech\",\"url\":\"/explore/speech\"},{\"label\":\"Biology\",\"url\":\"/explore/biology\"},{\"label\":\"Simulation\",\"url\":\"/explore/simulation\"},{\"label\":\"Climate \u0026 Weather\",\"url\":\"/explore/climate-weather\"},{\"label\":\"Safety \u0026 Moderation\",\"url\":\"/explore/safety-moderation\"}],\"industries\":[{\"label\":\"Automotive\",\"url\":\"/explore/automotive\"},{\"label\":\"Financial Services\",\"url\":\"/explore/financial-services\"},{\"label\":\"Gaming\",\"url\":\"/explore/gaming\"},{\"label\":\"Healthcare\",\"url\":\"/explore/healthcare\"},{\"label\":\"Industrial\",\"url\":\"/explore/industrial\"},{\"label\":\"Robotics\",\"url\":\"/explore/robotics\"}]},\"children\":\"Speech\"}],\"$L15\"]\n"])</script><script>self.__next_f.push([1,"e:[[\"$\",\"div\",null,{\"className\":\"mb-4 border-b border-n700 p-4 lg:p-8\",\"children\":[\"$\",\"$16\",null,{\"children\":[\"$L17\",[\"$\",\"$Lb\",null,{\"parallelRouterKey\":\"top\",\"segmentPath\":[\"children\",\"(app-bar-layout)\",\"children\",\"explore\",\"top\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Ld\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"p-4 lg:p-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"mx-auto flex max-w-[1600px]\",\"children\":[[\"$\",\"div\",null,{\"className\":\"hidden w-[220px] lg:block\",\"children\":[\"$\",\"$L18\",null,{\"initialNavItemsData\":[{\"label\":\"Explore\",\"url\":\"/explore/discover\",\"selectedPath\":\"/explore{/:category}\"},{\"label\":\"Models\",\"url\":\"/models\"},{\"label\":\"Blueprints\",\"url\":\"/blueprints\"},{\"label\":\"Internal AI\",\"url\":\"/agents\",\"isInternal\":true,\"selectedPath\":\"/internal{/*page}\"},{\"label\":\"GPUs\",\"url\":\"https://brev.nvidia.com/environment/new/public\"},{\"label\":\"Docs\",\"url\":\"https://docs.api.nvidia.com/\",\"isExternal\":true}],\"initialSidebarData\":{\"workstations\":[{\"label\":\"Run on RTX\",\"url\":\"/explore/run-on-rtx\"},{\"label\":\"Run on Spark\",\"url\":\"/spark\"}],\"models\":[{\"label\":\"Reasoning\",\"url\":\"/explore/reasoning\"},{\"label\":\"Vision\",\"url\":\"/explore/vision\"},{\"label\":\"Visual Design\",\"url\":\"/explore/visual-design\"},{\"label\":\"Retrieval\",\"url\":\"/explore/retrieval\"},{\"label\":\"Speech\",\"url\":\"/explore/speech\"},{\"label\":\"Biology\",\"url\":\"/explore/biology\"},{\"label\":\"Simulation\",\"url\":\"/explore/simulation\"},{\"label\":\"Climate \u0026 Weather\",\"url\":\"/explore/climate-weather\"},{\"label\":\"Safety \u0026 Moderation\",\"url\":\"/explore/safety-moderation\"}],\"industries\":[{\"label\":\"Automotive\",\"url\":\"/explore/automotive\"},{\"label\":\"Financial Services\",\"url\":\"/explore/financial-services\"},{\"label\":\"Gaming\",\"url\":\"/explore/gaming\"},{\"label\":\"Healthcare\",\"url\":\"/explore/healthcare\"},{\"label\":\"Industrial\",\"url\":\"/explore/industrial\"},{\"label\":\"Robotics\",\"url\":\"/explore/robotics\"}]}}]}],[\"$\",\"div\",null,{\"className\":\"flex w-full min-w-0 flex-1 flex-col border-n700 lg:border-l lg:pl-8\",\"children\":[\"$\",\"$Lb\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(app-bar-layout)\",\"children\",\"explore\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Ld\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]}]]}]}]]\n"])</script><script>self.__next_f.push([1,"20:[\"$\",\"div\",null,{\"aria-orientation\":\"vertical\",\"className\":\"bg-n700 w-px\",\"role\":\"separator\"}]\n"])</script><script>self.__next_f.push([1,"f:[\"$\",\"$L19\",null,{\"state\":{\"mutations\":[],\"queries\":[{\"state\":{\"data\":null,\"dataUpdateCount\":1,\"dataUpdatedAt\":1770323124840,\"error\":null,\"errorUpdateCount\":0,\"errorUpdatedAt\":0,\"fetchFailureCount\":0,\"fetchFailureReason\":null,\"fetchMeta\":null,\"isInvalidated\":false,\"status\":\"success\",\"fetchStatus\":\"idle\"},\"queryKey\":[\"notices/global.json\"],\"queryHash\":\"[\\\"notices/global.json\\\"]\"},{\"state\":{\"data\":1,\"dataUpdateCount\":1,\"dataUpdatedAt\":1770323124840,\"error\":null,\"errorUpdateCount\":0,\"errorUpdatedAt\":0,\"fetchFailureCount\":0,\"fetchFailureReason\":null,\"fetchMeta\":null,\"isInvalidated\":false,\"status\":\"success\",\"fetchStatus\":\"idle\"},\"queryKey\":[\"amplitude/session-sample-rate.number\"],\"queryHash\":\"[\\\"amplitude/session-sample-rate.number\\\"]\"},{\"state\":{\"data\":true,\"dataUpdateCount\":1,\"dataUpdatedAt\":1770323124840,\"error\":null,\"errorUpdateCount\":0,\"errorUpdatedAt\":0,\"fetchFailureCount\":0,\"fetchFailureReason\":null,\"fetchMeta\":null,\"isInvalidated\":false,\"status\":\"success\",\"fetchStatus\":\"idle\"},\"queryKey\":[\"killswitches/segment.enabled\"],\"queryHash\":\"[\\\"killswitches/segment.enabled\\\"]\"},{\"state\":{\"data\":true,\"dataUpdateCount\":1,\"dataUpdatedAt\":1770323124840,\"error\":null,\"errorUpdateCount\":0,\"errorUpdatedAt\":0,\"fetchFailureCount\":0,\"fetchFailureReason\":null,\"fetchMeta\":null,\"isInvalidated\":false,\"status\":\"success\",\"fetchStatus\":\"idle\"},\"queryKey\":[\"killswitches/adobe.enabled\"],\"queryHash\":\"[\\\"killswitches/adobe.enabled\\\"]\"},{\"state\":{\"data\":true,\"dataUpdateCount\":1,\"dataUpdatedAt\":1770323124840,\"error\":null,\"errorUpdateCount\":0,\"errorUpdatedAt\":0,\"fetchFailureCount\":0,\"fetchFailureReason\":null,\"fetchMeta\":null,\"isInvalidated\":false,\"status\":\"success\",\"fetchStatus\":\"idle\"},\"queryKey\":[\"killswitches/amplitude.enabled\"],\"queryHash\":\"[\\\"killswitches/amplitude.enabled\\\"]\"},{\"state\":{\"data\":false,\"dataUpdateCount\":1,\"dataUpdatedAt\":1770323124839,\"error\":null,\"errorUpdateCount\":0,\"errorUpdatedAt\":0,\"fetchFailureCount\":0,\"fetchFailureReason\":null,\"fetchMeta\":null,\"isInvalidated\":false,\"status\":\"success\",\"fetchStatus\":\"idle\"},\"queryKey\":[\"killswitches/customer-service-agent.enabled\"],\"queryHash\":\"[\\\"killswitches/customer-service-agent.enabled\\\"]\"},{\"state\":{\"data\":false,\"dataUpdateCount\":1,\"dataUpdatedAt\":1770323124839,\"error\":null,\"errorUpdateCount\":0,\"errorUpdatedAt\":0,\"fetchFailureCount\":0,\"fetchFailureReason\":null,\"fetchMeta\":null,\"isInvalidated\":false,\"status\":\"success\",\"fetchStatus\":\"idle\"},\"queryKey\":[\"killswitches/search-palette.enabled\"],\"queryHash\":\"[\\\"killswitches/search-palette.enabled\\\"]\"},{\"state\":{\"data\":false,\"dataUpdateCount\":1,\"dataUpdatedAt\":1770323124839,\"error\":null,\"errorUpdateCount\":0,\"errorUpdatedAt\":0,\"fetchFailureCount\":0,\"fetchFailureReason\":null,\"fetchMeta\":null,\"isInvalidated\":false,\"status\":\"success\",\"fetchStatus\":\"idle\"},\"queryKey\":[\"killswitches/search-page-v2.enabled\"],\"queryHash\":\"[\\\"killswitches/search-page-v2.enabled\\\"]\"},{\"state\":{\"data\":false,\"dataUpdateCount\":1,\"dataUpdatedAt\":1770323124839,\"error\":null,\"errorUpdateCount\":0,\"errorUpdatedAt\":0,\"fetchFailureCount\":0,\"fetchFailureReason\":null,\"fetchMeta\":null,\"isInvalidated\":false,\"status\":\"success\",\"fetchStatus\":\"idle\"},\"queryKey\":[\"killswitches/artifact-card-v2.enabled\"],\"queryHash\":\"[\\\"killswitches/artifact-card-v2.enabled\\\"]\"},{\"state\":{\"data\":null,\"dataUpdateCount\":1,\"dataUpdatedAt\":1770323124840,\"error\":null,\"errorUpdateCount\":0,\"errorUpdatedAt\":0,\"fetchFailureCount\":0,\"fetchFailureReason\":null,\"fetchMeta\":null,\"isInvalidated\":false,\"status\":\"success\",\"fetchStatus\":\"idle\"},\"queryKey\":[\"killswitches/search-playbooks.enabled\"],\"queryHash\":\"[\\\"killswitches/search-playbooks.enabled\\\"]\"},{\"state\":{\"data\":null,\"dataUpdateCount\":1,\"dataUpdatedAt\":1770323124840,\"error\":null,\"errorUpdateCount\":0,\"errorUpdatedAt\":0,\"fetchFailureCount\":0,\"fetchFailureReason\":null,\"fetchMeta\":null,\"isInvalidated\":false,\"status\":\"success\",\"fetchStatus\":\"idle\"},\"queryKey\":[\"killswitches/search-nemo-services.enabled\"],\"queryHash\":\"[\\\"killswitches/search-nemo-services.enabled\\\"]\"}]},\"children\":[[\"$\",\"$L1a\",null,{\"position\":\"top-center\"}],[\"$\",\"$L1b\",null,{}],[\"$\",\"$L1c\",null,{\"initialFlags\":{\"notices/global.json\":null,\"amplitude/session-sample-rate.number\":1,\"killswitches/segment.enabled\":true,\"killswitches/adobe.enabled\":true,\"killswitches/amplitude.enabled\":true,\"killswitches/customer-service-agent.enabled\":false,\"killswitches/search-palette.enabled\":false,\"killswitches/search-page-v2.enabled\":false,\"killswitches/artifact-card-v2.enabled\":false,\"killswitches/search-playbooks.enabled\":null,\"killswitches/search-nemo-services.enabled\":null},\"navItems\":[{\"label\":\"Explore\",\"url\":\"/explore/discover\",\"selectedPath\":\"/explore{/:category}\"},{\"label\":\"Models\",\"url\":\"/models\"},{\"label\":\"Blueprints\",\"url\":\"/blueprints\"},{\"label\":\"Internal AI\",\"url\":\"/agents\",\"isInternal\":true,\"selectedPath\":\"/internal{/*page}\"},{\"label\":\"GPUs\",\"url\":\"https://brev.nvidia.com/environment/new/public\"},{\"label\":\"Docs\",\"url\":\"https://docs.api.nvidia.com/\",\"isExternal\":true}]}],[\"$\",\"$L1d\",null,{}],[\"$\",\"div\",null,{\"className\":\"flex min-h-[calc(100dvh-var(--app-bar-height))] flex-col items-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"w-full flex-1\",\"children\":[\"$\",\"$Lb\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"(app-bar-layout)\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Ld\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"className\":\"flex min-h-[75vh] flex-col items-center justify-center gap-md text-manitoulinLightWhite\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-lg font-medium\",\"children\":\"404 | Not Found\"}],[\"$\",\"$L1e\",null,{\"href\":\"/\",\"children\":[\"Return Home\"],\"className\":\"inline-flex items-center justify-center gap-2 text-center font-sans font-bold leading-text flex-row btn-tertiary btn-md btn-rounded\"}]]}],\"notFoundStyles\":[]}]}],[\"$\",\"footer\",null,{\"className\":\"mt-8 w-full border-t border-white/20 p-4 text-center text-n500\",\"children\":[[\"$\",\"div\",null,{\"className\":\"nv-flex nv-flex--align-stretch nv-flex--direction-row nv-flex--justify-start nv-flex--wrap-nowrap mx-auto mb-2 flex-wrap justify-center gap-x-4\",\"data-testid\":\"nv-flex\",\"children\":[[\"$\",\"$1f\",\"0\",{\"children\":[[\"$\",\"a\",\".$Terms of Use\",{\"className\":\"text-sm font-normal hover:text-n000\",\"href\":\"https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA_Technology_Access_TOU.pdf\",\"children\":\"Terms of Use\"}],[\"$\",\"div\",null,{\"aria-orientation\":\"vertical\",\"className\":\"bg-n700 w-px\",\"role\":\"separator\"}]]}],[\"$\",\"$1f\",\"1\",{\"children\":[[\"$\",\"a\",\".$Privacy Policy\",{\"className\":\"text-sm font-normal hover:text-n000\",\"href\":\"https://www.nvidia.com/en-us/about-nvidia/privacy-policy/\",\"children\":\"Privacy Policy\"}],\"$20\"]}],[\"$\",\"$1f\",\"2\",{\"children\":[[\"$\",\"a\",\".$Your Privacy Choices\",{\"className\":\"text-sm font-normal hover:text-n000\",\"href\":\"https://www.nvidia.com/en-us/about-nvidia/privacy-center/\",\"children\":\"Your Privacy Choices\"}],\"$20\"]}],[\"$\",\"$1f\",\"3\",{\"children\":[[\"$\",\"a\",\".$Contact\",{\"className\":\"text-sm font-normal hover:text-n000\",\"href\":\"https://developer.nvidia.com/contact\",\"children\":\"Contact\"}],false]}]]}],[\"$\",\"p\",null,{\"className\":\"text-xs\",\"children\":[\"Copyright © \",2026,\" NVIDIA Corporation\"]}]]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"10:[\"$\",\"html\",null,{\"className\":\"nv-dark bg-transparent font-sans\",\"id\":\"app\",\"lang\":\"en\",\"children\":[[\"$\",\"head\",null,{\"children\":false}],[\"$\",\"body\",null,{\"className\":\"bg-manitoulinLightBlack\",\"data-nv-source\":null,\"data-scroll-lock\":false,\"children\":[[\"$\",\"main\",null,{\"children\":[\"$\",\"$16\",null,{\"children\":[\"$\",\"$L21\",null,{\"children\":[\"$\",\"$L22\",null,{\"initialData\":{\"currentUser\":\"$undefined\",\"userContext\":\"$undefined\",\"userSubscriptions\":\"$undefined\"},\"children\":[\"$\",\"$L23\",null,{\"children\":[\"$\",\"$L24\",null,{\"children\":[\"$\",\"$Lb\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$25\",\"errorStyles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/315e2c22febd8cea.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],\"errorScripts\":[],\"template\":[\"$\",\"$Ld\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"div\",null,{\"className\":\"flex min-h-[75vh] flex-col items-center justify-center gap-md text-manitoulinLightWhite\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-lg font-medium\",\"children\":\"404 | Not Found\"}],[\"$\",\"$L1e\",null,{\"href\":\"/\",\"children\":[\"Return Home\"],\"className\":\"inline-flex items-center justify-center gap-2 text-center font-sans font-bold leading-text flex-row btn-tertiary btn-md btn-rounded\"}]]}],\"notFoundStyles\":[]}]}]}]}]}]}]}],[\"$\",\"$L26\",null,{}],[\"$\",\"$L27\",null,{}],[\"$\",\"$L28\",null,{}],[\"$\",\"$L29\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"prerender\\\":[{\\\"where\\\":{\\\"and\\\":[{\\\"not\\\":{\\\"href_matches\\\":[\\\"/internal/agents*\\\",\\\"/internal/tools*\\\",\\\"/internal/packages*\\\",\\\"/settings*\\\"]}}]},\\\"eagerness\\\":\\\"moderate\\\"}]}\"},\"id\":\"speculationrules\",\"strategy\":\"beforeInteractive\",\"type\":\"speculationrules\"}],[[\"$\",\"$L29\",null,{\"data-document-language\":\"true\",\"data-domain-script\":\"3e2b62ff-7ae7-4ac5-87c8-d5949ecafff5\",\"src\":\"https://cdn.cookielaw.org/scripttemplates/otSDKStub.js\",\"strategy\":\"beforeInteractive\",\"type\":\"text/javascript\"}],[\"$\",\"$L29\",null,{\"async\":true,\"id\":\"OptanonWrapper\",\"children\":\"function OptanonWrapper() {\\n                  const event = new Event('bannerLoaded');\\n                  window.dispatchEvent(event);\\n                }\"}],[\"$\",\"$L29\",null,{\"async\":true,\"id\":\"GlobalPrivacyControl\",\"src\":\"https://images.nvidia.com/aem-dam/Solutions/ot-js/ot-custom.js\"}]]]}]]}]\n"])</script><script>self.__next_f.push([1,"11:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Explore Speech Models | Try NVIDIA NIM APIs\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Experience the leading models to build enterprise generative AI apps now.\"}],[\"$\",\"link\",\"4\",{\"rel\":\"canonical\",\"href\":\"https://build.nvidia.com/explore/speech\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:title\",\"content\":\"Explore Speech Models | Try NVIDIA NIM APIs\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:description\",\"content\":\"Experience the leading models to build enterprise generative AI apps now.\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:image:type\",\"content\":\"image/jpeg\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:image:width\",\"content\":\"1200\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:image:height\",\"content\":\"630\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:image\",\"content\":\"https://build.nvidia.com/opengraph-image.jpg?6ec102a0470b935b\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:title\",\"content\":\"Explore Speech Models | Try NVIDIA NIM APIs\"}],[\"$\",\"meta\",\"14\",{\"name\":\"twitter:description\",\"content\":\"Experience the leading models to build enterprise generative AI apps now.\"}],[\"$\",\"meta\",\"15\",{\"name\":\"twitter:image:type\",\"content\":\"image/jpeg\"}],[\"$\",\"meta\",\"16\",{\"name\":\"twitter:image:width\",\"content\":\"1200\"}],[\"$\",\"meta\",\"17\",{\"name\":\"twitter:image:height\",\"content\":\"630\"}],[\"$\",\"meta\",\"18\",{\"name\":\"twitter:image\",\"content\":\"https://build.nvidia.com/twitter-image.jpg?6ec102a0470b935b\"}],[\"$\",\"link\",\"19\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"48x48\"}]]\n9:null\n"])</script><script>self.__next_f.push([1,"2a:I[550280,[\"80650\",\"static/chunks/cef99e24-d6731e3d04a81d8a.js\",\"42393\",\"static/chunks/1c5dbc3d-c63152837a7859da.js\",\"71075\",\"static/chunks/a9670928-c55d94caff555700.js\",\"97049\",\"static/chunks/b794d9c1-6ad16db09e2cd0a0.js\",\"46781\",\"static/chunks/cb9c0786-c452c7cb5b6570dc.js\",\"88838\",\"static/chunks/df3b6c2d-589577bd4490795d.js\",\"33214\",\"static/chunks/33214-d9197868e7892301.js\",\"45617\",\"static/chunks/45617-cfa1b086b480deb6.js\",\"6753\",\"static/chunks/6753-e4b994d0c8f68c36.js\",\"87307\",\"static/chunks/87307-d266ed7473cbf6ae.js\",\"93690\",\"static/chunks/93690-f540e8a0b150e438.js\",\"13574\",\"static/chunks/13574-6e026190fef980e8.js\",\"98907\",\"static/chunks/98907-e60273a43439fd7b.js\",\"4854\",\"static/chunks/4854-39125d9813ba652b.js\",\"71917\",\"static/chunks/71917-5e847e6cfe5fa495.js\",\"25714\",\"static/chunks/25714-97415bf26ad42b0e.js\",\"38366\",\"static/chunks/38366-ccbd630daa1a2ff8.js\",\"37668\",\"static/chunks/37668-74087f4c53f603ed.js\",\"78476\",\"static/chunks/78476-b8b92bea90e6bda8.js\",\"39454\",\"static/chunks/39454-c3daaea0f5647c61.js\",\"8962\",\"static/chunks/8962-960d23709bd9a386.js\",\"13151\",\"static/chunks/13151-bf6d9cc282d1a471.js\",\"69692\",\"static/chunks/69692-c2058d2a817e04c0.js\",\"27466\",\"static/chunks/27466-c05096b2b9b5fddb.js\",\"50512\",\"static/chunks/50512-cd0d411310a39fe5.js\",\"32412\",\"static/chunks/32412-36bdc4cbdbdd62e0.js\",\"36038\",\"static/chunks/36038-505a7485b6daabcb.js\",\"80249\",\"static/chunks/80249-9d200a8a17c8b6c2.js\",\"66259\",\"static/chunks/66259-b4272c670d8e5d6b.js\",\"303\",\"static/chunks/app/(app-bar-layout)/explore/layout-2854c4f952cdfb16.js\"],\"PlaygroundContextProvider\"]\n2b:I[987358,[\"80650\",\"static/chunks/cef99e24-d6731e3d04a81d8a.js\",\"42393\",\"static/chunks/1c5dbc3d-c63152837a7859da.js\",\"71075\",\"static/chunks/a9670928-c55d94caff555700.js\",\"97049\",\"static/chunks/b794d9c1-6ad16db09e2cd0a0.js\",\"46781\",\"static/chunks/cb9c0786-c452c7cb5b6570dc.js\",\"88838\",\"static/chunks/df3b6c2d-589577bd4490795d.js\",\"33214\",\"static/chunks/33214-d9197868e7892301.js\",\"45617\",\"static/chunks/45617-cfa1b086b480deb6.js\",\"6753\",\"static/chunks/6753-e4b994d"])</script><script>self.__next_f.push([1,"0c8f68c36.js\",\"87307\",\"static/chunks/87307-d266ed7473cbf6ae.js\",\"93690\",\"static/chunks/93690-f540e8a0b150e438.js\",\"13574\",\"static/chunks/13574-6e026190fef980e8.js\",\"98907\",\"static/chunks/98907-e60273a43439fd7b.js\",\"4854\",\"static/chunks/4854-39125d9813ba652b.js\",\"71917\",\"static/chunks/71917-5e847e6cfe5fa495.js\",\"25714\",\"static/chunks/25714-97415bf26ad42b0e.js\",\"38366\",\"static/chunks/38366-ccbd630daa1a2ff8.js\",\"37668\",\"static/chunks/37668-74087f4c53f603ed.js\",\"78476\",\"static/chunks/78476-b8b92bea90e6bda8.js\",\"39454\",\"static/chunks/39454-c3daaea0f5647c61.js\",\"8962\",\"static/chunks/8962-960d23709bd9a386.js\",\"13151\",\"static/chunks/13151-bf6d9cc282d1a471.js\",\"69692\",\"static/chunks/69692-c2058d2a817e04c0.js\",\"27466\",\"static/chunks/27466-c05096b2b9b5fddb.js\",\"50512\",\"static/chunks/50512-cd0d411310a39fe5.js\",\"32412\",\"static/chunks/32412-36bdc4cbdbdd62e0.js\",\"36038\",\"static/chunks/36038-505a7485b6daabcb.js\",\"80249\",\"static/chunks/80249-9d200a8a17c8b6c2.js\",\"66259\",\"static/chunks/66259-b4272c670d8e5d6b.js\",\"303\",\"static/chunks/app/(app-bar-layout)/explore/layout-2854c4f952cdfb16.js\"],\"APIKeyButton\"]\n2c:I[978848,[\"80650\",\"static/chunks/cef99e24-d6731e3d04a81d8a.js\",\"42393\",\"static/chunks/1c5dbc3d-c63152837a7859da.js\",\"71075\",\"static/chunks/a9670928-c55d94caff555700.js\",\"97049\",\"static/chunks/b794d9c1-6ad16db09e2cd0a0.js\",\"46781\",\"static/chunks/cb9c0786-c452c7cb5b6570dc.js\",\"88838\",\"static/chunks/df3b6c2d-589577bd4490795d.js\",\"33214\",\"static/chunks/33214-d9197868e7892301.js\",\"45617\",\"static/chunks/45617-cfa1b086b480deb6.js\",\"6753\",\"static/chunks/6753-e4b994d0c8f68c36.js\",\"87307\",\"static/chunks/87307-d266ed7473cbf6ae.js\",\"93690\",\"static/chunks/93690-f540e8a0b150e438.js\",\"13574\",\"static/chunks/13574-6e026190fef980e8.js\",\"98907\",\"static/chunks/98907-e60273a43439fd7b.js\",\"4854\",\"static/chunks/4854-39125d9813ba652b.js\",\"71917\",\"static/chunks/71917-5e847e6cfe5fa495.js\",\"25714\",\"static/chunks/25714-97415bf26ad42b0e.js\",\"38366\",\"static/chunks/38366-ccbd630daa1a2ff8.js\",\"1928\",\"static/chunks/1928-65e1a03bda27fa5e.js\",\"37668\",\"static/ch"])</script><script>self.__next_f.push([1,"unks/37668-74087f4c53f603ed.js\",\"78476\",\"static/chunks/78476-b8b92bea90e6bda8.js\",\"67642\",\"static/chunks/67642-8949f573d31923ee.js\",\"58739\",\"static/chunks/58739-1aab14e336a381ac.js\",\"17839\",\"static/chunks/17839-68556964a45a9cee.js\",\"68462\",\"static/chunks/68462-de6efca665b84003.js\",\"36834\",\"static/chunks/36834-f7dd14020e490870.js\",\"42274\",\"static/chunks/42274-8dcfe58020717e14.js\",\"19371\",\"static/chunks/app/(app-bar-layout)/explore/%5Bcategory%5D/page-f358e48232b974d9.js\"],\"default\"]\n2d:[\"$\",\"div\",null,{\"aria-orientation\":\"vertical\",\"className\":\"bg-n700 w-px mx-2 max-[1056px]:hidden\",\"role\":\"separator\"}]\n"])</script><script>self.__next_f.push([1,"17:[\"$\",\"$L2a\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"mx-auto flex max-w-[1600px] flex-col gap-4\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col gap-4 min-[400px]:flex-row\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-1 flex-col justify-between gap-4 rounded-2 border border-gray-700 bg-n900 p-4 shadow-[0_4px_6px_0_rgba(0,0,0,0.12)]\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex justify-between gap-2\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col flex-wrap gap-2 self-start xs:flex-row xs:items-center\",\"children\":[\"$undefined\",[\"$\",\"h2\",null,{\"className\":\"tracking-less flex items-center gap-1 text-md font-medium leading-text text-primary xs:gap-2\",\"children\":[\"$undefined\",[\"$\",\"span\",null,{\"className\":\"-mb-1 inline-block font-bold leading-heading max-lg:text-sm\",\"children\":\"Deploy Models Now with NVIDIA NIM\"}]]}],[\"$\",\"span\",null,{\"className\":\"text-sm font-normal leading-body\",\"children\":\"Optimized inference for the world’s leading models\"}]]}],[\"$\",\"$L2b\",null,{\"className\":\"self-start text-nowrap\",\"size\":\"md\",\"tone\":\"primary\"}]]}],[\"$\",\"div\",null,{\"className\":\"nv-flex nv-flex--align-stretch nv-flex--direction-col nv-flex--justify-start nv-flex--wrap-nowrap min-h-8 flex-1 flex-row gap-2 rounded-2 bg-black/50 px-2 py-1.5 text-sm leading-body xs:px-4 max-[1056px]:flex-col max-[1056px]:p-2\",\"data-testid\":\"nv-flex\",\"role\":\"list\",\"children\":[[\"$\",\"$1f\",\"0\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center justify-center self-stretch flex-initial flex-wrap max-[1056px]:justify-start\",\"children\":[\"$\",\"div\",null,{\"className\":\"mr-2 flex gap-2 text-nowrap\",\"role\":\"listitem\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"fill\":\"none\",\"viewBox\":\"0 0 16 16\",\"width\":\"16\",\"height\":\"16\",\"display\":\"inline-block\",\"data-icon-name\":\"check\",\"className\":\"mt-px text-brand\",\"children\":[\"$\",\"path\",null,{\"fill\":\"currentColor\",\"d\":\"M14.354 4.354 6 12.707 1.646 8.354l.708-.708L6 11.293l7.647-7.647z\"}]}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap items-center gap-2\",\"children\":[\"Free serverless APIs for development\",[\"$\",\"$L2c\",null,{\"attributes\":{\"Link\":{\"href\":\"https://www.nvidia.com/en-us/data-center/dgx-cloud/\",\"target\":\"_blank\"}},\"density\":\"compact\",\"iconSlot\":[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"fill\":\"none\",\"viewBox\":\"0 0 16 16\",\"width\":\"1em\",\"height\":\"1em\",\"display\":\"inline-block\",\"data-icon-name\":\"cloud\",\"children\":[\"$\",\"path\",null,{\"fill\":\"currentColor\",\"d\":\"M8 2a4.5 4.5 0 0 1 4.14 2.732A4.25 4.25 0 0 1 10.75 13H4.5a3.501 3.501 0 0 1-.986-6.86A4.5 4.5 0 0 1 8 2\"}]}],\"children\":\"Accelerated by DGX Cloud\"}]]}]]}]}],[\"$\",\"div\",null,{\"aria-orientation\":\"vertical\",\"className\":\"bg-n700 w-px mx-2 max-[1056px]:hidden\",\"role\":\"separator\"}]]}],[\"$\",\"$1f\",\"1\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-1 items-center justify-center self-stretch max-[1056px]:justify-start\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex gap-2 text-nowrap\",\"role\":\"listitem\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"fill\":\"none\",\"viewBox\":\"0 0 16 16\",\"width\":\"16\",\"height\":\"16\",\"display\":\"inline-block\",\"data-icon-name\":\"check\",\"className\":\"mt-px text-brand\",\"children\":[\"$\",\"path\",null,{\"fill\":\"currentColor\",\"d\":\"M14.354 4.354 6 12.707 1.646 8.354l.708-.708L6 11.293l7.647-7.647z\"}]}],\"Self-Host on your GPU infrastructure\"]}]}],\"$2d\"]}],[\"$\",\"$1f\",\"2\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-1 items-center justify-center self-stretch max-[1056px]:justify-start\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex gap-2 text-nowrap\",\"role\":\"listitem\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"fill\":\"none\",\"viewBox\":\"0 0 16 16\",\"width\":\"16\",\"height\":\"16\",\"display\":\"inline-block\",\"data-icon-name\":\"check\",\"className\":\"mt-px text-brand\",\"children\":[\"$\",\"path\",null,{\"fill\":\"currentColor\",\"d\":\"M14.354 4.354 6 12.707 1.646 8.354l.708-.708L6 11.293l7.647-7.647z\"}]}],\"Continuous vulnerability fixes\"]}]}],false]}]]}]]}]}]}]}]\n"])</script><script>self.__next_f.push([1,"30:[\"$\",\"div\",null,{\"aria-orientation\":\"horizontal\",\"className\":\"bg-n700 h-px w-full\",\"role\":\"separator\"}]\n15:[[\"$\",\"div\",null,{\"className\":\"nv-flex nv-flex--align-stretch nv-flex--direction-col nv-flex--justify-start nv-flex--wrap-nowrap flex flex-col gap-8\",\"data-testid\":\"nv-flex\",\"children\":[[\"$\",\"$1f\",\"0\",{\"children\":[[\"$\",\"$16\",\".0:$0\",{\"children\":\"$L2e\"}],[\"$\",\"div\",null,{\"aria-orientation\":\"horizontal\",\"className\":\"bg-n700 h-px w-full\",\"role\":\"separator\"}]]}],[\"$\",\"$1f\",\"1\",{\"children\":[[\"$\",\"$16\",\".0:$1\",{\"children\":\"$L2f\"}],\"$30\"]}],[\"$\",\"$1f\",\"2\",{\"children\":[[\"$\",\"$16\",\".0:$2\",{\"children\":\"$L31\"}],\"$30\"]}],[\"$\",\"$1f\",\"3\",{\"children\":[[\"$\",\"$16\",\".0:$3\",{\"children\":\"$L32\"}],false]}]]}],\"$undefined\"]\n"])</script><script>self.__next_f.push([1,"36:I[990639,[\"80650\",\"static/chunks/cef99e24-d6731e3d04a81d8a.js\",\"42393\",\"static/chunks/1c5dbc3d-c63152837a7859da.js\",\"71075\",\"static/chunks/a9670928-c55d94caff555700.js\",\"97049\",\"static/chunks/b794d9c1-6ad16db09e2cd0a0.js\",\"46781\",\"static/chunks/cb9c0786-c452c7cb5b6570dc.js\",\"88838\",\"static/chunks/df3b6c2d-589577bd4490795d.js\",\"33214\",\"static/chunks/33214-d9197868e7892301.js\",\"45617\",\"static/chunks/45617-cfa1b086b480deb6.js\",\"6753\",\"static/chunks/6753-e4b994d0c8f68c36.js\",\"87307\",\"static/chunks/87307-d266ed7473cbf6ae.js\",\"93690\",\"static/chunks/93690-f540e8a0b150e438.js\",\"13574\",\"static/chunks/13574-6e026190fef980e8.js\",\"98907\",\"static/chunks/98907-e60273a43439fd7b.js\",\"4854\",\"static/chunks/4854-39125d9813ba652b.js\",\"71917\",\"static/chunks/71917-5e847e6cfe5fa495.js\",\"25714\",\"static/chunks/25714-97415bf26ad42b0e.js\",\"38366\",\"static/chunks/38366-ccbd630daa1a2ff8.js\",\"1928\",\"static/chunks/1928-65e1a03bda27fa5e.js\",\"37668\",\"static/chunks/37668-74087f4c53f603ed.js\",\"78476\",\"static/chunks/78476-b8b92bea90e6bda8.js\",\"67642\",\"static/chunks/67642-8949f573d31923ee.js\",\"58739\",\"static/chunks/58739-1aab14e336a381ac.js\",\"17839\",\"static/chunks/17839-68556964a45a9cee.js\",\"68462\",\"static/chunks/68462-de6efca665b84003.js\",\"36834\",\"static/chunks/36834-f7dd14020e490870.js\",\"42274\",\"static/chunks/42274-8dcfe58020717e14.js\",\"19371\",\"static/chunks/app/(app-bar-layout)/explore/%5Bcategory%5D/page-f358e48232b974d9.js\"],\"default\"]\n37:I[62735,[\"80650\",\"static/chunks/cef99e24-d6731e3d04a81d8a.js\",\"42393\",\"static/chunks/1c5dbc3d-c63152837a7859da.js\",\"71075\",\"static/chunks/a9670928-c55d94caff555700.js\",\"97049\",\"static/chunks/b794d9c1-6ad16db09e2cd0a0.js\",\"46781\",\"static/chunks/cb9c0786-c452c7cb5b6570dc.js\",\"88838\",\"static/chunks/df3b6c2d-589577bd4490795d.js\",\"33214\",\"static/chunks/33214-d9197868e7892301.js\",\"45617\",\"static/chunks/45617-cfa1b086b480deb6.js\",\"6753\",\"static/chunks/6753-e4b994d0c8f68c36.js\",\"87307\",\"static/chunks/87307-d266ed7473cbf6ae.js\",\"93690\",\"static/chunks/93690-f540e8a0b150e438.js\",\"13574\",\"static/chunks/13574-6e026190fef"])</script><script>self.__next_f.push([1,"980e8.js\",\"98907\",\"static/chunks/98907-e60273a43439fd7b.js\",\"4854\",\"static/chunks/4854-39125d9813ba652b.js\",\"71917\",\"static/chunks/71917-5e847e6cfe5fa495.js\",\"25714\",\"static/chunks/25714-97415bf26ad42b0e.js\",\"38366\",\"static/chunks/38366-ccbd630daa1a2ff8.js\",\"1928\",\"static/chunks/1928-65e1a03bda27fa5e.js\",\"37668\",\"static/chunks/37668-74087f4c53f603ed.js\",\"78476\",\"static/chunks/78476-b8b92bea90e6bda8.js\",\"67642\",\"static/chunks/67642-8949f573d31923ee.js\",\"58739\",\"static/chunks/58739-1aab14e336a381ac.js\",\"17839\",\"static/chunks/17839-68556964a45a9cee.js\",\"68462\",\"static/chunks/68462-de6efca665b84003.js\",\"36834\",\"static/chunks/36834-f7dd14020e490870.js\",\"42274\",\"static/chunks/42274-8dcfe58020717e14.js\",\"19371\",\"static/chunks/app/(app-bar-layout)/explore/%5Bcategory%5D/page-f358e48232b974d9.js\"],\"default\"]\n2e:[\"$\",\"section\",null,{\"data-testid\":\"explore-carousel-view\",\"children\":[\"$\",\"div\",null,{\"className\":\"relative\",\"children\":[\"$\",\"$16\",null,{\"children\":\"$L33\"}]}]}]\n2f:[\"$\",\"section\",null,{\"data-testid\":\"explore-carousel-view\",\"children\":[\"$\",\"div\",null,{\"className\":\"relative\",\"children\":[\"$\",\"$16\",null,{\"children\":\"$L34\"}]}]}]\n31:[\"$\",\"section\",null,{\"data-testid\":\"explore-carousel-view\",\"children\":[\"$\",\"div\",null,{\"className\":\"relative\",\"children\":[\"$\",\"$16\",null,{\"children\":\"$L35\"}]}]}]\n38:T1a78,"])</script><script>self.__next_f.push([1,"# Model Overview\n\n## Description\n\nMaxine Studio Voice enhances the input speech recorded through low quality \nmicrophones in noisy/reverberant environment to studio-recorded quality speech.\n\nStudio Voice is available under NVIDIA Maxine — a developer platform for \ndeploying AI features that enhance audio, video, and creating new experiences \nin real-time audio-video communication. Maxine's state-of-the-art models create \nhigh-quality AI effects using standard microphones and cameras without \nadditional special equipments.\n\nNVIDIA Maxine is exclusively part of NVIDIA AI Enterprise for production \nworkflows — an extensive library of full-stack software, including AI solution \nworkflows, frameworks, pre-trained models, and infrastructure optimization.  \n\n\n## Terms of use\n\nThe use of NVIDIA Maxine's Studio Voice is available as a demonstration of the \ninput and output of the Studio Voice generative model. As such the user may \nupload an audio file or select one of the sample inputs and download the \ngenerated audio for evaluation under the terms of the \n[NVIDIA MAXINE EVALUATION LICENSE AGREEMENT](https://developer.download.nvidia.com/maxine/nvidia-maxine-evaluation-license-24oct2023.pdf).  \n\n\n## References(s):\n\n* [NVIDIA Maxine](https://developer.nvidia.com/maxine)  \n\n\n## Model Architecture\n\n**Architecture Type:** Convolution Neural Networks (CNNs), Transformers,\n                       Generative Adversarial Networks (GANs)  \n**Network Architecture:** Encoder-Decoder  \n**Model Version:** 0.2  \n\n\n## Input:\n\n**Input Type(s):** Ordered List (audio samples)  \n**Input Format(s):** FP32 (-1.0 to 1.0)  \n**Other Properties Related to Input:** Pulse Code Modulation (PCM) audio samples\nwith no encoding or pre-processing; 16kHz or 48kHz sampling rate required.  \n\n## Output:\n\n**Output Type(s):** Ordered List (audio samples)  \n**Output Format:** FP32 (-1.0 to 1.0)  \n**Other Properties Related to Output:** PCM audio samples at input sampling rate\nwith no encoding or post-processing.  \n\n## Software Integration\n\n**Supported Hardware Platform(s):** Hopper, Ada, Ampere, Turing, Volta  \n**Test Hardware:** A10, L40, T10  \n**Supported Operating System(s):** Linux, Windows  \n\n\n# Training \u0026 Evaluation\n\n## Datasets\n\nNVIDIA models are trained on a diverse set of public and proprietary datasets.\nThe Studio Voice model is trained on a dataset that comprises of diverse \nEnglish accents.\n\n**Link:** [DAPS](https://ccrma.stanford.edu/~gautham/Site/daps.html)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nThe DAPS dataset has 15 versions of audio (3 professional versions and\n12 consumer device/real-world environment combinations). Each version consists\nof about 4.5 hours of data (about 14 minutes from each of 20 speakers).  \n\n**Link:** [LibriTTS](https://www.openslr.org/60/)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nLibriTTS is a multi-speaker English corpus of approximately 585 hours of read\nEnglish speech, which is resampled at 16kHZ.  \n\n**Link:** [VCTK](https://datashare.ed.ac.uk/handle/10283/3443)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nThis CSTR VCTK Corpus includes speech data uttered by 110 English speakers with\nvarious accents. Each speaker reads out about 400 sentences, which were selected\nfrom a newspaper, the rainbow passage and an elicitation paragraph used for the\nspeech accent archive.  \n\n**Link:** [HiFi-TTS](https://www.openslr.org/109/)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nA multi-speaker English dataset for training text-to-speech models.\nThe HiFi-TTS dataset contains about 291.6 hours of speech from 10 speakers with\nat least 17 hours per speaker sampled at 44.1 kHz.  \n\n**Link:** [Device Recorded VCTK (DR-VCTK)](https://github.com/nii-yamagishilab/downloader-DR-VCTK-complete)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nDevice recorded version of VCTK dataset on common consumer devices \n(laptop, tablet and smartphone) in office environment. This dataset contains \n109 English speakers with different accents. There are around 400 sentences \navailable from each speaker. For this recording, 8 different microphones were \nused. This dataset contains around 250 Gb of re-recorded speech.  \n\n**Link:** [Dataset of impulse responses from variable acoustics room Arni at Aalto Acoustic Labs](https://zenodo.org/records/6985104#.Y7_vv3ZBy3C)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nA dataset of impulse responses collected in the variable acoustics laboratory \nArni at Acoustics Lab of Aalto University, Espoo, Finland. IRs of 5342 \nconfigurations of sound absorption in Arni are included in the dataset. Each of \nthem were measured using an omnidirectional sound source and 5 sound receivers. \nFor each configuration, 5 impulse reponses (IRs) were captured. The total number\nof measurements in the dataset is 132 037.  \n\n**Link:** [Room Impulse Response and Noise Database](https://www.openslr.org/28/)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nA database of simulated and real room impulse responses, isotropic and\npoint-source noises. The audio files in this data are all in 16KHz sampling rate\nand 16-bit precision.  \n\n**Link:** [DNS Challenge 5](https://github.com/microsoft/DNS-Challenge/tree/2db96d5f75257df764a6ef66513b4b97bc707f30)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nCollated dataset of clean speech, noise and impulse response provided by \nMicrosoft for the ICASSP 2023 Deep Noise Suppression Challenge.  \n\n**Link:** [AudioSet](https://research.google.com/audioset/download.html)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nAudioSet consists of an expanding ontology of 632 audio event classes and \na collection of 2,084,320 human-labeled 10-second sound clips drawn from \nYouTube videos.  \n\n\n# Inference\n\n**Engine:** [Triton](https://developer.nvidia.com/triton-inference-server)  \n**Test Hardware:** A10, L40, T10  \n\n## Ethical Considerations\n\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established\npolicies and practices to enable development for a wide array of AI applications.\nWhen downloaded or used in accordance with our terms of service, developers\nshould work with their supporting model team to ensure this model meets\nrequirements for the relevant industry and use case and addresses unforeseen\nproduct misuse. For more detailed information on ethical considerations for this\nmodel, please see the Model Card++ [Explainability](explainability), [Bias](bias),\n[Safety \u0026 Security](safety-and-security), and [Privacy](privacy) Subcards. Please\nreport security vulnerabilities or NVIDIA AI Concerns\n[here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/)."])</script><script>self.__next_f.push([1,"39:T59f,| Field | Response |\n|-------|----------|\n| Intended Application \u0026 Domain: | Voice enhancement |\n| Model Type:                    | Speech Enhancement |\n| Intended Users:                | Content developers and Broadcasters |\n| Output:                        | Audio |\n| Describe how the model works:  | This model enhances the voice in input audio by removing acoustic noise, room reverberation, and bad frequency characteristics of the recording device while maintaining the linguistic and vocal properties. |\n| Name the adversely impacted groups this has been tested to deliver comparable outcomes regardless of: | Age (18+), Gender |\n| Verified to have met prescribed quality standards: | Yes |\n| Target Key Performance Indicator(s) (KPI(s)):      | Quality Mean Opinion Score (MOS), Non-Intrusive Speech Quality Assessment (NISQA), Latency, Throughput |\n| Technical Limitations: | The model may not work work well on a variety of demographic and regional representations of English or with very noisy or very low quality inputs. Exclaimatory emotions may not be transferred to the output. |\n| Potential Known Risks: | The model may not preserve some speaker characteristics like pitch/fundamental frequency of the speakers voice and might also impact intelligibility of speech. |\n| Licensing: | [NVIDIA Maxine Evaluation License Agreement](https://developer.download.nvidia.com/maxine/nvidia-maxine-evaluation-license-24oct2023.pdf) |3a:T40d,| Field | Response |\n|-------|----------|\n| Generatable or reverse engineerable personally-identifiable information (PII)? | None |\n| Was consent obtained for any PII used?          | Yes |\n| Protected class data used to create this model? | None |\n| How often is dataset reviewed?                  | Before Release |\n| Is a mechanism in place to honor data subject right of access or deletion of personal data? | No |\n| If PII collected for the development of the model, was it collected directly by NVIDIA?     | No |\n| If PII collected for the development of the model by NVIDIA, do you mai"])</script><script>self.__next_f.push([1,"ntain or have access to disclosures made to data subjects? | No |\n| If PII collected for the development of this AI model, was it minimized to only what was required? | Yes |\n| Is there provenance for all datasets used in training?              | Yes |\n| Does data labeling (annotation, metadata) comply with privacy laws? | Yes |\n| Is data compliant with data subject requests for data correction or removal, if such a request was made? | No |3b:T2409,"])</script><script>self.__next_f.push([1,"# Model Overview\n\n## Description\n\nMaxine Background Noise Removal (BNR) model is an audio background noise removal\nmodel from NVIDIA. It removes a variety of background noises from audio recordings.\nIt also retains emotive tones in speech, such as happy, sad, excited and angry tones\n\nBNR is available under NVIDIA Maxine — a developer platform for \ndeploying AI features that enhance audio, video, and creating new experiences \nin real-time audio-video communication. Maxine's state-of-the-art models create \nhigh-quality AI effects using standard microphones and cameras without \nadditional special equipments.\n\nNVIDIA Maxine is exclusively part of NVIDIA AI Enterprise for production \nworkflows — an extensive library of full-stack software, including AI solution \nworkflows, frameworks, pre-trained models, and infrastructure optimization.  \n\n\n## Terms of use\n\nThe use of NVIDIA Maxine's BNR is available as a demonstration of the \ninput and output of the BNR generative model. As such the user may \nupload an audio file or select one of the sample inputs and download the \ngenerated audio for evaluation under the terms of the \n[NVIDIA MAXINE EVALUATION LICENSE AGREEMENT](https://developer.download.nvidia.com/maxine/nvidia-maxine-evaluation-license-24oct2023.pdf).  \n\n\u003cb\u003eYou are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws\u003c/b\u003e\n\n## Reference(s):\n\n* [NVIDIA Maxine](https://developer.nvidia.com/maxine)\n\n\n## Model Architecture:\n**Architecture Type:** Residual Convolutional Recurrent Neural Network (CRNN)\n\n**Network Architecture:** SEASR\n\n**Model Version:** 1.0\n\n\n## Input:\n\n**Input Type(s):** Ordered List (audio samples)\n\n**Input Format(s):** FP32 (-1.0 to 1.0)\n\n**Other Properties Related to Input:** Pulse Code Modulation (PCM) audio samples\n\nwith no encoding or pre-processing; 16kHz or 48kHz sampling rate required.  \n\n## Output:\n\n**Output Type(s):** Ordered List (audio samples)\n\n**Output Format:** FP32 (-1.0 to 1.0)\n\n**Other Properties Related to Output:** PCM audio samples at input sampling rate\n\nwith no encoding or post-processing.  \n\n## Software Integration\n\n**Supported Hardware Microarchitecture Compatibility:**\n* Volta\n* Turing\n* Ada\n* Ampere\n* Hopper\n* Blackwell\n\n**Supported Operating System(s):**\n* Linux\n\n\n# Training \u0026 Evaluation\n\n## Datasets\n\nNVIDIA models are trained on a diverse set of public and proprietary datasets.\nThe BNR model is trained on a wide range of English language accents, \nsome European and Asian languanges, and 29 different noise profiles that are \ncommonly audible in real world. \n\n**Data Collection Method by dataset:** [Hybrid: Human, Synthetic]  \n\n**Labeling Method by dataset:** [Hybrid: Human, Synthetic]  \n\n**Link:** [AudioSet](https://research.google.com/audioset/)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nAudioSet consists of an expanding ontology of 632 audio event classes and \na collection of 2,084,320 human-labeled 10-second sound clips drawn from \nYouTube videos.  \n\n**Link:** [CREMA-D](https://www.kaggle.com/ejlok1/cremad)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nCREMA-D is a data set of 7,442 original clips from 91 actors. These clips were \nfrom 48 male and 43 female actors between the ages of 20 and 74 coming from a \nvariety of races and ethnicities (African America, Asian, Caucasian, Hispanic, \nand Unspecified). Actors spoke from a selection of 12 sentences. The sentences \nwere presented using one of six different emotions (Anger, Disgust, Fear, Happy,\nNeutral, and Sad) and four different emotion levels (Low, Medium, High, and \nUnspecified).  \n\n**Link:** [Crowdsourced high-quality UK and Ireland English Dialect speech data set](https://www.openslr.org/83/)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nThe dataset contains male and female high quality recordings of English from\nvarious dialects of the UK and Ireland for a total of 17,877 lines.  \n\n**Link:** [CSR-I WSJ0](https://catalog.ldc.upenn.edu/LDC93S6A)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nCorpus by the DARPA Spoken Language Program to support research on large-\nvocabulary Continuous Speech Recognition (CSR) systems. The first two CSR \nCorpora consist primarily of read speech with texts drawn from a machine-\nreadable corpus of Wall Street Journal news text and are thus often known as \nWSJ0 and WSJ1. WSJ0 consists of 123 speakers.  \n\n**Link:** [CSTR VCTK](https://datashare.ed.ac.uk/handle/10283/3443)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nThis CSTR VCTK Corpus includes speech data uttered by 110 English speakers with\nvarious accents. Each speaker reads out about 400 sentences, which were selected\nfrom a newspaper, the rainbow passage and an elicitation paragraph used for the\nspeech accent archive.  \n\n**Link:** [DAPS](https://ccrma.stanford.edu/~gautham/Site/daps.html)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nThe DAPS dataset has 15 versions of audio (3 professional versions and\n12 consumer device/real-world environment combinations). Each version consists\nof about 4.5 hours of data (about 14 minutes from each of 20 speakers).  \n\n**Link:** [DEMAND](https://zenodo.org/records/1227121)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nThe DEMAND (Diverse Environments Multichannel Acoustic Noise Database) contains a set of recordings that allow testing of algorithms using real-world noise in a variety of settings. This version provides 15 recordings. All recordings are made with a 16-channel array, with the smallest distance between microphones being 5 cm and the largest being 21.8 cm. It is a collection of multichannel recordings of accoustic noise in diverse environments.    \n\n**Link:** [Edinburgh 56 speaker dataset](https://datashare.ed.ac.uk/handle/10283/2791)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nClean and noisy parallel speech database from 56 speakers designed to train and \ntest speech enhancement methods that operate at 48kHz.  \n\n**Link:** [FreeField](https://archive.org/details/freefield1010)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nA dataset of standardised 7690 10-second excerpts from Freesound field recordings.  \n\n**Link:** [Freesound](https://freesound.org/)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nFreesound is a collaborative collection of 620,291 free sounds which contains speakers speaking in different emotions as well as female speakers speaking in high pitched voices. The audio data also contains few noise profiles too.   \n\n**Link:** GTC Dataset  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nA collection of talks from Nvidia GTC Conferences with a total of 103 speakers.  \n\n**Link:** [HiFi-TTS](https://www.openslr.org/109/)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nA multi-speaker English dataset for training text-to-speech models. The HiFi-TTS dataset contains about 291.6 hours of speech from 10 speakers with at least 17 hours per speaker sampled at 44.1 kHz.  \n\n**Link:** [LibriTTS](https://www.openslr.org/60/)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nLibriTTS is a multi-speaker English corpus of approximately 585 hours of read\nEnglish speech by 200 speakers, which is resampled at 16kHZ.  \n\n**Link:** [Vocal set dataset](https://zenodo.org/record/1193957#.Xt9uVUUzaUl)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nVocalSet is a singing voice dataset consisting of 10.1 hours of monophonic \nrecorded audio of professional singers demonstrating both standard and extended \nvocal techniques on all 5 vowels. Existing singing voice datasets aim to capture\na focused subset of singing voice characteristics, and generally consist of just\na few singers. VocalSet contains recordings from 20 different singers (9 male, \n11 female) and a range of voice types.  VocalSet aims to improve the state of \nexisting singing voice datasets and singing voice research by capturing not only\na range of vowels, but also a diverse set of voices on many different vocal \ntechniques, sung in contexts of scales, arpeggios, long tones, and excerpts.  \n\n\n# Inference\n\n**Engine:** [Triton](https://developer.nvidia.com/triton-inference-server)  \n**Test Hardware:** A10, A100, A16, A2, A30, A40, H100, L4, L40, RTX 4080, RTX 4090, RTX 5070, RTX 5080, RTX 5090, T4, V100\n\n## Ethical Considerations\n\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established\npolicies and practices to enable development for a wide array of AI applications.\nWhen downloaded or used in accordance with our terms of service, developers\nshould work with their supporting model team to ensure this model meets\nrequirements for the relevant industry and use case and addresses unforeseen\nproduct misuse. For more detailed information on ethical considerations for this\nmodel, please see the Model Card++ [Explainability](explainability), [Bias](bias),\n[Safety \u0026 Security](safety-and-security), and [Privacy](privacy) Subcards. Please\nreport security vulnerabilities or NVIDIA AI Concerns\n[here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/)."])</script><script>self.__next_f.push([1,"3c:T55a,| Field | Response |\n|-------|----------|\n| Intended Application \u0026 Domain: | Speech Enhancement |\n| Model Type:                    | Background Noise Removal |\n| Intended Users:                | Automatic Speech Recognition and Video Conference System Developers |\n| Output:                        | Audio |\n| Describe how the model works:  | This model segments out the background noises from the input noisy audio, retaining only speech for better intelligibility and improved ASR accuracy. |\n| Name the adversely impacted groups this has been tested to deliver comparable outcomes regardless of: | Age (18+), Gender |\n| Verified to have met prescribed quality standards: | Yes |\n| Target Key Performance Indicator(s) (KPI(s)):      | Scale Invariant Signal-to-Noise Ratio Improvement (SI-SNRi), Virtual Speech Quality Objective Listener (VISQOL), Perceptual Evaluation of Speech Quality (PESQ), Latency, Throughput |\n| Technical Limitations: | The model may not handle noises below 5dB or lower SNR. While the model can also work for other types of noises and at lower SNRs, the output speech quality could vary. |\n| Potential Known Risks: | The model may not detect and filter out all audio background noises. |\n| Licensing: | [NVIDIA Maxine Evaluation License Agreement](https://developer.download.nvidia.com/maxine/nvidia-maxine-evaluation-license-24oct2023.pdf) |3d:T489,| Field | Response |\n|-------|----------|\n| Generatable or reverse engineerable personally-identifiable information (PII)? | None |\n| Was consent obtained for any PII used?          | Yes            |\n| Protected class data used to create this model? | None           |\n| How often is dataset reviewed?                  | Before Release |\n| Is a mechanism in place to honor data subject right of access or deletion of personal data? | Yes |\n| If PII collected for the development of the model, was it collected directly by NVIDIA?     | Yes, for some training data |\n| If PII collected for the development of the model by NVIDIA, do you maintain or have access t"])</script><script>self.__next_f.push([1,"o disclosures made to data subjects? | Yes |\n| If PII collected for the development of this AI model, was it minimized to only what was required? | Yes |\n| Is there provenance for all datasets used in training?              | Yes |\n| Does data labeling (annotation, metadata) comply with privacy laws? | Yes |\n| Is data compliant with data subject requests for data correction or removal, if such a request was made? | No, not possible with externally-sourced data. Yes, for Nvidia collected data. |3f:[\"Digital Human\",\"Nvidia Maxine\",\"Run-on-RTX\",\"Speech Enhancement\",\"Speech-to-speech\"]\n41:{\"key\":\"AVAILABLE\",\"value\":\"true\"}\n42:{\"key\":\"PREVIEW\",\"value\":\"false\"}\n40:[\"$41\",\"$42\"]\n43:T1a78,"])</script><script>self.__next_f.push([1,"# Model Overview\n\n## Description\n\nMaxine Studio Voice enhances the input speech recorded through low quality \nmicrophones in noisy/reverberant environment to studio-recorded quality speech.\n\nStudio Voice is available under NVIDIA Maxine — a developer platform for \ndeploying AI features that enhance audio, video, and creating new experiences \nin real-time audio-video communication. Maxine's state-of-the-art models create \nhigh-quality AI effects using standard microphones and cameras without \nadditional special equipments.\n\nNVIDIA Maxine is exclusively part of NVIDIA AI Enterprise for production \nworkflows — an extensive library of full-stack software, including AI solution \nworkflows, frameworks, pre-trained models, and infrastructure optimization.  \n\n\n## Terms of use\n\nThe use of NVIDIA Maxine's Studio Voice is available as a demonstration of the \ninput and output of the Studio Voice generative model. As such the user may \nupload an audio file or select one of the sample inputs and download the \ngenerated audio for evaluation under the terms of the \n[NVIDIA MAXINE EVALUATION LICENSE AGREEMENT](https://developer.download.nvidia.com/maxine/nvidia-maxine-evaluation-license-24oct2023.pdf).  \n\n\n## References(s):\n\n* [NVIDIA Maxine](https://developer.nvidia.com/maxine)  \n\n\n## Model Architecture\n\n**Architecture Type:** Convolution Neural Networks (CNNs), Transformers,\n                       Generative Adversarial Networks (GANs)  \n**Network Architecture:** Encoder-Decoder  \n**Model Version:** 0.2  \n\n\n## Input:\n\n**Input Type(s):** Ordered List (audio samples)  \n**Input Format(s):** FP32 (-1.0 to 1.0)  \n**Other Properties Related to Input:** Pulse Code Modulation (PCM) audio samples\nwith no encoding or pre-processing; 16kHz or 48kHz sampling rate required.  \n\n## Output:\n\n**Output Type(s):** Ordered List (audio samples)  \n**Output Format:** FP32 (-1.0 to 1.0)  \n**Other Properties Related to Output:** PCM audio samples at input sampling rate\nwith no encoding or post-processing.  \n\n## Software Integration\n\n**Supported Hardware Platform(s):** Hopper, Ada, Ampere, Turing, Volta  \n**Test Hardware:** A10, L40, T10  \n**Supported Operating System(s):** Linux, Windows  \n\n\n# Training \u0026 Evaluation\n\n## Datasets\n\nNVIDIA models are trained on a diverse set of public and proprietary datasets.\nThe Studio Voice model is trained on a dataset that comprises of diverse \nEnglish accents.\n\n**Link:** [DAPS](https://ccrma.stanford.edu/~gautham/Site/daps.html)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nThe DAPS dataset has 15 versions of audio (3 professional versions and\n12 consumer device/real-world environment combinations). Each version consists\nof about 4.5 hours of data (about 14 minutes from each of 20 speakers).  \n\n**Link:** [LibriTTS](https://www.openslr.org/60/)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nLibriTTS is a multi-speaker English corpus of approximately 585 hours of read\nEnglish speech, which is resampled at 16kHZ.  \n\n**Link:** [VCTK](https://datashare.ed.ac.uk/handle/10283/3443)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nThis CSTR VCTK Corpus includes speech data uttered by 110 English speakers with\nvarious accents. Each speaker reads out about 400 sentences, which were selected\nfrom a newspaper, the rainbow passage and an elicitation paragraph used for the\nspeech accent archive.  \n\n**Link:** [HiFi-TTS](https://www.openslr.org/109/)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nA multi-speaker English dataset for training text-to-speech models.\nThe HiFi-TTS dataset contains about 291.6 hours of speech from 10 speakers with\nat least 17 hours per speaker sampled at 44.1 kHz.  \n\n**Link:** [Device Recorded VCTK (DR-VCTK)](https://github.com/nii-yamagishilab/downloader-DR-VCTK-complete)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nDevice recorded version of VCTK dataset on common consumer devices \n(laptop, tablet and smartphone) in office environment. This dataset contains \n109 English speakers with different accents. There are around 400 sentences \navailable from each speaker. For this recording, 8 different microphones were \nused. This dataset contains around 250 Gb of re-recorded speech.  \n\n**Link:** [Dataset of impulse responses from variable acoustics room Arni at Aalto Acoustic Labs](https://zenodo.org/records/6985104#.Y7_vv3ZBy3C)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nA dataset of impulse responses collected in the variable acoustics laboratory \nArni at Acoustics Lab of Aalto University, Espoo, Finland. IRs of 5342 \nconfigurations of sound absorption in Arni are included in the dataset. Each of \nthem were measured using an omnidirectional sound source and 5 sound receivers. \nFor each configuration, 5 impulse reponses (IRs) were captured. The total number\nof measurements in the dataset is 132 037.  \n\n**Link:** [Room Impulse Response and Noise Database](https://www.openslr.org/28/)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nA database of simulated and real room impulse responses, isotropic and\npoint-source noises. The audio files in this data are all in 16KHz sampling rate\nand 16-bit precision.  \n\n**Link:** [DNS Challenge 5](https://github.com/microsoft/DNS-Challenge/tree/2db96d5f75257df764a6ef66513b4b97bc707f30)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nCollated dataset of clean speech, noise and impulse response provided by \nMicrosoft for the ICASSP 2023 Deep Noise Suppression Challenge.  \n\n**Link:** [AudioSet](https://research.google.com/audioset/download.html)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nAudioSet consists of an expanding ontology of 632 audio event classes and \na collection of 2,084,320 human-labeled 10-second sound clips drawn from \nYouTube videos.  \n\n\n# Inference\n\n**Engine:** [Triton](https://developer.nvidia.com/triton-inference-server)  \n**Test Hardware:** A10, L40, T10  \n\n## Ethical Considerations\n\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established\npolicies and practices to enable development for a wide array of AI applications.\nWhen downloaded or used in accordance with our terms of service, developers\nshould work with their supporting model team to ensure this model meets\nrequirements for the relevant industry and use case and addresses unforeseen\nproduct misuse. For more detailed information on ethical considerations for this\nmodel, please see the Model Card++ [Explainability](explainability), [Bias](bias),\n[Safety \u0026 Security](safety-and-security), and [Privacy](privacy) Subcards. Please\nreport security vulnerabilities or NVIDIA AI Concerns\n[here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/)."])</script><script>self.__next_f.push([1,"44:T59f,| Field | Response |\n|-------|----------|\n| Intended Application \u0026 Domain: | Voice enhancement |\n| Model Type:                    | Speech Enhancement |\n| Intended Users:                | Content developers and Broadcasters |\n| Output:                        | Audio |\n| Describe how the model works:  | This model enhances the voice in input audio by removing acoustic noise, room reverberation, and bad frequency characteristics of the recording device while maintaining the linguistic and vocal properties. |\n| Name the adversely impacted groups this has been tested to deliver comparable outcomes regardless of: | Age (18+), Gender |\n| Verified to have met prescribed quality standards: | Yes |\n| Target Key Performance Indicator(s) (KPI(s)):      | Quality Mean Opinion Score (MOS), Non-Intrusive Speech Quality Assessment (NISQA), Latency, Throughput |\n| Technical Limitations: | The model may not work work well on a variety of demographic and regional representations of English or with very noisy or very low quality inputs. Exclaimatory emotions may not be transferred to the output. |\n| Potential Known Risks: | The model may not preserve some speaker characteristics like pitch/fundamental frequency of the speakers voice and might also impact intelligibility of speech. |\n| Licensing: | [NVIDIA Maxine Evaluation License Agreement](https://developer.download.nvidia.com/maxine/nvidia-maxine-evaluation-license-24oct2023.pdf) |45:T40d,| Field | Response |\n|-------|----------|\n| Generatable or reverse engineerable personally-identifiable information (PII)? | None |\n| Was consent obtained for any PII used?          | Yes |\n| Protected class data used to create this model? | None |\n| How often is dataset reviewed?                  | Before Release |\n| Is a mechanism in place to honor data subject right of access or deletion of personal data? | No |\n| If PII collected for the development of the model, was it collected directly by NVIDIA?     | No |\n| If PII collected for the development of the model by NVIDIA, do you mai"])</script><script>self.__next_f.push([1,"ntain or have access to disclosures made to data subjects? | No |\n| If PII collected for the development of this AI model, was it minimized to only what was required? | Yes |\n| Is there provenance for all datasets used in training?              | Yes |\n| Does data labeling (annotation, metadata) comply with privacy laws? | Yes |\n| Is data compliant with data subject requests for data correction or removal, if such a request was made? | No |3e:{\"artifactType\":\"ENDPOINT\",\"name\":\"studiovoice\",\"displayName\":\"studiovoice\",\"publisher\":\"nvidia\",\"shortDescription\":\"Enhance speech by correcting common audio degradations to create studio quality speech output.\",\"logo\":\"https://assets.ngc.nvidia.com/products/api-catalog/images/studiovoice.jpg\",\"labels\":\"$3f\",\"attributes\":\"$40\",\"updatedDate\":\"2025-06-13T14:12:57.045Z\",\"bias\":\"| Field | Response |\\n|-------|----------|\\n| Participation considerations from adversely impacted groups [protected classes](https://www.senate.ca.gov/content/protected-classes) in model design and testing: | Age (18+), Gender |\\n| Measures taken to mitigate against unwanted bias: | Evaluated using internal, proprietary data mix to achieve similar key performance indicators. |\",\"canGuestDownload\":true,\"createdDate\":\"2024-10-03T00:40:13.315Z\",\"description\":\"$43\",\"explainability\":\"$44\",\"isPublic\":true,\"isReadOnly\":true,\"orgName\":\"qc69jvmznzxy\",\"privacy\":\"$45\",\"safetyAndSecurity\":\"| Field | Response |\\n|-------|----------|\\n| Model Application(s):                             | Speech Enhancement |\\n| Describe the life critical impact (if present).   | Not Applicable for licensed use cases per [Maxine Evaluation EULA](https://developer.download.nvidia.com/maxine/nvidia-maxine-evaluation-license-24oct2023.pdf) |\\n| Use Case Restrictions:                            | Abide by [Maxine Evaluation EULA](https://developer.download.nvidia.com/maxine/nvidia-maxine-evaluation-license-24oct2023.pdf)  |\\n| Model and dataset restrictions:                   | The Principle of Least Privilege (PoLP) is applied limiting "])</script><script>self.__next_f.push([1,"access for dataset generation and model development. Restrictions enforce dataset access during training, and dataset license constraints adhered to. |\"}\n47:[\"Digital Human\",\"Nvidia Maxine\",\"Speech Enhancement\",\"Speech-to-speech\"]\n49:{\"key\":\"AVAILABLE\",\"value\":\"true\"}\n4a:{\"key\":\"PREVIEW\",\"value\":\"false\"}\n48:[\"$49\",\"$4a\"]\n4b:T2409,"])</script><script>self.__next_f.push([1,"# Model Overview\n\n## Description\n\nMaxine Background Noise Removal (BNR) model is an audio background noise removal\nmodel from NVIDIA. It removes a variety of background noises from audio recordings.\nIt also retains emotive tones in speech, such as happy, sad, excited and angry tones\n\nBNR is available under NVIDIA Maxine — a developer platform for \ndeploying AI features that enhance audio, video, and creating new experiences \nin real-time audio-video communication. Maxine's state-of-the-art models create \nhigh-quality AI effects using standard microphones and cameras without \nadditional special equipments.\n\nNVIDIA Maxine is exclusively part of NVIDIA AI Enterprise for production \nworkflows — an extensive library of full-stack software, including AI solution \nworkflows, frameworks, pre-trained models, and infrastructure optimization.  \n\n\n## Terms of use\n\nThe use of NVIDIA Maxine's BNR is available as a demonstration of the \ninput and output of the BNR generative model. As such the user may \nupload an audio file or select one of the sample inputs and download the \ngenerated audio for evaluation under the terms of the \n[NVIDIA MAXINE EVALUATION LICENSE AGREEMENT](https://developer.download.nvidia.com/maxine/nvidia-maxine-evaluation-license-24oct2023.pdf).  \n\n\u003cb\u003eYou are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws\u003c/b\u003e\n\n## Reference(s):\n\n* [NVIDIA Maxine](https://developer.nvidia.com/maxine)\n\n\n## Model Architecture:\n**Architecture Type:** Residual Convolutional Recurrent Neural Network (CRNN)\n\n**Network Architecture:** SEASR\n\n**Model Version:** 1.0\n\n\n## Input:\n\n**Input Type(s):** Ordered List (audio samples)\n\n**Input Format(s):** FP32 (-1.0 to 1.0)\n\n**Other Properties Related to Input:** Pulse Code Modulation (PCM) audio samples\n\nwith no encoding or pre-processing; 16kHz or 48kHz sampling rate required.  \n\n## Output:\n\n**Output Type(s):** Ordered List (audio samples)\n\n**Output Format:** FP32 (-1.0 to 1.0)\n\n**Other Properties Related to Output:** PCM audio samples at input sampling rate\n\nwith no encoding or post-processing.  \n\n## Software Integration\n\n**Supported Hardware Microarchitecture Compatibility:**\n* Volta\n* Turing\n* Ada\n* Ampere\n* Hopper\n* Blackwell\n\n**Supported Operating System(s):**\n* Linux\n\n\n# Training \u0026 Evaluation\n\n## Datasets\n\nNVIDIA models are trained on a diverse set of public and proprietary datasets.\nThe BNR model is trained on a wide range of English language accents, \nsome European and Asian languanges, and 29 different noise profiles that are \ncommonly audible in real world. \n\n**Data Collection Method by dataset:** [Hybrid: Human, Synthetic]  \n\n**Labeling Method by dataset:** [Hybrid: Human, Synthetic]  \n\n**Link:** [AudioSet](https://research.google.com/audioset/)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nAudioSet consists of an expanding ontology of 632 audio event classes and \na collection of 2,084,320 human-labeled 10-second sound clips drawn from \nYouTube videos.  \n\n**Link:** [CREMA-D](https://www.kaggle.com/ejlok1/cremad)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nCREMA-D is a data set of 7,442 original clips from 91 actors. These clips were \nfrom 48 male and 43 female actors between the ages of 20 and 74 coming from a \nvariety of races and ethnicities (African America, Asian, Caucasian, Hispanic, \nand Unspecified). Actors spoke from a selection of 12 sentences. The sentences \nwere presented using one of six different emotions (Anger, Disgust, Fear, Happy,\nNeutral, and Sad) and four different emotion levels (Low, Medium, High, and \nUnspecified).  \n\n**Link:** [Crowdsourced high-quality UK and Ireland English Dialect speech data set](https://www.openslr.org/83/)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nThe dataset contains male and female high quality recordings of English from\nvarious dialects of the UK and Ireland for a total of 17,877 lines.  \n\n**Link:** [CSR-I WSJ0](https://catalog.ldc.upenn.edu/LDC93S6A)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nCorpus by the DARPA Spoken Language Program to support research on large-\nvocabulary Continuous Speech Recognition (CSR) systems. The first two CSR \nCorpora consist primarily of read speech with texts drawn from a machine-\nreadable corpus of Wall Street Journal news text and are thus often known as \nWSJ0 and WSJ1. WSJ0 consists of 123 speakers.  \n\n**Link:** [CSTR VCTK](https://datashare.ed.ac.uk/handle/10283/3443)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nThis CSTR VCTK Corpus includes speech data uttered by 110 English speakers with\nvarious accents. Each speaker reads out about 400 sentences, which were selected\nfrom a newspaper, the rainbow passage and an elicitation paragraph used for the\nspeech accent archive.  \n\n**Link:** [DAPS](https://ccrma.stanford.edu/~gautham/Site/daps.html)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nThe DAPS dataset has 15 versions of audio (3 professional versions and\n12 consumer device/real-world environment combinations). Each version consists\nof about 4.5 hours of data (about 14 minutes from each of 20 speakers).  \n\n**Link:** [DEMAND](https://zenodo.org/records/1227121)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nThe DEMAND (Diverse Environments Multichannel Acoustic Noise Database) contains a set of recordings that allow testing of algorithms using real-world noise in a variety of settings. This version provides 15 recordings. All recordings are made with a 16-channel array, with the smallest distance between microphones being 5 cm and the largest being 21.8 cm. It is a collection of multichannel recordings of accoustic noise in diverse environments.    \n\n**Link:** [Edinburgh 56 speaker dataset](https://datashare.ed.ac.uk/handle/10283/2791)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nClean and noisy parallel speech database from 56 speakers designed to train and \ntest speech enhancement methods that operate at 48kHz.  \n\n**Link:** [FreeField](https://archive.org/details/freefield1010)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nA dataset of standardised 7690 10-second excerpts from Freesound field recordings.  \n\n**Link:** [Freesound](https://freesound.org/)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nFreesound is a collaborative collection of 620,291 free sounds which contains speakers speaking in different emotions as well as female speakers speaking in high pitched voices. The audio data also contains few noise profiles too.   \n\n**Link:** GTC Dataset  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nA collection of talks from Nvidia GTC Conferences with a total of 103 speakers.  \n\n**Link:** [HiFi-TTS](https://www.openslr.org/109/)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nA multi-speaker English dataset for training text-to-speech models. The HiFi-TTS dataset contains about 291.6 hours of speech from 10 speakers with at least 17 hours per speaker sampled at 44.1 kHz.  \n\n**Link:** [LibriTTS](https://www.openslr.org/60/)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nLibriTTS is a multi-speaker English corpus of approximately 585 hours of read\nEnglish speech by 200 speakers, which is resampled at 16kHZ.  \n\n**Link:** [Vocal set dataset](https://zenodo.org/record/1193957#.Xt9uVUUzaUl)  \n**Properties (Quantity, Dataset Descriptions, Sensor(s)):**  \nVocalSet is a singing voice dataset consisting of 10.1 hours of monophonic \nrecorded audio of professional singers demonstrating both standard and extended \nvocal techniques on all 5 vowels. Existing singing voice datasets aim to capture\na focused subset of singing voice characteristics, and generally consist of just\na few singers. VocalSet contains recordings from 20 different singers (9 male, \n11 female) and a range of voice types.  VocalSet aims to improve the state of \nexisting singing voice datasets and singing voice research by capturing not only\na range of vowels, but also a diverse set of voices on many different vocal \ntechniques, sung in contexts of scales, arpeggios, long tones, and excerpts.  \n\n\n# Inference\n\n**Engine:** [Triton](https://developer.nvidia.com/triton-inference-server)  \n**Test Hardware:** A10, A100, A16, A2, A30, A40, H100, L4, L40, RTX 4080, RTX 4090, RTX 5070, RTX 5080, RTX 5090, T4, V100\n\n## Ethical Considerations\n\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established\npolicies and practices to enable development for a wide array of AI applications.\nWhen downloaded or used in accordance with our terms of service, developers\nshould work with their supporting model team to ensure this model meets\nrequirements for the relevant industry and use case and addresses unforeseen\nproduct misuse. For more detailed information on ethical considerations for this\nmodel, please see the Model Card++ [Explainability](explainability), [Bias](bias),\n[Safety \u0026 Security](safety-and-security), and [Privacy](privacy) Subcards. Please\nreport security vulnerabilities or NVIDIA AI Concerns\n[here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/)."])</script><script>self.__next_f.push([1,"4c:T55a,| Field | Response |\n|-------|----------|\n| Intended Application \u0026 Domain: | Speech Enhancement |\n| Model Type:                    | Background Noise Removal |\n| Intended Users:                | Automatic Speech Recognition and Video Conference System Developers |\n| Output:                        | Audio |\n| Describe how the model works:  | This model segments out the background noises from the input noisy audio, retaining only speech for better intelligibility and improved ASR accuracy. |\n| Name the adversely impacted groups this has been tested to deliver comparable outcomes regardless of: | Age (18+), Gender |\n| Verified to have met prescribed quality standards: | Yes |\n| Target Key Performance Indicator(s) (KPI(s)):      | Scale Invariant Signal-to-Noise Ratio Improvement (SI-SNRi), Virtual Speech Quality Objective Listener (VISQOL), Perceptual Evaluation of Speech Quality (PESQ), Latency, Throughput |\n| Technical Limitations: | The model may not handle noises below 5dB or lower SNR. While the model can also work for other types of noises and at lower SNRs, the output speech quality could vary. |\n| Potential Known Risks: | The model may not detect and filter out all audio background noises. |\n| Licensing: | [NVIDIA Maxine Evaluation License Agreement](https://developer.download.nvidia.com/maxine/nvidia-maxine-evaluation-license-24oct2023.pdf) |4d:T489,| Field | Response |\n|-------|----------|\n| Generatable or reverse engineerable personally-identifiable information (PII)? | None |\n| Was consent obtained for any PII used?          | Yes            |\n| Protected class data used to create this model? | None           |\n| How often is dataset reviewed?                  | Before Release |\n| Is a mechanism in place to honor data subject right of access or deletion of personal data? | Yes |\n| If PII collected for the development of the model, was it collected directly by NVIDIA?     | Yes, for some training data |\n| If PII collected for the development of the model by NVIDIA, do you maintain or have access t"])</script><script>self.__next_f.push([1,"o disclosures made to data subjects? | Yes |\n| If PII collected for the development of this AI model, was it minimized to only what was required? | Yes |\n| Is there provenance for all datasets used in training?              | Yes |\n| Does data labeling (annotation, metadata) comply with privacy laws? | Yes |\n| Is data compliant with data subject requests for data correction or removal, if such a request was made? | No, not possible with externally-sourced data. Yes, for Nvidia collected data. |46:{\"artifactType\":\"ENDPOINT\",\"name\":\"bnr\",\"displayName\":\"Background Noise Removal\",\"publisher\":\"nvidia\",\"shortDescription\":\"Removes unwanted noises from audio improving speech intelligibility.\",\"logo\":\"https://assets.ngc.nvidia.com/products/api-catalog/images/bnr.jpg\",\"labels\":\"$47\",\"attributes\":\"$48\",\"updatedDate\":\"2025-06-13T17:42:41.479Z\",\"bias\":\"| Field | Response |\\n|-------|----------|\\n| Participation considerations from adversely impacted groups [protected classes](https://www.senate.ca.gov/content/protected-classes) in model design and testing: | Age (18+), Gender |\\n| Measures taken to mitigate against unwanted bias: | Evaluated using internal, proprietary data mix to achieve similar key performance indicators. |\",\"canGuestDownload\":true,\"createdDate\":\"2025-06-12T23:23:23.819Z\",\"description\":\"$4b\",\"explainability\":\"$4c\",\"isPublic\":true,\"isReadOnly\":true,\"orgName\":\"qc69jvmznzxy\",\"privacy\":\"$4d\"}\n"])</script><script>self.__next_f.push([1,"32:[\"$\",\"section\",null,{\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center justify-between\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"mb-sm text-ml font-medium leading-body tracking-less text-manitoulinLightWhite\",\"children\":\"Speech Enhancement\"}],\"$undefined\"]}],[\"$\",\"p\",null,{\"className\":\"mb-[30px] text-md font-normal text-manitoulinLightGray\",\"children\":\"Speech enhancing AI models for common voice degradations.\"}],[\"$\",\"$L36\",null,{\"fallback\":[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 gap-8 overflow-hidden lg:grid-cols-2 xl:grid-cols-2\",\"children\":[[\"$\",\"div\",\"0\",{\"children\":[\"$\",\"$L37\",null,{\"artifact\":{\"artifactType\":\"ENDPOINT\",\"name\":\"studiovoice\",\"displayName\":\"studiovoice\",\"publisher\":\"nvidia\",\"shortDescription\":\"Enhance speech by correcting common audio degradations to create studio quality speech output.\",\"logo\":\"https://assets.ngc.nvidia.com/products/api-catalog/images/studiovoice.jpg\",\"labels\":[\"Digital Human\",\"Nvidia Maxine\",\"Run-on-RTX\",\"Speech Enhancement\",\"Speech-to-speech\"],\"attributes\":[{\"key\":\"AVAILABLE\",\"value\":\"true\"},{\"key\":\"PREVIEW\",\"value\":\"false\"}],\"updatedDate\":\"2025-06-13T14:12:57.045Z\",\"bias\":\"| Field | Response |\\n|-------|----------|\\n| Participation considerations from adversely impacted groups [protected classes](https://www.senate.ca.gov/content/protected-classes) in model design and testing: | Age (18+), Gender |\\n| Measures taken to mitigate against unwanted bias: | Evaluated using internal, proprietary data mix to achieve similar key performance indicators. |\",\"canGuestDownload\":true,\"createdDate\":\"2024-10-03T00:40:13.315Z\",\"description\":\"$38\",\"explainability\":\"$39\",\"isPublic\":true,\"isReadOnly\":true,\"orgName\":\"qc69jvmznzxy\",\"privacy\":\"$3a\",\"safetyAndSecurity\":\"| Field | Response |\\n|-------|----------|\\n| Model Application(s):                             | Speech Enhancement |\\n| Describe the life critical impact (if present).   | Not Applicable for licensed use cases per [Maxine Evaluation EULA](https://developer.download.nvidia.com/maxine/nvidia-maxine-evaluation-license-24oct2023.pdf) |\\n| Use Case Restrictions:                            | Abide by [Maxine Evaluation EULA](https://developer.download.nvidia.com/maxine/nvidia-maxine-evaluation-license-24oct2023.pdf)  |\\n| Model and dataset restrictions:                   | The Principle of Least Privilege (PoLP) is applied limiting access for dataset generation and model development. Restrictions enforce dataset access during training, and dataset license constraints adhered to. |\"},\"layout\":\"horizontal\",\"showArtifactType\":true}]}],[\"$\",\"div\",\"1\",{\"children\":[\"$\",\"$L37\",null,{\"artifact\":{\"artifactType\":\"ENDPOINT\",\"name\":\"bnr\",\"displayName\":\"Background Noise Removal\",\"publisher\":\"nvidia\",\"shortDescription\":\"Removes unwanted noises from audio improving speech intelligibility.\",\"logo\":\"https://assets.ngc.nvidia.com/products/api-catalog/images/bnr.jpg\",\"labels\":[\"Digital Human\",\"Nvidia Maxine\",\"Speech Enhancement\",\"Speech-to-speech\"],\"attributes\":[{\"key\":\"AVAILABLE\",\"value\":\"true\"},{\"key\":\"PREVIEW\",\"value\":\"false\"}],\"updatedDate\":\"2025-06-13T17:42:41.479Z\",\"bias\":\"| Field | Response |\\n|-------|----------|\\n| Participation considerations from adversely impacted groups [protected classes](https://www.senate.ca.gov/content/protected-classes) in model design and testing: | Age (18+), Gender |\\n| Measures taken to mitigate against unwanted bias: | Evaluated using internal, proprietary data mix to achieve similar key performance indicators. |\",\"canGuestDownload\":true,\"createdDate\":\"2025-06-12T23:23:23.819Z\",\"description\":\"$3b\",\"explainability\":\"$3c\",\"isPublic\":true,\"isReadOnly\":true,\"orgName\":\"qc69jvmznzxy\",\"privacy\":\"$3d\"},\"layout\":\"horizontal\",\"showArtifactType\":true}]}]]}],\"flag\":\"killswitches/artifact-card-v2.enabled\",\"children\":[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 gap-6 overflow-hidden lg:grid-cols-2 2xl:grid-cols-3 xl:grid-cols-2\",\"children\":[[\"$\",\"div\",\"0\",{\"className\":\"flex\",\"children\":[\"$\",\"$L37\",null,{\"artifact\":\"$3e\",\"layout\":\"horizontal\",\"showArtifactType\":true}]}],[\"$\",\"div\",\"1\",{\"className\":\"flex\",\"children\":[\"$\",\"$L37\",null,{\"artifact\":\"$46\",\"layout\":\"horizontal\",\"showArtifactType\":true}]}]]}]}]]}]\n"])</script><script>$RS=function(a,b){a=document.getElementById(a);b=document.getElementById(b);for(a.parentNode.removeChild(a);a.firstChild;)b.parentNode.insertBefore(a.firstChild,b);b.parentNode.removeChild(b)};$RS("S:1","P:1")</script><script>$RC=function(b,c,e){c=document.getElementById(c);c.parentNode.removeChild(c);var a=document.getElementById(b);if(a){b=a.previousSibling;if(e)b.data="$!",a.setAttribute("data-dgst",e);else{e=b.parentNode;a=b.nextSibling;var f=0;do{if(a&&8===a.nodeType){var d=a.data;if("/$"===d)if(0===f)break;else f--;else"$"!==d&&"$?"!==d&&"$!"!==d||f++}d=a.nextSibling;e.removeChild(a);a=d}while(a);for(;c.firstChild;)e.insertBefore(c.firstChild,a);b.data="$"}b._reactRetry&&b._reactRetry()}};$RC("B:0","S:0")</script><script>self.__next_f.push([1,"4e:I[312293,[\"80650\",\"static/chunks/cef99e24-d6731e3d04a81d8a.js\",\"42393\",\"static/chunks/1c5dbc3d-c63152837a7859da.js\",\"71075\",\"static/chunks/a9670928-c55d94caff555700.js\",\"97049\",\"static/chunks/b794d9c1-6ad16db09e2cd0a0.js\",\"46781\",\"static/chunks/cb9c0786-c452c7cb5b6570dc.js\",\"88838\",\"static/chunks/df3b6c2d-589577bd4490795d.js\",\"33214\",\"static/chunks/33214-d9197868e7892301.js\",\"45617\",\"static/chunks/45617-cfa1b086b480deb6.js\",\"6753\",\"static/chunks/6753-e4b994d0c8f68c36.js\",\"87307\",\"static/chunks/87307-d266ed7473cbf6ae.js\",\"93690\",\"static/chunks/93690-f540e8a0b150e438.js\",\"13574\",\"static/chunks/13574-6e026190fef980e8.js\",\"98907\",\"static/chunks/98907-e60273a43439fd7b.js\",\"4854\",\"static/chunks/4854-39125d9813ba652b.js\",\"71917\",\"static/chunks/71917-5e847e6cfe5fa495.js\",\"25714\",\"static/chunks/25714-97415bf26ad42b0e.js\",\"38366\",\"static/chunks/38366-ccbd630daa1a2ff8.js\",\"1928\",\"static/chunks/1928-65e1a03bda27fa5e.js\",\"37668\",\"static/chunks/37668-74087f4c53f603ed.js\",\"78476\",\"static/chunks/78476-b8b92bea90e6bda8.js\",\"67642\",\"static/chunks/67642-8949f573d31923ee.js\",\"58739\",\"static/chunks/58739-1aab14e336a381ac.js\",\"17839\",\"static/chunks/17839-68556964a45a9cee.js\",\"68462\",\"static/chunks/68462-de6efca665b84003.js\",\"36834\",\"static/chunks/36834-f7dd14020e490870.js\",\"42274\",\"static/chunks/42274-8dcfe58020717e14.js\",\"19371\",\"static/chunks/app/(app-bar-layout)/explore/%5Bcategory%5D/page-f358e48232b974d9.js\"],\"default\"]\n4f:T142a,"])</script><script>self.__next_f.push([1,"Overview\n# Speech Synthesis: Magpie TTS Multilingual Model Overview\n\n## Description:\nThe Magpie TTS Multilingual model converts text into audio (speech).\n\nMagpie TTS is a generative model, designed to be used as the first part of a neural text-to-speech system in conjunction with an audio codec model. This model uses the International Phonetic Alphabet (IPA) for inference and training, and it can output a female or a male voice for English-US and European-Spanish. In addition, it uses character-based encoding for French. \n\nAudio Codec is a neural codec model for speech applications. It is the second part of a two-stage speech synthesis pipeline.\n\nThis model is ready for commercial use.  \n\n### License/Terms of Use: \u003cbr\u003e \n\nGOVERNING TERMS: Use of this model is governed by the [NVIDIA Community Model License Agreement](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/).\n\n\n## References:\nTTS model papers:\n[Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance](https://arxiv.org/pdf/2502.05236)\n[Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment](https://arxiv.org/pdf/2406.17957)\n[Low Frame-rate Speech Codec: A Codec Designed for Fast High-quality Speech LLM Training and Inference](https://arxiv.org/abs/2409.12117)\n\n\n## Model Architecture: \nNetwork Architecture: T5-TTS + Audio codec \nT5-TTS is an encoder-decoder transformer model for text-to-speech synthesis that improves robustness by learning monotonic alignment between text and speech tokens. The model takes text tokens and reference audio codes as input and autoregressively predicts acoustic tokens of the target speech.\nLow Frame Rate Speech Codec - 21Hz is a neural audio compression model that quantizes speech or audio signals into discrete tokens at a low temporal rate of 21 frames per second. The model typically employs a multi-stage encoding process to compress the input audio into a sequence of discrete codes while preserving essential acoustic characteristics despite the aggressive temporal compression. During encoding, it analyzes longer windows of audio to capture relevant acoustic features before downsampling to 21Hz, and during decoding, it uses neural upsampling techniques to reconstruct high-fidelity audio at the original sampling rate. This lower frame rate allows for efficient storage and transmission while still maintaining reasonable audio quality for applications like speech synthesis and audio compression.\n\n## Input: \n**Input Type:** Text \u003cbr\u003e\n**Input Format:** Strings (Graphemes in US English) \u003cbr\u003e\n**Input Parameters:** One-Dimensional (1D) \u003cbr\u003e\n\n\n## Output: \n**Output Type:** Audio  \u003cbr\u003e\n**Output Format:** Audio of shape (batch x time) in wav format  \u003cbr\u003e\n**Output Parameters:** Mono, PCM-encoded 16 bit audio; sampling rate of 22.05 kHz; 20 Second Maximum Length; Depending on input, this model can output a female or a male voice for English US with two (2) emotions for the female voice and six (6) emotions for male voices.  The female voice is classified as “neutral” and “calm.”  The male voice is classified as “neutral,” “calm,” “happy,” and “fearful”, “sad”, and “angry.”  \u003cbr\u003e\n\n\n## Software Integration:\n**Runtime Engine(s):** Riva 2.19.0 or greater \u003cbr\u003e\n\n**Supported Hardware Platform(s):** \u003cbr\u003e\n* NVIDIA A30 \u003cbr\u003e\n* NVIDIA A100 \u003cbr\u003e\n* NVIDIA H100 \u003cbr\u003e\n* NVIDIA A2 \u003cbr\u003e\n* NVIDIA A10 \u003cbr\u003e\n* NVIDIA A16 \u003cbr\u003e\n* NVIDIA A40 \u003cbr\u003e\n* NVIDIA L4 \u003cbr\u003e\n* NVIDIA L40 \u003cbr\u003e\n* GeForce RTX 40xx \u003cbr\u003e\n* GeForce RTX 50xx \u003cbr\u003e\n* Blackwell RTX 60xx \u003cbr\u003e\n\n\n**Supported Operating System(s):** \u003cbr\u003e\n* Linux \u003cbr\u003e\n\n## Inference\n\n**Engine:** Triton \u003cbr\u003e\n**Test Hardware:** \u003cbr\u003e\n* NVIDIA A30 \u003cbr\u003e\n* NVIDIA A100 \u003cbr\u003e\n* NVIDIA H100 \u003cbr\u003e\n* NVIDIA A2 \u003cbr\u003e\n* NVIDIA A10 \u003cbr\u003e\n* NVIDIA A16 \u003cbr\u003e\n* NVIDIA A40 \u003cbr\u003e\n* NVIDIA L4 \u003cbr\u003e\n* NVIDIA L40 \u003cbr\u003e\n* GeForce RTX 40xx \u003cbr\u003e\n* GeForce RTX 50xx \u003cbr\u003e\n* Blackwell RTX 60xx \u003cbr\u003e\n\n## Model Version(s): \nmagpie-tts-multilingual v1\u003cbr\u003e\n\n## Ethical Considerations (For NVIDIA Models Only):\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety \u0026 Security, and Privacy Subcards.  Please report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).\n\n## GOVERNING TERMS: \nThis trial is governed by the [NVIDIA API Trial Terms of Service](https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf). The use of this model is governed by the [AI Foundation Models Community License Agreement](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/)"])</script><script>self.__next_f.push([1,"50:T567,Field  |  Response\n:------|:----------\nIntended Application \u0026 Domain:  |  Speech Synthesis\nModel Task  |  Speech Synthesis \nIntended Users  |  This model is intended for developers building interactive call centers, virtual assistants, and language learning assistants to improve pronunciation, automatically generate voice-overs, narrate or comment on videos, and/or provide audio alternatives for visually impaired users or people with light sensitivity.\nModel Output  |  Speech codes (passed onto Codec model to decode and create audio files (.wav)).\nHow the model works | This model synthesizes input text characters into audio representation.\nTechnical Limitations  |  This model only has the capacity to produce a voice in the language, dialect and gender(s) in which it is trained.  This model makes no effort to moderate or modify input text. \nPerformance Metrics  |  % preference when compared with available alternatives\u003cbr\u003eword error rate(wer)\u003cbr\u003echaracter error rate(cer)\u003cbr\u003emean opinion score (MOS)\nPotential Known Risks  |  This model may unnaturally synthesize vocabulary not included in the pronunciation dictionary or omit phonetic symbols not used in training. This model may insert unexpected words or phonemes. \nLicensing:  |  [https://docs.nvidia.com/ai-foundation-models-community-license.pdf](https://docs.nvidia.com/ai-foundation-models-community-license.pdf)51:T7d5,### Getting Started\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Instructions below demonstrate usage of \u003c%- name %\u003e model using Python gRPC client.\n\n### Prerequisites\n\nYou will need a system with Git and Python 3+ installed.\n\n### Install Riva Python Client\n\n```bash\npip install -U nvidia-riva-client\n```\n\n### Download Python Client\n\nDownload Python client code by cloning \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e.\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\n### Run Python Client\n\nOpen a command terminal and execute below command to synthesize audio from the exampl"])</script><script>self.__next_f.push([1,"e text. If you have generated the API key, it will be auto-populated in the command.\n\n```bash\npython python-clients/scripts/tts/talk.py \\\n    --server grpc.nvcf.nvidia.com:443 --use-ssl \\\n    --metadata function-id \"\u003c%- nvcfFunctionId %\u003e\"  \\\n    --metadata authorization \"Bearer \u003c%- apiKey %\u003e\" \\\n    --language-code en-US \\\n    --text \"this audio is generated from nvidia's text to speech model\" \\\n    --voice \"Magpie-Multilingual.EN-US.Aria\" \\\n    --output audio.wav\n```\n\nList of available voices can be obtained using below command.\n\n```bash\npython python-clients/scripts/tts/talk.py \\\n    --server grpc.nvcf.nvidia.com:443 --use-ssl \\\n    --metadata function-id \"\u003c%- nvcfFunctionId %\u003e\"  \\\n    --metadata authorization \"Bearer \u003c%- apiKey %\u003e\" \\\n    --list-voices\n```\n\n### Support for gRPC clients in other languages\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Proto files can be downloaded from \u003ca href=\"https://github.com/nvidia-riva/common/archive/refs/heads/main.zip\"\u003eRiva gRPC Proto files\u003c/a\u003e and compiled to target language using \u003ca href=\"https://grpc.io/docs/protoc-installation/\"\u003eProtoc compiler\u003c/a\u003e. Example Riva clients in C++ and Python languages are provided below.\n\n* \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e\n* \u003ca href=\"https://github.com/nvidia-riva/cpp-clients\"\u003eC++ Client Repository\u003c/a\u003e\n52:Tae1,"])</script><script>self.__next_f.push([1,"## Step 1. Generate API Key\n\n::generate-api-key\n\n## Step 2. Pull and Run the NIM\n\n```bash\n$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: \u003cPASTE_API_KEY_HERE\u003e\n```\n\nRefer [Supported Models](https://docs.nvidia.com/nim/riva/tts/latest/getting-started.html#supported-models) for full list of models.\n\n```bash\nexport NGC_API_KEY=\u003cPASTE_API_KEY_HERE\u003e\n\ndocker run -it --rm --name=magpie-tts-multilingual \\\n    --runtime=nvidia \\\n    --gpus '\"device=0\"' \\\n    --shm-size=8GB \\\n    -e NGC_API_KEY=$NGC_API_KEY \\\n    -e NIM_HTTP_API_PORT=9000 \\\n    -e NIM_GRPC_API_PORT=50051 \\\n    -p 9000:9000 \\\n    -p 50051:50051 \\\n    nvcr.io/nim/nvidia/magpie-tts-multilingual:latest\n```\n\nIt may take a up to 30 minutes depending on your network speed, for the container to be ready and start accepting requests from the time the docker container is started.\n\nOpen a new terminal and run following command to check if the service is ready to handle inference requests\n\n```bash\ncurl -X 'GET' 'http://localhost:9000/v1/health/ready'\n```\n\nIf the service is ready, you get a response similar to the following.\n```bash\n{\"ready\":true}\n```\n\n## Step 3. Test the NIM\n\nOpen a new terminal and run following commands to synthesize audio from text using sample client scripts.\n\nInstall the Riva Python client package\n\n```bash\nsudo apt-get install python3-pip\npip install -U nvidia-riva-client\n```\n\nDownload Riva sample clients\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\nRun Text to Speech inference in English (en-US)\n\n```bash\npython3 python-clients/scripts/tts/talk.py --server 0.0.0.0:50051 \\\n    --language-code en-US \\\n    --voice Magpie-Multilingual.EN-US.Aria \\\n    --text \"Hello, this is a speech synthesizer.\" \\\n    --output output_english.wav\n```\n\nRun Text to Speech inference in Spanish (es-US)\n\n```bash\npython3 python-clients/scripts/tts/talk.py --server 0.0.0.0:50051 \\\n    --language-code es-US \\\n    --voice Magpie-Multilingual.ES-US.Diego \\\n    --text \"Vamos al parque los domingos para jugar con nuestros amigos.\" \\\n    --output output_spanish.wav\n```\n\nRun Text to Speech inference in French (fr-FR)\n\n```bash\npython3 python-clients/scripts/tts/talk.py --server 0.0.0.0:50051 \\\n    --language-code fr-FR \\\n    --voice Magpie-Multilingual.FR-FR.Louise \\\n    --text \"Je vais au supermarché avec ma mère pour acheter des légumes.\" \\\n    --output output_french.wav\n```\n\nOn running the above commands, the synthesized audio will be saved to files with corresponding names.\n\nYou can list available voices with following command.\n```bash\npython3 python-clients/scripts/tts/talk.py --server 0.0.0.0:50051 \\\n    --list-voices\n```\n\nFor more details on getting started with this NIM, visit the [Riva TTS NIM Docs](https://docs.nvidia.com/nim/riva/tts/latest/overview.html)."])</script><script>self.__next_f.push([1,"53:T1357,"])</script><script>self.__next_f.push([1,"Overview\n# Speech Synthesis: Magpie TTS Zeroshot Model Overview\n\n## Description:\nThe Magpie TTS Zeroshot model converts text into audio (speech).\n\nMagpie TTS is a generative model, designed to be used as the first part of a neural text-to-speech system in conjunction with an audio codec model. This model uses the International Phonetic Alphabet (IPA) for inference and training, and it can output a female or a male voice for English-US and European-Spanish. In addition, it uses character-based encoding for French. \n\nAudio Codec is a neural codec model for speech applications. It is the second part of a two-stage speech synthesis pipeline.\n\nThis model is ready for commercial use.  \n\n### License/Terms of Use: \u003cbr\u003e \n[NVIDIA AI Foundation Models Community License Agreement](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/)\n\n\n## References:\nTTS model papers:\n[Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance](https://arxiv.org/pdf/2502.05236)\n[Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment](https://arxiv.org/pdf/2406.17957)\n[Low Frame-rate Speech Codec: A Codec Designed for Fast High-quality Speech LLM Training and Inference](https://arxiv.org/abs/2409.12117)\n\n\n## Model Architecture: \nNetwork Architecture: T5-TTS + Audio codec \nT5-TTS is an encoder-decoder transformer model for text-to-speech synthesis that improves robustness by learning monotonic alignment between text and speech tokens. The model takes text tokens and reference audio codes as input and autoregressively predicts acoustic tokens of the target speech.\nLow Frame Rate Speech Codec - 21Hz is a neural audio compression model that quantizes speech or audio signals into discrete tokens at a low temporal rate of 21 frames per second. The model typically employs a multi-stage encoding process to compress the input audio into a sequence of discrete codes while preserving essential acoustic characteristics despite the aggressive temporal compression. During encoding, it analyzes longer windows of audio to capture relevant acoustic features before downsampling to 21Hz, and during decoding, it uses neural upsampling techniques to reconstruct high-fidelity audio at the original sampling rate. This lower frame rate allows for efficient storage and transmission while still maintaining reasonable audio quality for applications like speech synthesis and audio compression.\n\n## Input: \n**Input Type:** Text \u003cbr\u003e\n**Input Format:** Strings (Graphemes in US English) \u003cbr\u003e\n**Input Parameters:** One-Dimensional (1D) \u003cbr\u003e\n\n\n## Output: \n**Output Type:** Audio  \u003cbr\u003e\n**Output Format:** Audio of shape (batch x time) in wav format  \u003cbr\u003e\n**Output Parameters:** Mono, PCM-encoded 16 bit audio; sampling rate of 22.05 kHz; 20 Second Maximum Length; Depending on input, this model can output a female or a male voice for English US with two (2) emotions for the female voice and six (6) emotions for male voices.  The female voice is classified as “neutral” and “calm.”  The male voice is classified as “neutral,” “calm,” “happy,” and “fearful”, “sad”, and “angry.”  \u003cbr\u003e\n\n\n## Software Integration:\n**Runtime Engine(s):** Riva 2.19.0 or greater \u003cbr\u003e\n\n**Supported Hardware Platform(s):** \u003cbr\u003e\n* NVIDIA Turing T4 \u003cbr\u003e\n* NVIDIA A100 GPU \u003cbr\u003e\n* NVIDIA A30 GPU \u003cbr\u003e\n* NVIDIA A10 GPU \u003cbr\u003e\n* NVIDIA H100 GPU \u003cbr\u003e\n* NVIDIA L4 GPU \u003cbr\u003e\n* NVIDIA L40 GPU \u003cbr\u003e\n\n\n**Supported Operating System(s):** \u003cbr\u003e\n* Linux \u003cbr\u003e\n\n## Inference\n\n**Engine:** Triton \u003cbr\u003e\n**Test Hardware:** \u003cbr\u003e\n* NVIDIA Turing T4 \u003cbr\u003e\n* NVIDIA A100 GPU \u003cbr\u003e\n* NVIDIA A30 GPU \u003cbr\u003e\n* NVIDIA A10 GPU \u003cbr\u003e\n* NVIDIA H100 GPU \u003cbr\u003e\n* NVIDIA L4 GPU \u003cbr\u003e\n* NVIDIA L40 GPU \u003cbr\u003e\n\n## Model Version(s): \nmagpie-tts-zeroshot v1\u003cbr\u003e\n\n## Ethical Considerations (For NVIDIA Models Only):\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety \u0026 Security, and Privacy Subcards.  Please report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).\n\n## GOVERNING TERMS: \nThis trial is governed by the [NVIDIA API Trial Terms of Service](https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf). The use of this model is governed by the [AI Foundation Models Community License Agreement](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/)"])</script><script>self.__next_f.push([1,"54:T567,Field  |  Response\n:------|:----------\nIntended Application \u0026 Domain:  |  Speech Synthesis\nModel Task  |  Speech Synthesis \nIntended Users  |  This model is intended for developers building interactive call centers, virtual assistants, and language learning assistants to improve pronunciation, automatically generate voice-overs, narrate or comment on videos, and/or provide audio alternatives for visually impaired users or people with light sensitivity.\nModel Output  |  Speech codes (passed onto Codec model to decode and create audio files (.wav)).\nHow the model works | This model synthesizes input text characters into audio representation.\nTechnical Limitations  |  This model only has the capacity to produce a voice in the language, dialect and gender(s) in which it is trained.  This model makes no effort to moderate or modify input text. \nPerformance Metrics  |  % preference when compared with available alternatives\u003cbr\u003eword error rate(wer)\u003cbr\u003echaracter error rate(cer)\u003cbr\u003emean opinion score (MOS)\nPotential Known Risks  |  This model may unnaturally synthesize vocabulary not included in the pronunciation dictionary or omit phonetic symbols not used in training. This model may insert unexpected words or phonemes. \nLicensing:  |  [https://docs.nvidia.com/ai-foundation-models-community-license.pdf](https://docs.nvidia.com/ai-foundation-models-community-license.pdf)55:T11aa,"])</script><script>self.__next_f.push([1,"# Speech Synthesis: Magpie TTS Flow Model Overview\n\n## Description:\nWith only a prompt of 5 seconds or less, the Magpie TTS Flow model can analyze a speaker’s voice and replicate voice qualities such as pitch, timbre and speech rate to achieve a speaker similarity of over 70%, and an MOS score of 4.40. Maintaining the original characteristics that capture unique voice audio signature, it can create high-quality audio (speech) when used in combination with a vocoder model like BigVGAN [1].\n\nMagpie TTS Flow [2] is an alignment-aware pre-training method that builds upon E2TTS’s [3] training framework to learn alignment between unit sequences and speech frames. By using de-duplicated units that retain only phonetic content, Magpie TTS Flow effectively learns alignment without relying on a phoneme duration predictor. This allows for direct application to zero-shot voice conversion, where phonetic content can be transferred to the target speaker’s voice without additional fine-tuning. This model is packaged with BigVGAN, a universal vocoder that generalizes well for various out-of-distribution scenarios without fine-tuning.\n\nThis model is ready for commercial use.\n\n**You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws.**\n\n## License/Terms of Use: \n[NVIDIA AI Foundation Models Community License Agreement](https://docs.nvidia.com/ai-foundation-models-community-license.pdf)\n\n\n## References:\n[1] [BigVGAN: A Universal Neural Vocoder with Large-Scale Training](https://arxiv.org/abs/2206.04658) \u003cbr\u003e\n[2] [Magpie-TTS-Flow Paper](https://openreview.net/forum?id=e2p1BWR3vq) \u003cbr\u003e\n[3] [E2 TTS: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS](https://arxiv.org/abs/2406.18009) \u003cbr\u003e\n[4] [Flow Matching for Generative Modeling](https://arxiv.org/abs/2210.02747) \u003cbr\u003e\n\n\n## Model Architecture: \nArchitecture Type: Flow Matching \u003cbr\u003e\nNetwork Architecture: Optimal Transport Conditional Flow Matching (OT-CFM)-based Masked Speech Modeling\n\nFlow Matching [4] (FM) is a simulation-free approach for training Continuous Normalizing Flows (CNFs) based on regressing vector fields of fixed conditional probability paths. It is compatible with a general family of Gaussian probability paths for transforming between noise and data samples — which subsumes existing diffusion paths as specific instances. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization.\n\n\n## Input: \n*Input Type:* Text + Audio  \u003cbr\u003e\n*Input Format:*  \u003cbr\u003e\nFor Text: Strings (Graphemes in US English) \u003cbr\u003e\nFor Audio: wav file \u003cbr\u003e\n*Input Parameters:* \u003cbr\u003e\nFor text: One-Dimensional (1D) \u003cbr\u003e\nFor audio prompt: Two-Dimensional (batch x time) \u003cbr\u003e\n*Other Properties related to Input:* \u003cbr\u003e\nFor Audio: Recommended format for prompt: Mono, PCM-encoded 16 bit audio; sampling rate of 22.05 kHz; between 3 and 5 second duration. \u003cbr\u003e\n\n\n## Output: \n*Output Type:* Audio \u003cbr\u003e\n*Output Format:* Audio of shape (batch x time) in wav format \u003cbr\u003e\n*Output Parameters:* Two-Dimensional (batch x time)  \u003cbr\u003e\n*Other Properties related to Output:* Mono, PCM-encoded 16 bit audio; sampling rate of 22.05 kHz; 20 Second Maximum Length. \u003cbr\u003e\n\n**Supported Operating System(s):** \u003cbr\u003e\n* Linux \u003cbr\u003e\n\n\n## Model Version(s): \nMagpie-TTS-Flow_v1\u003cbr\u003e\n\n## Inference:\n**Engine:** Triton \u003cbr\u003e\n**Test Hardware:** \u003cbr\u003e\n\n* NVIDIA A100 GPU \u003cbr\u003e\n* NVIDIA A30 GPU \u003cbr\u003e\n* NVIDIA A10 GPU \u003cbr\u003e\n* NVIDIA H100 GPU \u003cbr\u003e\n* NVIDIA L4 GPU \u003cbr\u003e\n* NVIDIA L40 GPU \u003cbr\u003e\n\n\n## Ethical Considerations:\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  \n\n\nFor more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety \u0026 Security, and Privacy Subcards. \n\n\nPlease report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/)."])</script><script>self.__next_f.push([1,"56:T762,Field  |  Response\n:------|:----------\nIntended Application \u0026 Domain:  |  Speech Synthesis\nModel Task  |  Speech Synthesis and Voice Characterization\nIntended Users  |  This model is intended for developers building interactive call centers, virtual assistants, and language learning assistants to improve pronunciation, automatically generate voice-overs, narrate or comment on videos, and provide audio alternatives for visually impaired users or people with light sensitivity.\nModel Output  | Audio of shape (batch x time) in wav format\nDescribe how the model works | Model takes input text and outputs an audio representation of the text. It can be used as a zero-shot voice characterization model. When given a reference audio sample to replicate along with an input text, the produced synthetic audio will be similar to this reference.\nName the adversely impacted groups this has been tested to deliver comparable outcomes regardless of  |  Gender, Age (including people in older age brackets)\nTechnical Limitations  |  Model only has the capacity to produce a voice in the languages, dialects and gender(s) in which it is trained.  This model makes no effort to moderate or modify input text. Languages that are underrepresented may not sound as natural.\nVerified to have met prescribed NVIDIA quality standards  |  Yes \nPerformance Metrics  |  % preference when compared with available alternatives \u003cbr\u003eword error rate (wer) \u003cbr\u003echaracter error rate (cer) \u003cbr\u003emean opinion score (MOS)\nPotential Known Risks  |  This model has the ability to replicate the characteristics of an individual's voice but may unnaturally synthesize vocabulary not included in the pronunciation dictionary or omit phonetic symbols not used in training. \nLicensing:  |  [https://docs.nvidia.com/ai-foundation-models-community-license.pdf](https://docs.nvidia.com/ai-foundation-models-community-license.pdf)57:{\"name\":\"magpie-tts-multilingual\",\"type\":\"model\"}\n58:{\"name\":\"magpie-tts-zeroshot\",\"type\":\"model\"}\n59:{\"name\":\"magpie-tts-flow\",\"type\":\"model\"}\n5a:"])</script><script>self.__next_f.push([1,"{\"name\":\"fastpitch-hifigan-tts\",\"type\":\"model\"}\n"])</script><script>self.__next_f.push([1,"34:[\"$\",\"$L4e\",null,{\"data\":[{\"endpoint\":{\"artifact\":{\"artifactType\":\"ENDPOINT\",\"name\":\"magpie-tts-multilingual\",\"displayName\":\"magpie-tts-multilingual\",\"publisher\":\"nvidia\",\"shortDescription\":\"Natural and expressive voices in multiple languages. For voice agents and brand ambassadors.\",\"logo\":\"https://assets.ngc.nvidia.com/products/api-catalog/images/magpie-tts-multilingual.jpg\",\"labels\":[\"NVIDIA NIM\",\"NVIDIA Riva\",\"TTS\",\"multilingual\",\"Text-to-Speech\"],\"attributes\":[{\"key\":\"AVAILABLE\",\"value\":\"true\"},{\"key\":\"PREVIEW\",\"value\":\"false\"}],\"updatedDate\":\"2025-06-26T19:40:46.733Z\",\"bias\":\"Field  |  Response\\n:------|:----------\\nWhat is the language balance of the model validation data?  |  US English: 58%,Spanish (North America) 15%, 27% French (France)\\nWhat is the geographic origin language balance of the model validation data?  |  Unknown\\nWhat is the accent balance of the model validation data?  |  Unknown \\nMeasures taken to mitigate against unwanted bias:  |  This model was trained on a variety of languages and accents, including: US English, French, German, Italian, European Spanish, Dutch, Polish, and Brazilian Portuguese.\\nParticipation considerations from adversely impacted groups [protected classes](https://www.senate.ca.gov/content/protected-classes) in model design and testing:  |  None\",\"canGuestDownload\":true,\"createdDate\":\"2025-03-18T16:38:59.013Z\",\"description\":\"$4f\",\"explainability\":\"$50\",\"isPublic\":true,\"isReadOnly\":true,\"orgName\":\"qc69jvmznzxy\",\"privacy\":\"Field  |  Response\\n:------|:---------\\nGeneratable or reverse engineerable personal data?  |  None\\nPersonal data used to create this model? |  Yes - Voice\\nWas consent obtained for any personal data used? Yes\\nHow often is the dataset reviewed?  |  Before release (during training)\\nIs a mechanism in place to honor data subject right of access or deletion of personal data?  |  Yes\\nIf personal data collected for the development of the model, was it collected directly by NVIDIA?  |  Yes\\nIf personal data was collected for the development of the model by NVIDIA, do you maintain or have access to disclosures made to data subjects?  |  Yes\\nIf personal data collected for the development of this AI model, was it minimized to only what was required?  |  Yes\\nIs there provenance for all datasets used in training?  |  Yes\\nDoes data labeling (annotation, metadata) comply with privacy laws?  |  Yes\\nIs data compliant with data subject requests for data correction or removal, if such a request was made?  |  Yes\",\"safetyAndSecurity\":\"Field  |  Response\\n:------|:---------\\nModel Application(s):  |  Speech synthesis\\nDescribe the life critical impact (if present).  |  Not Applicable\\nModel and dataset restrictions:  |  The Principle of least privilege (PoLP) is applied limiting access for dataset generation and model development.  Restrictions enforce dataset access during training, and dataset license constraints adhered to.\\nUse case restrictions for the model.  |  Abide by [https://developer.nvidia.com/riva/ga/license](https://developer.nvidia.com/riva/ga/license)\"},\"requestStatus\":{\"statusCode\":\"SUCCESS\",\"requestId\":\"70dbb5ee-997e-434f-999e-93084df1451a\"}},\"spec\":{\"namespace\":\"qc69jvmznzxy\",\"updatedDate\":\"2025-12-23T05:50:49.156Z\",\"nvcfFunctionId\":\"877104f7-e885-42b9-8de8-f6e4c6303969\",\"createdDate\":\"2025-03-18T16:38:59.329Z\",\"attributes\":{\"apiDocsUrl\":\"https://docs.nvidia.com/nim/riva/tts/latest/protos.html\",\"termsOfUse\":\"\u003cb\u003eGOVERNING TERMS\u003c/b\u003e: Your use of this API is governed by the \u003ca href=\\\"https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA API Trial Service Terms of Use\u003c/a\u003e; and the use of this model is governed by the \u003ca href=\\\"https://docs.nvidia.com/ai-foundation-models-community-license.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA AI Foundation Models Community License\u003c/a\u003e.\\n\",\"showUnavailableBanner\":false,\"cta\":{\"text\":\"Run Anywhere - Notify Me\",\"url\":\"https://www.nvidia.com/en-us/ai/nim-notifyme/\"},\"usage\":\"$51\",\"deploy\":[{\"label\":\"Linux with Docker\",\"filename\":\"linux.md\",\"contents\":\"$52\"}]},\"artifactName\":\"magpie-tts-multilingual\"},\"config\":{\"name\":\"magpie-tts-multilingual\",\"type\":\"model\"}},{\"endpoint\":{\"artifact\":{\"artifactType\":\"ENDPOINT\",\"name\":\"magpie-tts-zeroshot\",\"displayName\":\"magpie-tts-zeroshot\",\"publisher\":\"nvidia\",\"shortDescription\":\"Expressive and engaging text-to-speech, generated from a short audio sample.\",\"logo\":\"https://assets.ngc.nvidia.com/products/api-catalog/images/magpie-tts-zeroshot.jpg\",\"labels\":[\"NVIDIA NIM\",\"NVIDIA Riva\",\"TTS\",\"Text-to-Speech\"],\"attributes\":[{\"key\":\"AVAILABLE\",\"value\":\"false\"},{\"key\":\"PREVIEW\",\"value\":\"true\"}],\"updatedDate\":\"2025-06-12T15:03:36.401Z\",\"bias\":\"Field  |  Response\\n:------|:----------\\nWhat is the language balance of the model validation data?  |  US English: 58%,Spanish (North America) 15%, 27% French (France)\\nWhat is the geographic origin language balance of the model validation data?  |  Unknown\\nWhat is the accent balance of the model validation data?  |  Unknown \\nMeasures taken to mitigate against unwanted bias:  |  This model was trained on a variety of languages and accents, including: US English, French, German, Italian, European Spanish, Dutch, Polish, and Brazilian Portuguese.\\nParticipation considerations from adversely impacted groups [protected classes](https://www.senate.ca.gov/content/protected-classes) in model design and testing:  |  None\",\"canGuestDownload\":true,\"createdDate\":\"2025-05-22T12:35:43.777Z\",\"description\":\"$53\",\"explainability\":\"$54\",\"isPublic\":true,\"isReadOnly\":true,\"orgName\":\"qc69jvmznzxy\",\"privacy\":\"Field  |  Response\\n:------|:---------\\nGeneratable or reverse engineerable personal data?  |  None\\nPersonal data used to create this model? |  Yes - Voice\\nWas consent obtained for any personal data used? Yes\\nHow often is the dataset reviewed?  |  Before release (during training)\\nIs a mechanism in place to honor data subject right of access or deletion of personal data?  |  Yes\\nIf personal data collected for the development of the model, was it collected directly by NVIDIA?  |  Yes\\nIf personal data was collected for the development of the model by NVIDIA, do you maintain or have access to disclosures made to data subjects?  |  Yes\\nIf personal data collected for the development of this AI model, was it minimized to only what was required?  |  Yes\\nIs there provenance for all datasets used in training?  |  Yes\\nDoes data labeling (annotation, metadata) comply with privacy laws?  |  Yes\\nIs data compliant with data subject requests for data correction or removal, if such a request was made?  |  Yes\",\"safetyAndSecurity\":\"Field  |  Response\\n:------|:---------\\nModel Application(s):  |  Speech synthesis\\nDescribe the life critical impact (if present).  |  Not Applicable\\nModel and dataset restrictions:  |  The Principle of least privilege (PoLP) is applied limiting access for dataset generation and model development.  Restrictions enforce dataset access during training, and dataset license constraints adhered to.\\nUse case restrictions for the model.  |  Abide by [https://developer.nvidia.com/riva/ga/license](https://developer.nvidia.com/riva/ga/license)\"},\"requestStatus\":{\"statusCode\":\"SUCCESS\",\"requestId\":\"9bba20c6-410e-4158-a100-7fb3a22455eb\"}},\"spec\":{\"namespace\":\"qc69jvmznzxy\",\"updatedDate\":\"2025-06-12T15:03:36.824Z\",\"nvcfFunctionId\":\"55cf67bf-600f-4b04-8eac-12ed39537a08\",\"createdDate\":\"2025-05-22T12:35:44.050Z\",\"attributes\":{\"apiDocsUrl\":\"https://docs.nvidia.com/nim/riva/tts/latest/protos.html\",\"termsOfUse\":\"\u003cb\u003eGOVERNING TERMS\u003c/b\u003e: Your use of this API is governed by the \u003ca href=\\\"https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA API Trial Service Terms of Use\u003c/a\u003e; and the use of this model is governed by the \u003ca href=\\\"https://docs.nvidia.com/ai-foundation-models-community-license.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA AI Foundation Models Community License\u003c/a\u003e.\\n\",\"showUnavailableBanner\":false,\"cta\":{\"text\":\"Apply for Access\",\"url\":\"https://developer.nvidia.com/riva-tts-zeroshot-models\"}},\"artifactName\":\"magpie-tts-zeroshot\"},\"config\":{\"name\":\"magpie-tts-zeroshot\",\"type\":\"model\"}},{\"endpoint\":{\"artifact\":{\"artifactType\":\"ENDPOINT\",\"name\":\"magpie-tts-flow\",\"displayName\":\"magpie-tts-flow\",\"publisher\":\"nvidia\",\"shortDescription\":\"Expressive and engaging text-to-speech, generated from a short audio sample.\",\"logo\":\"https://assets.ngc.nvidia.com/products/api-catalog/images/magpie-tts-flow.jpg\",\"labels\":[\"NVIDIA NIM\",\"NVIDIA Riva\",\"TTS\",\"Text-to-Speech\"],\"attributes\":[{\"key\":\"AVAILABLE\",\"value\":\"false\"},{\"key\":\"PREVIEW\",\"value\":\"true\"}],\"updatedDate\":\"2025-07-10T02:02:24.864Z\",\"bias\":\"Field  |  Response\\n:------|:----------\\nWhat is the language balance of the model validation data?  |  US English: 100%\\nWhat is the geographic origin language balance of the model validation data?  |  United States: 100%\\nWhat is the accent balance of the model validation data?  |  US English: 100%\\nMeasures taken to mitigate against unwanted bias:  |  This model was trained on a variety of languages and accents, including: US English, French, German, Italian, European Spanish, Dutch, Polish, and Brazilian Portuguese.\\nParticipation considerations from adversely impacted groups [protected classes](https://www.senate.ca.gov/content/protected-classes) in model design and testing:  |  Evaluators of several genders, age groups, language backgrounds and geographic locations were recruited when assessing the quality of this model.\",\"canGuestDownload\":true,\"createdDate\":\"2025-07-10T02:02:24.864Z\",\"description\":\"$55\",\"explainability\":\"$56\",\"isPublic\":true,\"isReadOnly\":true,\"orgName\":\"qc69jvmznzxy\",\"privacy\":\"Field  |  Response\\n:------|:---------\\nGeneratable or reverse engineerable personal information?  |  None\\nPersonal data used to create this model?  | Yes - Voice\\nWas consent obtained for any personal data used?  | Yes\\nHow often is the dataset reviewed?  |  Before release (during training)\\nIs a mechanism in place to honor data subject right of access or deletion of personal data?  |  Yes\\nIf personal data was collected for the development of the model, was it collected directly by NVIDIA?  |  Yes\\nIf personal data was collected for the development of the model by NVIDIA, do you maintain or have access to disclosures made to data subjects?  |  Yes\\nIf personal data was collected for the development of this AI model, was it minimized to only what was required?  |  Yes\\nIs there provenance for all datasets used in training?  |  Yes\\nDoes data labeling (annotation, metadata) comply with privacy laws?  |  Yes\\nIs data compliant with data subject requests for data correction or removal, if such a request was made?  |  Yes\",\"safetyAndSecurity\":\"Field  |  Response\\n:------|:---------\\nModel Application(s):  |  Speech synthesis and Voice Characterization\\nDescribe the life critical impact (if present).  |  Not Applicable\\nModel and dataset restrictions:  |  The Principle of least privilege (PoLP) is applied limiting access for dataset generation and model development.  Restrictions enforce dataset access during training, and dataset license constraints adhered to.\\nUse case restrictions for the model.  |  Abide by [https://docs.nvidia.com/ai-foundation-models-community-license.pdf](https://docs.nvidia.com/ai-foundation-models-community-license.pdf)\"},\"requestStatus\":{\"statusCode\":\"SUCCESS\",\"requestId\":\"573747dc-3b71-4005-b1bf-c81f7361f22c\"}},\"spec\":{\"namespace\":\"qc69jvmznzxy\",\"nvcfFunctionId\":\"9b129749-e999-455b-a4bc-3c16858bbbc4\",\"createdDate\":\"2025-07-10T02:02:25.069Z\",\"attributes\":{\"apiDocsUrl\":\"https://docs.nvidia.com/nim/riva/tts/latest/protos.html\",\"termsOfUse\":\"\u003cb\u003eGOVERNING TERMS\u003c/b\u003e: Your use of this API is governed by the \u003ca href=\\\"https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA API Trial Service Terms of Use\u003c/a\u003e; and the use of this model is governed by the \u003ca href=\\\"https://docs.nvidia.com/ai-foundation-models-community-license.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA AI Foundation Models Community License\u003c/a\u003e.\\n\",\"showUnavailableBanner\":false,\"cta\":{\"text\":\"Apply for Access\",\"url\":\"https://developer.nvidia.com/riva-tts-zeroshot-models\"}},\"artifactName\":\"magpie-tts-flow\"},\"config\":{\"name\":\"magpie-tts-flow\",\"type\":\"model\"}},{\"endpoint\":\"$undefined\",\"spec\":\"$undefined\",\"config\":{\"name\":\"fastpitch-hifigan-tts\",\"type\":\"model\"}}],\"items\":[\"$57\",\"$58\",\"$59\",\"$5a\"],\"mode\":\"$undefined\",\"params\":{\"category\":\"speech\"},\"slotTitle\":[[\"$\",\"div\",null,{\"className\":\"mb-2 flex items-start gap-2 max-xs:justify-between\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-ml font-medium leading-body tracking-less text-manitoulinLightWhite mb-0\",\"children\":\"Convert Text to Speech (TTS)\"}],\"$undefined\"]}],[\"$\",\"p\",null,{\"className\":\"text-md font-normal text-manitoulinLightGray mb-0\",\"children\":\"Convert written text to spoken audio in multiple languages with NVIDIA Nemotron Speech models.\"}],\" \"]}]\n"])</script><script>self.__next_f.push([1,"5b:T19e2,"])</script><script>self.__next_f.push([1,"# Speech Recognition: Parakeet \n          \n## Description\n\nRIVA Parakeet-CTC-XL-0.6B ASR Taiwanese Mandarin (around 600M parameters) [1,2] is trained on an ASR dataset with around 90 hours of Taiwanese Mandarin (zh-TW) speech. The model transcribes speech in Taiwanese Mandarin (Traditional Chinese), in upper case and lower case alphabets along with spaces. While the model does not transcribe text with punctuation (period, comma, and question mark), the fused Language Model (LM) decoding may attempt to provide punctuation capabilities. \n\nThis model is ready for commercial use.\n\n## License/Terms of Use \n\n\u003c!-- GOVERNING TERMS: The use of this model is governed by the NVIDIA Community Model License (found at https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/). --\u003e\n\n### Deployment Geography:\n\nGlobal\n\n### Use Case:\n\nThis model serves developers, researchers, academics, and industries building applications that require speech-to-text capabilities, including but not limited to: conversational AI, voice assistants, transcription services, subtitle generation, and voice analytics platforms.\n\n### Release Date:\n\nBuild.Nvidia.com 10/09/2025 via [URL] \u003cbr\u003e\n\u003c!-- Github [Insert MM/DD/YYYY] via [URL] \u003cbr\u003e\nHugging Face [Insert MM/DD/YYYY] via [URL] \u003cbr\u003e --\u003e\nNGC 10/09/2025 via [URL] \u003cbr\u003e\n\u003c!-- Other [Insert MM/DD/YYY] via [URL]\u003cbr\u003e --\u003e\n\n## References\n\n[1] [Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition](https://arxiv.org/abs/2305.05084) \u003cbr\u003e\n[2] [Fast-Conformer-CTC Model](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/models.html) \u003cbr\u003e\n[3] [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100) \u003cbr\u003e\n\n## Model Architecture\n\n**Architecture Type:** Parakeet-CTC (also known as FastConformer-CTC) [1], [2], which is an optimized version of the Conformer model [3], features 8x depthwise-separable convolutional downsampling with CTC loss. \u003cbr\u003e \n**Network Architecture:** Parakeet-CTC-XL-0.6B \u003cbr\u003e\n**This model was developed based on [FastConformer](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fast-conformer) architecture.** \u003cbr\u003e\n**This model has 600 million model parameters.**\n\n## Input \n\n**Input Type(s):** Audio \u003cbr\u003e\n**Input Format:** wav \u003cbr\u003e\n**Input Parameters:** One-Dimensional (1D) \u003cbr\u003e\n**Other Properties Related to Input:** Maximum Length in seconds specific to GPU Memory, No Pre-Processing Needed, Mono channel is required. \u003cbr\u003e\n\n## Output \n\n**Output Type(s):** Text \n**Output Format:** String (in Mandarin and English) \u003cbr\u003e\n**Output Parameters:** One-Dimensional (1D) \u003cbr\u003e\n**Other Properties Related to Output:** No Maximum Character Length, Does not handle special characters. \u003cbr\u003e\n\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA’s hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. \n\n## How to Use this Model\n\nThe [Riva Quick Start Guide](https://ngc.nvidia.com/catalog/resources/nvidia:riva:riva_quickstart) is recommended as the starting point for trying out Riva models. For more information on using this model with Riva Speech Services, see the [Riva User Guide](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-overview.html).\n\n## Suggested Reading\n\nRefer to the [Riva documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/index.html) for more information.\n\n## Software Integration\n\n**Runtime Engine(s):** \n* Riva 2.19.0 or higher \u003cbr\u003e\n\n**Supported Hardware Microarchitecture Compatibility:** \u003cbr\u003e\n* NVIDIA Ampere \u003cbr\u003e\n* NVIDIA Hopper \u003cbr\u003e\n* NVIDIA Jetson \u003cbr\u003e\n* NVIDIA Turing \u003cbr\u003e\n* NVIDIA Volta \u003cbr\u003e\n\n**[Preferred/Supported] Operating System(s):** \u003cbr\u003e\n* Linux \u003cbr\u003e\n* Linux 4 Tegra \u003cbr\u003e\n\nThe integration of foundation and fine-tuned models into AI systems requires additional testing using use-case-specific data to ensure safe and effective deployment. Following the V-model methodology, iterative testing and validation at both unit and system levels are essential to mitigate risks, meet technical and functional requirements, and ensure compliance with safety and ethical standards before deployment.\n\n\n## Model Version(s): \n\nParakeet-CTC-XL-0.6b_zh-TW_1.0\n\n# Training and Evaluation Datasets: \n\n## Training Dataset\n\n**Data Modality**\u003cbr\u003e\n\n* Other: Speech\u003cbr\u003e\n\n**Text Training Data Size**\u003cbr\u003e\n\nLess than a Billion Tokens\n\n**Non-Audio, Image, Text Training Data Size**\u003cbr\u003e\n\nThe model was trained approximately 90 hours of Taiwanese Mandarin speech data:\n* Common Voice Corpus 20.0\n* TechOrange-Podcast\n\n**Data Collection Method by dataset**\u003cbr\u003e\n\n* Human \u003cbr\u003e\n\n**Labeling Method by dataset**\u003cbr\u003e\n\n* Human \u003cbr\u003e\n\n**Properties:**  \u003cbr\u003e\n\nThis model is trained on around 90 hours of Taiwanese Mandarin (zh-TW) speech, comprised of a dynamic blend of public and internal proprietary datasets.\n\n## Evaluation Dataset\n\n**Data Modality**\u003cbr\u003e\n\n* Other: Speech\u003cbr\u003e\n\n**Text Evaluation Data Size**\u003cbr\u003e\n\nLess than a Billion Tokens\n\n**Non-Audio, Image, Text Evaluation Data Size**\u003cbr\u003e\n\nThe model was evaluated approximately 17.5 hours of Taiwanese Mandarin speech data:\n* Common Voice Corpus 20.0\n* TechOrange-Podcast\n* NV-zh-tw-subtitle\n\n**Data Collection Method by dataset**\u003cbr\u003e\n\n* Human \u003cbr\u003e\n\n**Labeling Method by dataset**\u003cbr\u003e\n\n* Human \u003cbr\u003e\n\n**Properties:**\n\nA dynamic blend of public and internal proprietary datasets.\n\n## Inference\n\n**Acceleration Engine:** Triton \u003cbr\u003e\n**Test Hardware:** \u003cbr\u003e\n* NVIDIA A10 \n* NVIDIA A100 \n* NVIDIA A30 \n* NVIDIA H100 \n* NVIDIA Jetson Orin\n* NVIDIA L4 \n* NVIDIA L40 \n* NVIDIA Turing T4\n* NVIDIA Volta V100\n\n## Ethical Considerations\n\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\n\nFor more detailed information on ethical considerations for this model, please see the Model Card++ Bias, Explainability, Safety \u0026 Security, and Privacy Subcards [here](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/models/parakeet-ctc-riva-0-6b-unified-zh-cn/bias).  \n\nPlease report model quality, risk, security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/)."])</script><script>self.__next_f.push([1,"5c:T9db,"])</script><script>self.__next_f.push([1,"Field                                                                                                  |  Response\n:------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------\nIntended Task/Domain:                                                                       |  Speech Transcription\nModel Type:                                                                                                 |  Transformer\nIntended Users:                                                                                        |  This model is intended for developers and data scientists building interactive call centers, virtual assistants, and language learning assistants.\nOutput:                                                                                                |  Transcribed text with timestamps and confidence scores.\nDescribe how the model works:                                                                          |  Model transcribes audio input into text for the input language.\nName the adversely impacted groups this has been tested to deliver comparable outcomes regardless of:  |  Age, Gender, National Origin\nTechnical Limitations \u0026 Mitigation:                                                                                 |  Transcripts may not be 100% accurate. Accuracy varies based on the characteristics of input audio (domain, use case, accent, noise, speech type, context of speech, etc.).\nVerified to have met prescribed NVIDIA quality standards:                                                     |  Yes\nPerformance Metrics:                                                                                   |  Word error rate (WER), silence robustness (characters per minute of silent audio), latency (in milliseconds), and throughput (total audio processed per unit of time).\nPotential Known Risks:                                                                                 |  Not recommended for word-for-word transcription as accuracy varies based on the characteristics of input audio (domain, use case, accent, noise, speech type, and context of speech).\nLicensing:                                                                                             |  [https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/)"])</script><script>self.__next_f.push([1,"5d:T5b8,Field                                                                                                                              |  Response\n:----------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------\nGeneratable or reverse engineerable personal data?                                                     |  No\nPersonal data used to create this model?                                                     |  No\nWas consent obtained for any personal data used?                                                                                   |  Not Applicable\nHow often is dataset reviewed?                                                                                                     |  Before Release\nIs there data provenance for all datasets used in training?                                                                        |  Yes\nDoes data labeling (annotation, metadata) comply with privacy laws?                                                                |  Yes  \nIs data compliant with data subject requests for data correction or removal, if such a request was made?                           |  The data is compliant where applicable, but is not applicable for all data.\nApplicable Privacy Policy | [https://www.nvidia.com/en-us/about-nvidia/privacy-policy/](https://www.nvidia.com/en-us/about-nvidia/privacy-policy/)5e:T68b,### Getting Started\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Instructions below demonstrate usage of \u003c%- name %\u003e model using Python gRPC client.\n\n### Prerequisites\n\nYou will need a system with Git and Python 3+ installed.\n\n### Install Riva Python Client\n\n```bash\npip install -U nvidia-riva-client\n```\n\n### Download Python Client\n\nDownload Python client code by cloning \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e.\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\n### Run Python Client\n\nOpen"])</script><script>self.__next_f.push([1," a command terminal and execute below command to transcribe audio. Make sure you have a speech file in 16-bit Mono format in WAV/OGG/OPUS container. If you have generated the API key, it will be auto-populated in the command.\n\n```bash\npython python-clients/scripts/asr/transcribe_file.py \\\n    --server grpc.nvcf.nvidia.com:443 --use-ssl \\\n    --metadata function-id \"\u003c%- nvcfFunctionId %\u003e\" \\\n    --metadata \"authorization\" \"Bearer \u003c%- apiKey %\u003e\" \\\n    --language-code zh-TW \\\n    --input-file \u003cpath_to_audio_file\u003e\n```\n\n### Support for gRPC clients in other languages\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Proto files can be downloaded from \u003ca href=\"https://github.com/nvidia-riva/common/archive/refs/heads/main.zip\"\u003eRiva gRPC Proto files\u003c/a\u003e and compiled to target language using \u003ca href=\"https://grpc.io/docs/protoc-installation/\"\u003eProtoc compiler\u003c/a\u003e. Example Riva clients in C++ and Python languages are provided below.\n\n* \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e\n* \u003ca href=\"https://github.com/nvidia-riva/cpp-clients\"\u003eC++ Client Repository\u003c/a\u003e\n5f:T71a,## Step 1. Generate API Key\n\n::generate-api-key\n\n## Step 2. Pull and Run the NIM\n\n```bash\n$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: \u003cPASTE_API_KEY_HERE\u003e\n```\n\nRefer [Supported Models](https://docs.nvidia.com/nim/riva/asr/latest/getting-started.html#supported-models) for full list of models.\n\n```bash\nexport NGC_API_KEY=\u003cPASTE_API_KEY_HERE\u003e\n\ndocker run -it --rm --name=parakeet-ctc-0.6b-zh-tw \\\n   --runtime=nvidia \\\n   --gpus '\"device=0\"' \\\n   --shm-size=8GB \\\n   -e NGC_API_KEY \\\n   -e NIM_HTTP_API_PORT=9000 \\\n   -e NIM_GRPC_API_PORT=50051 \\\n   -p 9000:9000 \\\n   -p 50051:50051 \\\n   -e NIM_TAGS_SELECTOR=mode=str,vad=silero,diarizer=sortformer \\\n   nvcr.io/nim/nvidia/parakeet-ctc-0.6b-zh-tw:latest\n```\n\nIt may take a up to 30 minutes depending on your network speed, for the container to be ready and start accepting requests from the time the docker container is started.\n\n## Step 3. Test the NIM\n\nOpen a new termina"])</script><script>self.__next_f.push([1,"l and run following command to check if the service is ready to handle inference requests\n\n```bash\ncurl -X 'GET' 'http://localhost:9000/v1/health/ready'\n```\n\nIf the service is ready, you get a response similar to the following.\n```bash\n{\"ready\":true}\n```\n\nInstall the Riva Python client package\n\n```bash\nsudo apt-get install python3-pip\npip install -U nvidia-riva-client\n```\n\nDownload Riva sample clients\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\nRun Speech to Text inference in streaming modes. Riva ASR supports Mono, 16-bit audio in WAV, OPUS and FLAC formats.\n\n```bash\npython3 python-clients/scripts/asr/transcribe_file.py --server 0.0.0.0:50051 --input-file \u003cpath_to_speech_file\u003e --language-code zh-TW\n```\n\n\nFor more details on getting started with this NIM, visit the [Riva ASR NIM Docs](https://docs.nvidia.com/nim/riva/asr/latest/overview.html).60:T18c6,"])</script><script>self.__next_f.push([1,"## \u003cspan style=\"color:#76b900;\"\u003eParakeet-tdt-0.6b-v2 English speech to text model\u003c/span\u003e\n\n## \u003cspan style=\"color:#466f00;\"\u003eDescription:\u003c/span\u003e\n\n`Parakeet-tdt-0.6b-v2` is a 600-million-parameter automatic speech recognition (ASR) model designed for high-quality English transcription, featuring support for punctuation, capitalization, and accurate timestamp prediction.\n\nThis XL variant of the [FastConformer](https://arxiv.org/abs/2305.05084) architecture integrates the [TDT](https://arxiv.org/abs/2304.06795) decoder and is trained with full attention, enabling efficient transcription of audio segments up to 24 minutes in a single pass.\n\n**Key Features**\n- Accurate word-level timestamp predictions  \n- Automatic punctuation and capitalization  \n- Robust performance on spoken numbers, and song lyrics transcription \n\n\nThis model is ready for commercial/non-commercial use.\n\n\n## \u003cspan style=\"color:#466f00;\"\u003eLicense/Terms of Use:\u003c/span\u003e\n\nGOVERNING TERMS: Use of this model is governed by the [NVIDIA Community Model License Agreement](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/).\n\n### \u003cspan style=\"color:#466f00;\"\u003eDeployment Geography:\u003c/span\u003e\nGlobal\n\n\n### \u003cspan style=\"color:#466f00;\"\u003eUse Case:\u003c/span\u003e\n\nThis model serves developers, researchers, academics, and industries building applications that require speech-to-text capabilities, including but not limited to: conversational AI, voice assistants, transcription services, subtitle generation, and voice analytics platforms.\n\n\n### \u003cspan style=\"color:#466f00;\"\u003eModel Architecture:\u003c/span\u003e\n\n**Architecture Type**: \n\nFastConformer-TDT\n\n**Network Architecture**:\n\n* This model was developed based on [FastConformer encoder](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fast-conformer) architecture and TDT decoder.\n* This model has 600 million model parameters.\n\n### \u003cspan style=\"color:#466f00;\"\u003eInput:\u003c/span\u003e\n**Input Type(s):** 16kHz Audio \u003cbr\u003e\n**Input Format(s):** `.wav` and `.flac` audio formats \u003cbr\u003e\n**Input Parameters:** 1D (audio signal) \u003cbr\u003e\n**Other Properties Related to Input:**  Monochannel audio \u003cbr\u003e\n\n### \u003cspan style=\"color:#466f00;\"\u003eOutput:\u003c/span\u003e\n**Output Type(s):**  Text \u003cbr\u003e\n**Output Format:**  String \u003cbr\u003e\n**Output Parameters:**  1D (text) \u003cbr\u003e\n**Other Properties Related to Output:** Punctuations and Capitalizations included. \u003cbr\u003e\n\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. \n\n\n## \u003cspan style=\"color:#466f00;\"\u003eSoftware Integration:\u003c/span\u003e\n\n**Runtime Engine(s):**\n* Riva\n\n\n**Supported Hardware Microarchitecture Compatibility:** \n* NVIDIA Ampere\n* NVIDIA Blackwell  \n* NVIDIA Hopper\n* NVIDIA Volta\n\n**[Preferred/Supported] Operating System(s):**\n\n- Linux\n\n**Hardware Specific Requirements:**\n\nAtleast 2GB RAM for model to load. The bigger the RAM, the larger audio input it supports.\n\n#### Model Version\n\nCurrent version: parakeet-0.6b-tdt-v2\n\n## \u003cspan style=\"color:#466f00;\"\u003eTraining and Evaluation Datasets:\u003c/span\u003e\n\n### \u003cspan style=\"color:#466f00;\"\u003eTraining\u003c/span\u003e\n\nThis model was trained using the NeMo toolkit [3], following the strategies below:\n\n- Initialized from a wav2vec SSL checkpoint pretrained on the LibriLight dataset[7].  \n- Trained for 150,000 steps on 128 A100 GPUs. \n- Dataset corpora were balanced using a temperature sampling value of 0.5.  \n- Stage 2 fine-tuning was performed for 2,500 steps on 4 A100 GPUs using approximately 500 hours of high-quality, human-transcribed data of NeMo ASR Set 3.0.  \n\nTraining was conducted using this [example script](https://github.com/NVIDIA/NeMo/blob/main/examples/asr/asr_transducer/speech_to_text_rnnt_bpe.py) and [TDT configuration](https://github.com/NVIDIA/NeMo/blob/main/examples/asr/conf/fastconformer/hybrid_transducer_ctc/fastconformer_hybrid_tdt_ctc_bpe.yaml).\n\nThe tokenizer was constructed from the training set transcripts using this [script](https://github.com/NVIDIA/NeMo/blob/main/scripts/tokenizers/process_asr_text_tokenizer.py).\n\n**Data Collection Method by dataset**\n\n* Hybrid: Automated, Human\n\n**Labeling Method by dataset**\n\n* Hybrid: Synthetic, Human \n\n**Properties:**\n\n* Noise robust data from various sources\n* Single channel, 16kHz sampled data\n\n\n## \u003cspan style=\"color:#466f00;\"\u003eReferences\u003c/span\u003e\n\n[1] [Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition](https://arxiv.org/abs/2305.05084)\n\n[2] [Efficient Sequence Transduction by Jointly Predicting Tokens and Durations](https://arxiv.org/abs/2304.06795)\n\n[3] [NVIDIA NeMo Toolkit](https://github.com/NVIDIA/NeMo)\n\n[4] [Youtube-commons: A massive open corpus for conversational and multimodal data](https://huggingface.co/blog/Pclanglais/youtube-commons)\n\n[5] [Yodas: Youtube-oriented dataset for audio and speech](https://arxiv.org/abs/2406.00899)\n\n[6] [HuggingFace ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)\n\n[7] [MOSEL: 950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages](https://arxiv.org/abs/2410.01036) \n\n## \u003cspan style=\"color:#466f00;\"\u003eInference:\u003c/span\u003e\n\n**Engine**: \n* NVIDIA NeMo\n\n**Test Hardware**:\n* NVIDIA A10\n* NVIDIA A100\n* NVIDIA A30\n* NVIDIA H100\n* NVIDIA L4\n* NVIDIA L40\n* NVIDIA Turing T4\n* NVIDIA Volta V100\n\n## \u003cspan style=\"color:#466f00;\"\u003eEthical Considerations:\u003c/span\u003e\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\n\nFor more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety \u0026 Security, and Privacy Subcards [here](https://developer.nvidia.com/blog/enhancing-ai-transparency-and-ethical-considerations-with-model-card/).\n\nPlease report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/)."])</script><script>self.__next_f.push([1,"61:T8c8,"])</script><script>self.__next_f.push([1,"Field                                                                                                  |  Response\n:------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------:\nIntended Domain                                                                   |  Speech to Text Transcription\nModel Type                                                                                            |  FastConformer\nIntended Users                                                                                        |  This model is intended for developers, researchers, academics, and industries building conversational based applications. \nOutput                                                                                                |  Text \nDescribe how the model works                                                                          |  Speech input is encoded into embeddings and passed into conformer-based model and output a text response.\nName the adversely impacted groups this has been tested to deliver comparable outcomes regardless of  |  Not Applicable\nTechnical Limitations \u0026 Mitigation                                                                    |  Transcripts may be not 100% accurate. Accuracy varies based on language and characteristics of input audio (Domain, Use Case, Accent, Noise, Speech Type, Context of speech, etc.)\nVerified to have met prescribed NVIDIA quality standards  |  Yes\nPerformance Metrics                                                                                   | Word Error Rate\nPotential Known Risks                                                                                 |  If a word is not trained in the language model and not presented in vocabulary, the word is not likely to be recognized. Not recommended for word-for-word/incomplete sentences as accuracy varies based on the context of input text\nLicensing                                                                                             |  GOVERNING TERMS: Use of this model is governed by the [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode.en) license."])</script><script>self.__next_f.push([1,"62:T464,Field                                                                                                                              |  Response\n:----------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------------------------:\nGeneratable or reverse engineerable personal data?                                                     |  None\nPersonal data used to create this model?                                                                                       |  None\nIs there provenance for all datasets used in training?                                                                                |  Yes\nDoes data labeling (annotation, metadata) comply with privacy laws?                                                                |  Yes\nIs data compliant with data subject requests for data correction or removal, if such a request was made?                           |  No, not possible with externally-sourced data.\nApplicable Privacy Policy        | https://www.nvidia.com/en-us/about-nvidia/privacy-policy/63:T712,### Getting Started\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Instructions below demonstrate usage of \u003c%- name %\u003e model using Python gRPC client.\n\n### Prerequisites\n\nYou will need a system with Git and Python 3+ installed.\n\n### Install Riva Python Client\n\n```bash\npip install -U nvidia-riva-client\n```\n\n### Download Python Client\n\nDownload Python client code by cloning \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e.\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\n### Run Python Client\n\nOpen a command terminal and execute below command to transcribe audio. Make sure you have a speech file in 16-bit Mono format in WAV/OGG/OPUS container. If you have generated the API key, it will be auto-populated in the command.\n\nBelow command demonstrates transcription of English audio file.\n\n```bash\npython python-clients/scripts/asr/transc"])</script><script>self.__next_f.push([1,"ribe_file_offline.py \\\n    --server grpc.nvcf.nvidia.com:443 --use-ssl \\\n    --metadata function-id \"\u003c%- nvcfFunctionId %\u003e\" \\\n    --metadata \"authorization\" \"Bearer \u003c%- apiKey %\u003e\" \\\n    --language-code en-US \\\n    --word-time-offsets --automatic-punctuation \\\n    --input-file \u003cpath_to_audio_file\u003e\n```\n\n### Support for gRPC clients in other programming languages\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Proto files can be downloaded from \u003ca href=\"https://github.com/nvidia-riva/common/archive/refs/heads/main.zip\"\u003eRiva gRPC Proto files\u003c/a\u003e and compiled to target language using \u003ca href=\"https://grpc.io/docs/protoc-installation/\"\u003eProtoc compiler\u003c/a\u003e. Example Riva clients in C++ and Python languages are provided below.\n\n* \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e\n* \u003ca href=\"https://github.com/nvidia-riva/cpp-clients\"\u003eC++ Client Repository\u003c/a\u003e\n64:T70b,## Step 1. Generate API Key\n\n::generate-api-key\n\n## Step 2. Pull and Run the NIM\n\n```bash\n$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: \u003cPASTE_API_KEY_HERE\u003e\n```\n\nRefer to [Supported Models](https://docs.nvidia.com/nim/riva/asr/latest/getting-started.html#supported-models) for full list of models.\n\n```bash\nexport NGC_API_KEY=\u003cPASTE_API_KEY_HERE\u003e\n\ndocker run -it --rm --name=parakeet-tdt \\\n   --runtime=nvidia \\\n   --gpus '\"device=0\"' \\\n   --shm-size=8GB \\\n   -e NGC_API_KEY \\\n   -e NIM_HTTP_API_PORT=9000 \\\n   -e NIM_GRPC_API_PORT=50051 \\\n   -p 9000:9000 \\\n   -p 50051:50051 \\\n   nvcr.io/nim/nvidia/parakeet-tdt-0.6b-v2:latest\n```\n\nIt may take a up to 30 minutes depending on your network speed, for the container to be ready and start accepting requests from the time the docker container is started.\n\n## Step 3. Test the NIM\n\nOpen a new terminal and run following command to check if the service is ready to handle inference requests\n\n```bash\ncurl -X 'GET' 'http://localhost:9000/v1/health/ready'\n```\n\nIf the service is ready, you get a response similar to the following.\n```bash\n{\"ready\":true}\n```\n\nInstall the Riva Python cl"])</script><script>self.__next_f.push([1,"ient package\n\n```bash\nsudo apt-get install python3-pip\npip install -U nvidia-riva-client\n```\n\nDownload Riva sample clients\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\nRun Speech to Text inference in offline modes. Riva ASR supports Mono, 16-bit audio in WAV, OPUS and FLAC formats.\n\n```bash\npython3 python-clients/scripts/asr/transcribe_file_offline.py --server 0.0.0.0:50051 \\\n   --language-code en-US --word-time-offsets --automatic-punctuation \\\n   --input-file \u003cpath_to_speech_file\u003e \n```\n\n\nFor more details on getting started with this NIM, visit the [Riva ASR NIM Docs](https://docs.nvidia.com/nim/riva/asr/latest/overview.html).\n65:Taf7,"])</script><script>self.__next_f.push([1,"# Model Overview\nParakeet is a major step forward in the evolution of conversational AI. Its exceptional accuracy, coupled with the flexibility and ease of use offered by NeMo, empowers developers to create more natural and intuitive voice-powered applications. The possibilities are endless, from enhancing the accuracy of virtual assistants to enabling seamless real-time communication.\n\n## Description\nParakeet transcribes audio into text, using spaces and apostrophes where needed \u003cbr\u003e\n\n## Terms of use\nGOVERNING TERMS: Use of this model is governed by the [NVIDIA Community Model License Agreement](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/).\n\n# Disclaimer\nAI models generate responses and outputs based on complex algorithms and machine learning techniques, and those responses or outputs may be inaccurate or indecent. By testing this model, you assume the risk of any harm caused by any response or output of the model. Please do not upload any confidential information or personal data. Your use is logged for security. \n\n# References\n* [Fast Conformer](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fast-conformer)\n\n## Model Architecture\n**Architecture Type:** Convolutional Neural Network + Transformer \u003cbr\u003e\n**Network Architecture:** Fast Conformer Encoder with CTC Decoder \u003cbr\u003e\n\n## Input\n**Input Type(s):** Audio in English \u003cbr\u003e\n**Input Format(s):** Linear PCM 16-bit 1 channel \u003cbr\u003e\n\n## Output\n**Output Type(s):** Text String in English with Timestamps \u003cbr\u003e\n\n## Software Integration\n**Runtime Engine(s):** \n* Riva 2.15.0 or Higher \u003cbr\u003e\n\n**Supported Operating System(s):**  \u003cbr\u003e\n* Linux \u003cbr\u003e\n\n## Model Version\nParakeet-1.1b-ctc-en-us-asr-set-6.0 \u003cbr\u003e\n\n# Inference\n**Engine:** Triton \u003cbr\u003e\n**Test Hardware:**  \u003cbr\u003e\n* NVIDIA A2 \u003cbr\u003e\n* NVIDIA A10 \u003cbr\u003e\n* NVIDIA A16 \u003cbr\u003e\n* NVIDIA A30 \u003cbr\u003e\n* NVIDIA A40 \u003cbr\u003e\n* NVIDIA A100 \u003cbr\u003e\n* NVIDIA H100 \u003cbr\u003e\n* NVIDIA L4 \u003cbr\u003e\n* NVIDIA L40 \u003cbr\u003e\n* GeForce RTX 40xx \u003cbr\u003e\n* GeForce RTX 50xx \u003cbr\u003e\n* Blackwell RTX 60xx \u003cbr\u003e\n\n\n## Ethical Considerations (For NVIDIA Models Only):\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety \u0026 Security, and Privacy Subcards.  Please report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/)."])</script><script>self.__next_f.push([1,"66:T917,"])</script><script>self.__next_f.push([1,"Field                                                                                                  |  Response\n:------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------\nIntended Applications \u0026 Domains:                                                                       |  Speech Transcription\nTypes:                                                                                                 |  Speech Transcription\nIntended Users:                                                                                        |  Data Scientists in Contact Center Transcription, Video Conferencing Transcription, Virtual Assistants, etc\nOutput:                                                                                                |  Transcribed text with timestamps and confidence scores\nDescribe how the model works:                                                                          |  Model transcribes audio input into text for the input language\nName the adversely impacted groups this has been tested to deliver comparable outcomes regardless of:  |  Age, Gender, National Origin\nTechnical Limitations:                                                                                 |  Transcripts may  not be 100% accurate. Accuracy varies based on the characteristics of input audio (Domain, Use Case, Accent, Noise, Speech Type, Context of speech, etc.)\nVerified to have met prescribed NVIDIA quality standards:                                                     |  Yes\nPerformance Metrics:                                                                                   |  Word Error Rate (WER), Silence Robustness (Characters/mins of silent audio), Latency (in milliseconds), Throughput (Total audio processed per unit of time)\nPotential Known Risks:                                                                                 |  Not recommended for word-for-word transcription as accuracy varies based on the characteristics of input audio (domain, use case, accent, noise, speech type, and context of speech)\nLicensing:                                                                                             |  https://developer.nvidia.com/riva/ga/license"])</script><script>self.__next_f.push([1,"67:T81f,"])</script><script>self.__next_f.push([1,"Field                                                                                                                              |  Response\n:----------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------\nGeneratable or reverse engineerable personally-identifiable information (PII)?                                                     |  None\nWas consent obtained for any PII used?                                                                                             |  Yes\nProtected class data used to create this model?                                                                                    |  Age, Gender, Linguistic Background, National Origin\nHow often is dataset reviewed?                                                                                                     |  Before Release\nIs a mechanism in place to honor data subject right of access or deletion of personal data?                                        |  No\nIf PII collected for the development of the model, was it collected directly by NVIDIA?                                            |  PII not collected by NVIDIA for development of model\nIf PII collected for the development of the model by NVIDIA, do you maintain or have access to disclosures made to data subjects?  |  Not applicable\nIf PII collected for the development of this AI model, was it minimized to only what was required?                                 |  Yes\nIs there provenance for all datasets used in training?                                                                                                                          |  Yes\nDoes data labeling (annotation, metadata) comply with privacy laws?                                                                |  Yes  \nIs data compliant with data subject requests for data correction or removal, if such a request was made?                           |  The data is compliant where applicable, but is not applicable for all data."])</script><script>self.__next_f.push([1,"68:T68b,### Getting Started\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Instructions below demonstrate usage of \u003c%- name %\u003e model using Python gRPC client.\n\n### Prerequisites\n\nYou will need a system with Git and Python 3+ installed.\n\n### Install Riva Python Client\n\n```bash\npip install -U nvidia-riva-client\n```\n\n### Download Python Client\n\nDownload Python client code by cloning \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e.\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\n### Run Python Client\n\nOpen a command terminal and execute below command to transcribe audio. Make sure you have a speech file in 16-bit Mono format in WAV/OGG/OPUS container. If you have generated the API key, it will be auto-populated in the command.\n\n```bash\npython python-clients/scripts/asr/transcribe_file.py \\\n    --server grpc.nvcf.nvidia.com:443 --use-ssl \\\n    --metadata function-id \"\u003c%- nvcfFunctionId %\u003e\" \\\n    --metadata \"authorization\" \"Bearer \u003c%- apiKey %\u003e\" \\\n    --language-code en-US \\\n    --input-file \u003cpath_to_audio_file\u003e\n```\n\n### Support for gRPC clients in other languages\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Proto files can be downloaded from \u003ca href=\"https://github.com/nvidia-riva/common/archive/refs/heads/main.zip\"\u003eRiva gRPC Proto files\u003c/a\u003e and compiled to target language using \u003ca href=\"https://grpc.io/docs/protoc-installation/\"\u003eProtoc compiler\u003c/a\u003e. Example Riva clients in C++ and Python languages are provided below.\n\n* \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e\n* \u003ca href=\"https://github.com/nvidia-riva/cpp-clients\"\u003eC++ Client Repository\u003c/a\u003e\n69:T6fb,## Step 1. Generate API Key\n\n::generate-api-key\n\n## Step 2. Pull and Run the NIM\n\n```bash\n$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: \u003cPASTE_API_KEY_HERE\u003e\n```\n\nRefer [Supported Models](https://docs.nvidia.com/nim/riva/asr/latest/getting-started.html#supported-models) for full list of models.\n\n```bash\nexport NGC_API_KEY=\u003cPASTE_API_KEY_HERE\u003e\n\ndoc"])</script><script>self.__next_f.push([1,"ker run -it --rm --name=parakeet-1-1b-ctc-en-us \\\n   --runtime=nvidia \\\n   --gpus '\"device=0\"' \\\n   --shm-size=8GB \\\n   -e NGC_API_KEY \\\n   -e NIM_HTTP_API_PORT=9000 \\\n   -e NIM_GRPC_API_PORT=50051 \\\n   -p 9000:9000 \\\n   -p 50051:50051 \\\n   -e NIM_TAGS_SELECTOR=mode=str \\\n   nvcr.io/nim/nvidia/parakeet-1-1b-ctc-en-us:latest\n```\n\nIt may take a up to 30 minutes depending on your network speed, for the container to be ready and start accepting requests from the time the docker container is started.\n\n## Step 3. Test the NIM\n\nOpen a new terminal and run following command to check if the service is ready to handle inference requests\n\n```bash\ncurl -X 'GET' 'http://localhost:9000/v1/health/ready'\n```\n\nIf the service is ready, you get a response similar to the following.\n```bash\n{\"ready\":true}\n```\n\nInstall the Riva Python client package\n\n```bash\nsudo apt-get install python3-pip\npip install -U nvidia-riva-client\n```\n\nDownload Riva sample clients\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\nRun Speech to Text inference in streaming modes. Riva ASR supports Mono, 16-bit audio in WAV, OPUS and FLAC formats.\n\n```bash\npython3 python-clients/scripts/asr/transcribe_file.py --server 0.0.0.0:50051 --input-file \u003cpath_to_speech_file\u003e --language-code en-US\n```\n\n\nFor more details on getting started with this NIM, visit the [Riva ASR NIM Docs](https://docs.nvidia.com/nim/riva/asr/latest/overview.html).6a:Te6c,"])</script><script>self.__next_f.push([1,"# Model Overview\nParakeet-CTC-XL-0.6B (around 600M parameters) is trained on ASRSet with over 35000 hours of English (en-US) speech. The model transcribes speech in lower case English alphabet along with spaces and apostrophes.\n\n## Description\nParakeet transcribes audio into text, using spaces and apostrophes where needed \u003cbr\u003e\n\n## Terms of use\nBy using this software or microservice, you are agreeing to the [terms and conditions](https://www.nvidia.com/en-us/data-center/products/nvidia-ai-enterprise/eula/) of the license and acceptable use policy.\n\n# Disclaimer\nAI models generate responses and outputs based on complex algorithms and machine learning techniques, and those responses or outputs may be inaccurate or indecent. By testing this model, you assume the risk of any harm caused by any response or output of the model. Please do not upload any confidential information or personal data. Your use is logged for security. \n\n# References\n* [Paper](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fast-conformer)\n* [Fast-Conformer-CTC Model](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fast-conformer)\n* [Conformer](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/models.html#conformer)\n\n## Model Architecture\n**Architecture Type:** Parakeet-CTC (also known as FastConformer-CTC), which is an optimized version of Conformer model with 8x depthwise-separable convolutional downsampling with CTC loss \u003cbr\u003e \n**Network Architecture:** Parakeet-CTC-XL-0.6B \u003cbr\u003e\n\n## Input\n**Input Type(s):** Audio in English \u003cbr\u003e\n**Input Format(s):** Linear PCM 16-bit 1 channel \u003cbr\u003e\n**Other Properties Related to Input:** Maximum Length in seconds specific to GPU Memory, No Pre-Processing Needed, Mono channel is required \u003cbr\u003e\n\n## Output\n**Output Type(s):** Text String in English with Timestamps \u003cbr\u003e\n**Output Parameters:** 1-Dimension \u003cbr\u003e\n**Other Properties Related to Output:** No Maximum Character Length, Does not handle special characters \u003cbr\u003e\n\n\n## Software Integration\n**Runtime Engine(s):** \n* Riva 2.15.0 or Higher \u003cbr\u003e\n\n**Supported Operating System(s):**  \u003cbr\u003e\n* Linux \u003cbr\u003e\n\n## Model Version\nParakeet-0.6b-ctc-en-us-asr-set-6.0 \u003cbr\u003e\n\n# Inference\n**Engine:** Triton \u003cbr\u003e\n**Test Hardware:**  \u003cbr\u003e\n* NVIDIA A2 \u003cbr\u003e\n* NVIDIA A10 \u003cbr\u003e\n* NVIDIA A16 \u003cbr\u003e\n* NVIDIA A30 \u003cbr\u003e\n* NVIDIA A40 \u003cbr\u003e\n* NVIDIA A100 \u003cbr\u003e\n* NVIDIA H100 \u003cbr\u003e\n* NVIDIA L4 \u003cbr\u003e\n* NVIDIA L40 \u003cbr\u003e\n* GeForce RTX 40xx \u003cbr\u003e\n* GeForce RTX 50xx \u003cbr\u003e\n* Blackwell RTX 60xx \u003cbr\u003e\n\n\n## Ethical Considerations (For NVIDIA Models Only):\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety \u0026 Security, and Privacy Subcards.  Please report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).\n\n## GOVERNING TERMS: \nThis trial is governed by the NVIDIA API Trial Terms of Service (found at https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf). The use of this model is governed by the AI Foundation Models Community License Agreement (found at NVIDIA Agreements | Enterprise Software | NVIDIA AI Foundation Models Community License Agreement)."])</script><script>self.__next_f.push([1,"6b:T996,"])</script><script>self.__next_f.push([1,"Field                                                                                                  |  Response\n:------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------\nIntended Applications \u0026 Domains:                                                                       |  Speech Transcription\nTypes:                                                                                                 |  Speech Transcription\nIntended Users:                                                                                        |  This model is intended for developers and data scientists building interactive call centers, virtual assistants, language learning assistants.\nOutput:                                                                                                |  Transcribed text with timestamps and confidence scores\nDescribe how the model works:                                                                          |  Model transcribes audio input into text for the input language\nName the adversely impacted groups this has been tested to deliver comparable outcomes regardless of:  |  Age, Gender, National Origin\nTechnical Limitations:                                                                                 |  Transcripts may not be 100% accurate. Accuracy varies based on the characteristics of input audio (Domain, Use Case, Accent, Noise, Speech Type, Context of speech, etc.)\nVerified to have met prescribed NVIDIA quality standards:                                                     |  Yes\nPerformance Metrics:                                                                                   |  Word Error Rate (WER), Silence Robustness (Characters/mins of silent audio), Latency (in milliseconds), Throughput (Total audio processed per unit of time)\nPotential Known Risks:                                                                                 |  Not recommended for word-for-word transcription as accuracy varies based on the characteristics of input audio (domain, use case, accent, noise, speech type, and context of speech)\nLicensing:                                                                                             |  [https://docs.nvidia.com/ai-foundation-models-community-license.pdf](https://docs.nvidia.com/ai-foundation-models-community-license.pdf)"])</script><script>self.__next_f.push([1,"6c:T83d,"])</script><script>self.__next_f.push([1,"Field                                                                                                                              |  Response\n:----------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------\nGeneratable or reverse engineerable personally-identifiable information (PII)?                                                     |  None\nWas consent obtained for any personal data used?                                                                                   |  Yes\nProtected class data used to create this model?                                                                                    |  Age, Gender, Linguistic Background, National Origin\nHow often is dataset reviewed?                                                                                                     |  Before Release\nIs a mechanism in place to honor data subject right of access or deletion of personal data?                                        |  No\nIf Personal data collected for the development of the model, was it collected directly by NVIDIA?                                  |  Personal data not collected by NVIDIA for development of model\nIf Personal data collected for the development of the model by NVIDIA, do you maintain or have access to disclosures made to data subjects?  |  Not Applicable\nIf Personal data collected for the development of this AI model, was it minimized to only what was required?                                 |  Yes\nIs there provenance for all datasets used in training?                                                                                                                          |  Yes\nDoes data labeling (annotation, metadata) comply with privacy laws?                                                                |  Yes  \nIs data compliant with data subject requests for data correction or removal, if such a request was made?                           |  The data is compliant where applicable, but is not applicable for all data."])</script><script>self.__next_f.push([1,"6d:T688,### Getting Started\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Instructions below demonstrate usage of \u003c%- name %\u003e model using Python gRPC client.\n\n### Prerequisites\n\nYou will need a system with Git and Python 3+ installed.\n\n### Install Riva Python Client\n\n```bash\npip install nvidia-riva-client\n```\n\n### Download Python Client\n\nDownload Python client code by cloning \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e.\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\n### Run Python Client\n\nOpen a command terminal and execute below command to transcribe audio. Make sure you have a speech file in 16-bit Mono format in WAV/OGG/OPUS container. If you have generated the API key, it will be auto-populated in the command.\n\n```bash\npython python-clients/scripts/asr/transcribe_file.py \\\n    --server grpc.nvcf.nvidia.com:443 --use-ssl \\\n    --metadata function-id \"\u003c%- nvcfFunctionId %\u003e\" \\\n    --metadata \"authorization\" \"Bearer \u003c%- apiKey %\u003e\" \\\n    --language-code en-US \\\n    --input-file \u003cpath_to_audio_file\u003e\n```\n\n### Support for gRPC clients in other languages\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Proto files can be downloaded from \u003ca href=\"https://github.com/nvidia-riva/common/archive/refs/heads/main.zip\"\u003eRiva gRPC Proto files\u003c/a\u003e and compiled to target language using \u003ca href=\"https://grpc.io/docs/protoc-installation/\"\u003eProtoc compiler\u003c/a\u003e. Example Riva clients in C++ and Python languages are provided below.\n\n* \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e\n* \u003ca href=\"https://github.com/nvidia-riva/cpp-clients\"\u003eC++ Client Repository\u003c/a\u003e\n6e:T703,## Step 1. Generate API Key\n\n::generate-api-key\n\n## Step 2. Pull and Run the NIM\n\n```bash\n$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: \u003cPASTE_API_KEY_HERE\u003e\n```\n\nRefer [Supported Models](https://docs.nvidia.com/nim/riva/asr/latest/getting-started.html#supported-models) for full list of models.\n\n```bash\nexport NGC_API_KEY=\u003cPASTE_API_KEY_HERE\u003e\n\ndocker"])</script><script>self.__next_f.push([1," run -it --rm \\\n   --runtime=nvidia \\\n   --gpus '\"device=0\"' \\\n   --shm-size=8GB \\\n   -e NGC_API_KEY \\\n   -e NIM_HTTP_API_PORT=9000 \\\n   -e NIM_GRPC_API_PORT=50051 \\\n   -p 9000:9000 \\\n   -p 50051:50051 \\\n   -e NIM_TAGS_SELECTOR=name=parakeet-0-6b-ctc-en-us,mode=ofl,bs=1 \\\n   nvcr.io/nim/nvidia/parakeet-0-6b-ctc-en-us:latest\n```\n\nIt may take a up to 30 minutes depending on your network speed, for the container to be ready and start accepting requests from the time the docker container is started.\n\n## Step 3. Test the NIM\n\nOpen a new terminal and run following command to check if the service is ready to handle inference requests\n\n```bash\ncurl -X 'GET' 'http://localhost:9000/v1/health/ready'\n```\n\nIf the service is ready, you get a response similar to the following.\n```bash\n{\"ready\":true}\n```\n\nInstall the Riva Python client package\n\n```bash\nsudo apt-get install python3-pip\npip install nvidia-riva-client\n```\n\nDownload Riva sample clients\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\nRun Speech to Text inference in streaming modes. Riva ASR supports Mono, 16-bit audio in WAV, OPUS and FLAC formats.\n\n```bash\npython3 python-clients/scripts/asr/transcribe_file_offline.py --server 0.0.0.0:50051 --input-file \u003cpath_to_speech_file\u003e --language-code en-US\n```\n\n\nFor more details on getting started with this NIM, visit the [Riva ASR NIM Docs](https://docs.nvidia.com/nim/riva/asr/latest/overview.html).6f:Tc82,"])</script><script>self.__next_f.push([1,"## Requirements\n\n* NVIDIA GeForce RTX 40xx or above (see [supported GPUs](https://docs.nvidia.com/nim/riva/asr/latest/support-matrix.html))  \n* Install the latest [NVIDIA GPU Driver](https://www.nvidia.com/en-us/drivers/) on Windows (Version 570+)  \n\n## Step 1\\. Open the Windows Subsystem for Linux 2 \\- WSL2 \\- Distro\n\n[Install WSL2](https://assets.ngc.nvidia.com/products/api-catalog/rtx/Riva_ASR_Setup.exe). For additional instructions refer to the [documentation](https://docs.nvidia.com/nim/wsl2/latest/getting-started.html#installation).\n\nOnce installed, open the `NVIDIA-Workbench` WSL2 distro using the following command in the Windows terminal.\n\n```\nwsl -d NVIDIA-Workbench\n```\n\n## Step 2\\. Run the Container\n\n::generate-api-key\n\n```\n$ podman login nvcr.io\nUsername: $oauthtoken\nPassword: \u003cPASTE_API_KEY_HERE\u003e\n```\n\nPull and run the NVIDIA NIM with the command below.\n\n```\nexport NGC_API_KEY=\u003cPASTE_API_KEY_HERE\u003e\nexport LOCAL_NIM_CACHE=~/.cache/nim\nmkdir -p \"$LOCAL_NIM_CACHE\"\nchmod -R a+w \"$LOCAL_NIM_CACHE\"\npodman run -it --rm \\\n    --device nvidia.com/gpu=all \\\n    --shm-size=16GB \\\n    -e NGC_API_KEY=$NGC_API_KEY \\\n    -e NIM_TAGS_SELECTOR=name=parakeet-0-6b-ctc-en-us,mode=ofl,bs=1 \\\n    -e NIM_HTTP_API_PORT=9000 \\\n    -e NIM_GRPC_API_PORT=50051 \\\n    -e NIM_RELAX_MEM_CONSTRAINTS=1 \\\n    -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n    -u $(id -u) \\\n    -p 9000:9000 \\\n    -p 50051:50051 \\\n    nvcr.io/nim/nvidia/parakeet-0-6b-ctc-en-us:latest\n```\n\nIt may take up to 30 minutes depending on your network speed for the container to be ready and start accepting requests from the time the docker container is started.\n\n## Step 3\\. Test the NIM\n\nOpen a new Distro instance and run following command to check if the service is ready to handle inference requests\n\n```\ncurl -X 'GET' 'http://localhost:9000/v1/health/ready'\n```\n\nIf the service is ready, you get a response similar to the following.\n\n```\n{\"ready\":true}\n```\n\nInstall the Riva Python client package\n\n```\nsudo apt-get install python3-pip\npip install nvidia-riva-client\n```\n\nDownload Riva sample clients\n\n```\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\nRun Speech to Text inference in streaming modes. Riva ASR supports Mono, 16-bit audio in WAV, OPUS and FLAC formats.\n\n```\npython3 python-clients/scripts/asr/transcribe_file_offline.py --server 0.0.0.0:50051 --input-file \u003cpath_to_speech_file\u003e --language-code en-US\n```\n\nFor more details on getting started with this NIM, visit the [NVIDIA NIM Docs](https://docs.nvidia.com/nim/riva/asr/latest/overview.html).\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n"])</script><script>self.__next_f.push([1,"70:T572,Field                                                                                               |  Response\n:---------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------\nWhat is the language balance of the model validation data? | ~3% per language across following languages and dialects: Arabic, Danish, Czech, German, American English, British English, Latin American Spanish, European Spanish, Metropole French, Canadian French, Hebrew, Hindi, Italian, Japanese, Korean, Norwegian Bokmal, Dutch, Polish, European Portuguese, Brazilian Portuguese, Russian, Swedish, Thai, Turkish, Mandarin Chinese.  \nWhat is the geographic origin language balance of the model validation data? | Middle East, United States, Europe, East Asia, South Asia, Africa, Latin America, Southeast Asia\nWhat is the accent balance of the model validation data? | ~3% per language.\nParticipation considerations from adversely impacted groups ([protected classes](https://www.senate.ca.gov/content/protected-classes)) in model design and testing:  |  Age, Gender, Linguistic Background\nMeasures taken to mitigate against unwanted bias:                                                   |  Used a custom dataset to validate model performance across demographic groups (gender, age, and language).71:T1a25,"])</script><script>self.__next_f.push([1,"# Speech Recognition/Translation: Canary\n          \n## Description\n\nRIVA Canary-1b-Flash (around 1B parameters) [1][2] is a multi-lingual multi-tasking model that supports automatic speech-to-text recognition (ASR) and automatic speech-to-text translation (AST) between all non-English and English in both directions for the following languages: Arabic (ar-AR), Danish (da-DK), Czech (cs-CZ), English (en-US), Spanish (es-US, es-ES), German (de-DE), French (fr-FR), Hindi (hi-IN), Italian (it-IT), Portuguese (pt-BR, pt-PT), Japanese (ja-JP), Korean (ko-KR), Dutch (nl-NL), Norwegian (nb-NO), Polish (pl-PO), Russian (ru-RU), Swedish (sv-SE), Turkish (tr-TR), and Mandarin (zh-CN) with punctuation and capitalization (PnC). Bulgarian (bg-BG), Greek (el-GR), Estonian (et-EE), Finnish (fi-FI), Croatian (hr-HR), Hungarian (hu-HU), Indonesian (id-ID), Lithuanian (lt-LT), Latvian (lv-LV), Romanian (ro-RO), Slovak (sk-SK), Slovenian (sl-SI), Ukrainian (uk-UA) and Vietnamese (vi-VN) are also supported as target languages for translation from English. British English (en-GB), Canadian French (fr-CA), and Norwegian Nynorsk (nn-NO) are supported for ASR and translation to English. Hebrew (he-IL) is supported for ASR. Thai (th-TH) is supported for ASR and translation target from English.\n\nThis model is ready for commercial use. \n\n### License/Terms of Use\n \nGOVERNING TERMS: The NIM container is governed by the [NVIDIA Software License Agreement](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/) and [Product-Specific Terms for AI Products](https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/). Use of this model is governed by the [NVIDIA Community Model License](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/).\n\n\n## Deployment Geography:\nGlobal\n\n## Use Case:\nThis model serves developers, researchers, academics, and industries building applications that require speech-to-text capabilities, including but not limited to: conversational AI, voice assistants, transcription services, subtitle generation, and voice analytics platforms.\n\n#Release Date:\nBuild.Nvidia.com [10/15/2025] via [https://build.nvidia.com/nvidia/canary-1b-asr] \nHugging Face [10/15/2025] via [URL] \nNGC [10/15/2025] via [https://build.stg.ngc.nvidia.com/nvidia/canary-1b-asr] \n\n\n## References\n\n[1] [Less is More: Accurate Speech Recognition \u0026 Translation without Web-Scale Data](https://arxiv.org/pdf/2406.19674) \u003cbr\u003e\n[2] [Training and Inference Efficiency of Encoder-Decoder Speech Models](https://arxiv.org/pdf/2503.05931) \u003cbr\u003e\n[3] [New Standard for Speech Recognition and Translation from the NVIDIA NeMo Canary Model](https://developer.nvidia.com/blog/new-standard-for-speech-recognition-and-translation-from-the-nvidia-nemo-canary-model/) \u003cbr\u003e\n[4] [Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition](https://arxiv.org/abs/2305.05084) \u003cbr\u003e\n[6] [Attention Is All You Need](https://arxiv.org/abs/1706.03762) \u003cbr\u003e\n\n## Model Architecture\n\n**Architecture Type:** This model was developed based on the Canary Flash Architecture [2]. An encoder-decoder model with FastConformer [4] encoder and Transformer decoder [5]  \u003cbr\u003e\n**Network Architecture:** 42 layer encoder, 8 layer decoder, Number of model parameters: 918M \u003cbr\u003e\n\n\n## Input\n \n**Input Type(s):** Audio \u003cbr\u003e\n**Input Format:** wav \u003cbr\u003e\n**Input Parameters:** One-Dimensional (1D) \u003cbr\u003e\n**Other Properties Related to Input:** Mono channel is required \u003cbr\u003e\n\n## Output\n \n**Output Type(s):** Text \u003cbr\u003e\n**Output Format:** String \u003cbr\u003e\n**Output Parameters:** One-Dimensional (1D) \u003cbr\u003e\n**Other Properties Related to Output:** No Maximum Character Length, Does not handle special characters \u003cbr\u003e\n\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA’s hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.\n\n## Software Integration\n\n**Runtime Engine(s):** \n* Riva 2.23.0 or higher \u003cbr\u003e\n\n**Supported Hardware Microarchitecture Compatibility:** \u003cbr\u003e\n* NVIDIA Ampere \u003cbr\u003e\n* NVIDIA Hopper \u003cbr\u003e\n* NVIDIA Jetson \u003cbr\u003e\n* NVIDIA Turing \u003cbr\u003e\n* NVIDIA Volta \u003cbr\u003e\n\n**[Preferred/Supported] Operating System(s):** \u003cbr\u003e\n* Linux \u003cbr\u003e\n* Linux 4 Tegra \u003cbr\u003e\n\nThe integration of foundation and fine-tuned models into AI systems requires additional testing using use-case-specific data to ensure safe and effective deployment. Following the V-model methodology, iterative testing and validation at both unit and system levels are essential to mitigate risks, meet technical and functional requirements, and ensure compliance with safety and ethical standards before deployment.\n\n## Model Version(s): \n\nCanary-1B-Flash 2.0\n\n# Training \u0026 Evaluation \n\n## Training Dataset\n\n**Data Modality**\n\n[Audio]\u003cbr\u003e\n[Text]\n\n**Audio Training Data Size**\n\n[10,000 to 1 Million Hours]\n\n**Data Collection Method by dataset** \u003cbr\u003e\n\n* Human \u003cbr\u003e\n\n**Labeling Method by dataset**\u003cbr\u003e\n\n* ASR: Human \u003cbr\u003e\n* AST: Human, Synthetic \u003cbr\u003e\n\n**Properties:**  \u003cbr\u003e\n\nMixture of organic ASR data aligned with human voices and machine generated translations to create AST pairings.\n\n## Evaluation Dataset\n\n**Data Collection Method by dataset** \u003cbr\u003e\n\n* Human \u003cbr\u003e\n\n**Labeling Method by dataset**\u003cbr\u003e\n\n* Hybrid: Human, Synthetic \u003cbr\u003e \n\n**Properties:**\n\nA dynamic blend of public and internal proprietary and customer datasets aligning text with human audio data.\n\n## Inference\n\n**Acceleration Engine:** Triton \u003cbr\u003e\n**Test Hardware:** \u003cbr\u003e\n* NVIDIA A10 \n* NVIDIA A100 \n* NVIDIA A30 \n* NVIDIA H100 \n* NVIDIA L4 \n* NVIDIA L40 \n* NVIDIA Turing T4\n* NVIDIA Volta V100\n\n## Get Help\n\n### Enterprise Support\n\nGet access to knowledge base articles and support cases or [submit a ticket](https://www.nvidia.com/en-us/data-center/products/ai-enterprise-suite/support/).\n\n## Ethical Considerations\n\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  \n\nFor more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety \u0026 Security, and Privacy Subcards.  \n\nPlease report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/)."])</script><script>self.__next_f.push([1,"72:Tc68,"])</script><script>self.__next_f.push([1,"Field                                                                                                  |  Response\n:------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------\nIntended Applications \u0026 Domains:                                                                       |  FastConformer-Encoder Transformer-Decoder Model for Automatic Speech Recognition (ASR) and Translation (AST)\nModel Type:                                                                                            |  Speech Recognition and Speech Translation\nIntended Users:                                                                                        |  This model is intended for developers and data scientists who are building interactive call-center applications, virtual assistants, and language-learning assistants.\nOutput:                                                                                                |  Text Sequence\nDescribe how the model works:                                                                          |  Model transcribes audio input into text for the input language or translates audio input into target language based on  probability of next word and audio input.\nName the adversely impacted groups this has been tested to deliver comparable outcomes regardless of:  |  Age, Gender, National Origin\nTechnical Limitations:                                                                                 |  Transcripts may not be 100% accurate. \u003cbr\u003e  Translations may be technically correct but lack fluency. \u003cbr\u003e  Translations may show bias towards grammatical gender in some European languages. \u003cbr\u003e  Translations may show bias towards formal-informal tense in Korean and Japanese. \u003cbr\u003e  Dialects not represented in training will likely see poor performance on both ASR and AST.\nVerified to have met prescribed NVIDIA quality standards:                                              |  Yes\nPerformance Metrics:                                                                                   |  Word Error Rate (WER) of the ASR transcription. Bilingual Evaluation Understudy (BLEU) of the AST translation.\nPotential Known Risks:                                                                                 |  If an audio sample occurs outside the domain of training, there is a higher likelihood of mistranslation/poor transcription. Dialects not represented in training will likely see poor performance on both ASR and AST.\nLicensing:                                                                                             |  The NIM container is governed by the [NVIDIA Software License Agreement](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/) and [Product-Specific Terms for AI Products.](https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/) Use of this model is governed by the [NVIDIA Community Model License.](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/)"])</script><script>self.__next_f.push([1,"73:T505,Field                                                                                                                              |  Response\n:----------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------\nGeneratable or reverse engineerable personal data?                                                                                 |  No\nPersonal data used to create this model?                                                                                           |  No\nHow often is dataset reviewed?                                                                                                     |  Before Release\nIs there provenance for all datasets used in training?                                                                             |  Yes\nIs data compliant with data subject requests for data correction or removal, if such a request was made?                           |  The data is compliant where applicable, but is not applicable for all data.\nApplicable Privacy Policy                                                                                                          | https://www.nvidia.com/en-us/about-nvidia/privacy-policy/74:T447,Field                                               |  Response\n:---------------------------------------------------|:----------------------------------\nModel Application Field(s):                         |  Automatic Speech Recognition \u0026 Translation\nDescribe the life-critical impacts (if present).    |  Not Applicable\nUse Case Restriction(s):                            |  Abide by the [NVIDIA Software License Agreement](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/) and [Product-Specific Terms for AI Products](https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/). Use of this model is governed by the [NVIDIA Community Model License](https://w"])</script><script>self.__next_f.push([1,"ww.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/).\nDescribe access restrictions (if any):              | The Principle of Least Privilege (PoLP) is applied limiting access for dataset generation and model development. Restrictions enforce dataset access during training and dataset license constraints adhered to.75:T9a8,"])</script><script>self.__next_f.push([1,"### Getting Started\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Instructions below demonstrate usage of \u003c%- name %\u003e model using Python gRPC client.\n\n### Prerequisites\n\nYou will need a system with Git and Python 3+ installed.\n\n### Install Riva Python Client\n\n```bash\npip install nvidia-riva-client\n```\n\n### Download Python Client\n\nDownload Python client code by cloning \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e.\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\n### Run Python Client\n\nMake sure you have a speech file in Mono, 16-bit audio in WAV, OPUS and FLAC formats. If you have generated the API key, it will be auto-populated in the command. Open a command terminal and execute below command to transcribe audio. If you know the source language, it is recommended to pass `source_language` in custom configuration parameter.\n\nBelow command demonstrates transcription of English audio file.\n\n```bash\npython python-clients/scripts/asr/transcribe_file_offline.py \\\n    --server grpc.nvcf.nvidia.com:443 --use-ssl \\\n    --metadata function-id \"\u003c%- nvcfFunctionId %\u003e\" \\\n    --metadata \"authorization\" \"Bearer \u003c%- apiKey %\u003e\" \\\n    --language-code en-US \\\n    --input-file \u003cpath_to_audio_file\u003e\n```\n\nBelow command demonstrates translation from English audio to Hindi.\n\n```bash\npython python-clients/scripts/asr/transcribe_file_offline.py \\\n    --server grpc.nvcf.nvidia.com:443 --use-ssl \\\n    --metadata function-id \"\u003c%- nvcfFunctionId %\u003e\" \\\n    --metadata \"authorization\" \"Bearer \u003c%- apiKey %\u003e\" \\\n    --language-code en-US \\\n    --custom-configuration \"target_language:hi-IN,task:translate\" \\\n    --input-file \u003cpath_to_audio_file\u003e\n```\n\nOne can transcribe and translate supported languages by changing the source language via `--language-code` and target language via `target_language` parameter.\n\n### Support for gRPC clients in other programming languages\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Proto files can be downloaded from \u003ca href=\"https://github.com/nvidia-riva/common/archive/refs/heads/main.zip\"\u003eRiva gRPC Proto files\u003c/a\u003e and compiled to target language using \u003ca href=\"https://grpc.io/docs/protoc-installation/\"\u003eProtoc compiler\u003c/a\u003e. Example Riva clients in C++ and Python languages are provided below.\n\n* \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e\n* \u003ca href=\"https://github.com/nvidia-riva/cpp-clients\"\u003eC++ Client Repository\u003c/a\u003e\n"])</script><script>self.__next_f.push([1,"76:T6ec,## Step 1. Generate API Key\n\n::generate-api-key\n\n## Step 2. Pull and Run the NIM\n\n```bash\n$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: \u003cPASTE_API_KEY_HERE\u003e\n```\n\nRefer to [Supported Models](https://docs.nvidia.com/nim/riva/asr/latest/getting-started.html#supported-models) for full list of models.\n\n```bash\nexport NGC_API_KEY=\u003cPASTE_API_KEY_HERE\u003e\n\ndocker run -it --rm --name=canary-1b \\\n   --runtime=nvidia \\\n   --gpus '\"device=0\"' \\\n   --shm-size=8GB \\\n   -e NGC_API_KEY \\\n   -e NIM_HTTP_API_PORT=9000 \\\n   -e NIM_GRPC_API_PORT=50051 \\\n   -p 9000:9000 \\\n   -p 50051:50051 \\\n   -e NIM_TAGS_SELECTOR=name=canary-1b \\\n   nvcr.io/nim/nvidia/canary-1b:latest\n```\n\nIt may take a up to 30 minutes depending on your network speed, for the container to be ready and start accepting requests from the time the docker container is started.\n\n## Step 3. Test the NIM\n\nOpen a new terminal and run following command to check if the service is ready to handle inference requests\n\n```bash\ncurl -X 'GET' 'http://localhost:9000/v1/health/ready'\n```\n\nIf the service is ready, you get a response similar to the following.\n```bash\n{\"ready\":true}\n```\n\nInstall the Riva Python client package\n\n```bash\nsudo apt-get install python3-pip\npip install nvidia-riva-client\n```\n\nDownload Riva sample clients\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\nRun Speech to Text inference in offline modes. Riva ASR supports Mono, 16-bit audio in WAV, OPUS and FLAC formats.\n\n```bash\npython3 python-clients/scripts/asr/transcribe_file_offline.py --server 0.0.0.0:50051 --input-file \u003cpath_to_speech_file\u003e --language-code en-US\n```\n\n\nFor more details on getting started with this NIM, visit the [Riva ASR NIM Docs](https://docs.nvidia.com/nim/riva/asr/latest/overview.html).\n77:T669,Field                                                                                               |  Response\n:---------------------------------------------------------------------------------------------------|:----------------------------------------------"])</script><script>self.__next_f.push([1,"-----------------------------------\nWhat is the language balance of the model validation data? | en-US: 35.0%, es-US: 9.66%, ko-KR: 8.07%, ru-RU: 6.33%, ar-AR: 5.65%, de-DE: 5.2%, fr-FR: 4.6%, hi-IN: 4.59%, it-IT: 4.2%, pt-BR: 3.5%, ja-JP: 2.82%, es-ES: 1.9%, pt-PT: 1.5%, nl-NL: 1.4%, tr-TR: 1.3%, fr-CA: 1.1%, en-GB: 0.95%, pl-PL: 0.64%, th-TH: 0.59%, he-IL: 0.26%, cs-CZ: 0.21%, da-DK: 0.19%, nb-NO: 0.18%, sv-SE: 0.16%\nWhat is the geographic origin language balance of the model validation data? | North America: 45.75%, Europe: 27.47%, Asia: 16.1%, Middle East: 7.2%, South America: 3.5%\nWhat is the accent balance of the model validation data? | en-US: 35.0%, es-US: 9.66%, ko-KR: 8.07%, ru-RU: 6.33%, ar-AR: 5.65%, de-DE: 5.2%, fr-FR: 4.6%, hi-IN: 4.59%, it-IT: 4.2%, pt-BR: 3.5%, ja-JP: 2.82%, es-ES: 1.9%, pt-PT: 1.5%, nl-NL: 1.4%, tr-TR: 1.3%, fr-CA: 1.1%, en-GB: 0.95%, pl-PL: 0.64%, th-TH: 0.59%, he-IL: 0.26%, cs-CZ: 0.21%, da-DK: 0.19%, nb-NO: 0.18%, sv-SE: 0.16%\nParticipation considerations from adversely impacted groups ([protected classes](https://www.senate.ca.gov/content/protected-classes)) in model design and testing:  |  Age, Gender, Linguistic Background\nMeasures taken to mitigate against unwanted bias:                                                   |  Used custom dataset to validate model performance across gender, age, and linguistic demographics78:Tda9,"])</script><script>self.__next_f.push([1,"# Speech Recognition: Parakeet \n          \n## Description\n\nRIVA Parakeet-RNNT-XXL-1.1B ASR Multilingual with Universal Tokenizer (around 1.1B parameters) [1], [2] is trained on ASR Set with over 90,000 hours of speech. Individual tokenizers are trained for each language and then merged into a single tokenizer. The model transcribes 25 languages (English(en-US, en-GB), Spanish (es-US, es-ES), German (de-DE), French (fr-FR, fr-CA), Italian (it-IT), Arabic (ar-AR), Japanese (ja-JP), Korean (ko-KR), Portuguese (pt-BR, pt-PT), Russian (ru-RU), Hindi (hi-IN), Dutch (nl-NL), Danish (da-DK), Norwegian Nynorsk (nn-NO), Norwegian Bokmal (nb-NO), Czech (cs-CZ), Polish (pl-PL), Swedish (sv-SE), Thai (th-TH), Turkish (tr-TR), and Hebrew (he-IL)) in upper case and lower case alphabets along with punctuations, spaces, and apostrophes.\n\nThis model is ready for commercial use.\n\n## License/Terms of Use\n\nGOVERNING TERMS: The use of this model is governed by the NVIDIA Community Model License (found at https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/).\n\n## References\n\n[1] [Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition](https://arxiv.org/abs/2305.05084) \u003cbr\u003e\n[2] [Fast-Conformer-RNNT Model](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/models.html) \u003cbr\u003e\n[3] [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100) \u003cbr\u003e\n\n## Model Architecture\n \n**Architecture Type:** Parakeet-RNNT (also known as FastConformer-RNNT) [1], [2] which is an optimized version of Conformer model [3] with 8x depthwise-separable convolutional downsampling with Hybrid loss \u003cbr\u003e\n**Network Architecture:** Parakeet-RNNT-XXL-1.1B \u003cbr\u003e\n\n## Input \n\n**Input Type(s):** Audio \u003cbr\u003e\n**Input Format(s):** wav \u003cbr\u003e\n**Input Parameters:** 1-Dimension \u003cbr\u003e\n**Other Properties Related to Input:** Maximum Length in seconds specific to GPU Memory, No Pre-Processing Needed, Mono channel is required \u003cbr\u003e\n\n## Output \n\n**Output Type(s):** Text \u003cbr\u003e\n**Output Format:** String \u003cbr\u003e\n**Output Parameters:** 1-Dimension \u003cbr\u003e\n**Other Properties Related to Output:** No Maximum Character Length, Does not handle special characters \u003cbr\u003e\n\n## Software Integration\n\n**Supported Hardware Microarchitecture Compatibility:** \u003cbr\u003e\n* NVIDIA Blackwell \u003cbr\u003e\n* NVIDIA Ampere \u003cbr\u003e\n* NVIDIA Hopper \u003cbr\u003e\n* NVIDIA Jetson \u003cbr\u003e\n* NVIDIA Turing \u003cbr\u003e\n\n**[Preferred/Supported] Operating System(s):** \u003cbr\u003e\n* Linux \u003cbr\u003e\n\n## Model Version(s): \n\nParakeet-RNNT-XXL-1.1b_universal_spe8.5k_1.0\n\n## Inference\n\n**Engine:** Triton \u003cbr\u003e\n**Test Hardware:** \u003cbr\u003e\n* NVIDIA A10 \n* NVIDIA A100 \n* NVIDIA A30 \n* NVIDIA H100 \n* NVIDIA L4 \n* NVIDIA L40 \n* NVIDIA Turing T4\n\n\n## Ethical Considerations (For NVIDIA Models Only):\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety \u0026 Security, and Privacy Subcards.  Please report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/)."])</script><script>self.__next_f.push([1,"79:T9e0,"])</script><script>self.__next_f.push([1,"Field                                                                                                  |  Response\n:------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------\nIntended Applications \u0026 Domains:                                                                       |  Speech Transcription\nModel Type:                                                                                                 |  Speech Transcription\nIntended Users:                                                                                        |  This model is intended for developers and data scientists building interactive call centers, virtual assistants, and language learning assistants\nOutput:                                                                                                |  Transcribed text with timestamps and confidence scores\nDescribe how the model works:                                                                          |  Model transcribes audio input into text for 25  languages\nName the adversely impacted groups this has been tested to deliver comparable outcomes regardless of:  |  Age, Gender, National Origin\nTechnical Limitations:                                                                                 |  Transcripts may not be 100% accurate. Accuracy varies based on language and characteristics of input audio (Domain, Use Case, Accent, Noise, Speech Type, Context of speech, etc.)\nVerified to have met prescribed NVIDIA quality standards:                                                     |  Yes\nPerformance Metrics:                                                                                   |  Word Error Rate (WER), Silence Robustness (Characters/mins of silent audio), Latency (in milliseconds), Throughput (Total audio processed per unit of time)\nPotential Known Risks:                                                                                 |  Not recommended for word-for-word transcription as accuracy varies based on the characteristics of input audio (language, domain, use case, accent, noise, speech type, and context of speech)\nLicensing:                                                                                             |  [https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/)"])</script><script>self.__next_f.push([1,"7a:T4e4,Field                                                                                                                              |  Response\n:----------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------\nGeneratable or reverse engineerable personal data?                                                     |  None\nPersonal data used to create this model?                                                                                   |  None\nHow often is dataset reviewed?                                                                                                     |  Before Release\nIs there data provenance for all datasets used in training?                                                                                                                          |  Yes\nDoes data labeling (annotation, metadata) comply with privacy laws?                                                                |  Yes  \nIs data compliant with data subject requests for data correction or removal, if such a request was made?                           |  The data is compliant where applicable, but is not applicable for all data.7b:T66f,### Getting Started\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Instructions below demonstrate usage of \u003c%- name %\u003e model using Python gRPC client.\n\n### Prerequisites\n\nYou will need a system with Git and Python 3+ installed.\n\n### Install Riva Python Client\n\n```bash\npip install -U nvidia-riva-client\n```\n\n### Download Python Client\n\nDownload Python client code by cloning \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e.\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\n### Run Python Client\n\nOpen a command terminal and execute below command to transcribe audio. Make sure you have a speech file in 16-bit Mono format in WAV/OGG/OPUS container. If you have generated the API key, it will be auto-populated in"])</script><script>self.__next_f.push([1," the command.\n\n```bash\npython python-clients/scripts/asr/transcribe_file.py \\\n    --server grpc.nvcf.nvidia.com:443 --use-ssl \\\n    --metadata function-id \"\u003c%- nvcfFunctionId %\u003e\" \\\n    --metadata \"authorization\" \"Bearer \u003c%- apiKey %\u003e\" \\\n    --input-file \u003cpath_to_audio_file\u003e\n```\n\n### Support for gRPC clients in other languages\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Proto files can be downloaded from \u003ca href=\"https://github.com/nvidia-riva/common/archive/refs/heads/main.zip\"\u003eRiva gRPC Proto files\u003c/a\u003e and compiled to target language using \u003ca href=\"https://grpc.io/docs/protoc-installation/\"\u003eProtoc compiler\u003c/a\u003e. Example Riva clients in C++ and Python languages are provided below.\n\n* \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e\n* \u003ca href=\"https://github.com/nvidia-riva/cpp-clients\"\u003eC++ Client Repository\u003c/a\u003e\n7c:T6f5,## Step 1. Generate API Key\n\n::generate-api-key\n\n## Step 2. Pull and Run the NIM\n\n```bash\n$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: \u003cPASTE_API_KEY_HERE\u003e\n```\n\nRefer [Supported Models](https://docs.nvidia.com/nim/riva/asr/latest/getting-started.html#supported-models) for full list of models.\n\n```bash\nexport NGC_API_KEY=\u003cPASTE_API_KEY_HERE\u003e\n\ndocker run -it --rm --name=parakeet-1-1b-rnnt-multilingual \\\n   --runtime=nvidia \\\n   --gpus '\"device=0\"' \\\n   --shm-size=8GB \\\n   -e NGC_API_KEY \\\n   -e NIM_HTTP_API_PORT=9000 \\\n   -e NIM_GRPC_API_PORT=50051 \\\n   -p 9000:9000 \\\n   -p 50051:50051 \\\n   -e NIM_TAGS_SELECTOR=mode=str \\\n   nvcr.io/nim/nvidia/parakeet-1-1b-rnnt-multilingual:latest\n```\n\nIt may take a up to 30 minutes depending on your network speed, for the container to be ready and start accepting requests from the time the docker container is started.\n\n## Step 3. Test the NIM\n\nOpen a new terminal and run following command to check if the service is ready to handle inference requests\n\n```bash\ncurl -X 'GET' 'http://localhost:9000/v1/health/ready'\n```\n\nIf the service is ready, you get a response similar to the following.\n```bash\n{\"ready\":true}\n```\n"])</script><script>self.__next_f.push([1,"\nInstall the Riva Python client package\n\n```bash\nsudo apt-get install python3-pip\npip install -U nvidia-riva-client\n```\n\nDownload Riva sample clients\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\nRun Speech to Text inference in streaming modes. Riva ASR supports Mono, 16-bit audio in WAV, OPUS and FLAC formats.\n\n```bash\npython3 python-clients/scripts/asr/transcribe_file.py --server 0.0.0.0:50051 --input-file \u003cpath_to_speech_file\u003e\n```\n\n\nFor more details on getting started with this NIM, visit the [Riva ASR NIM Docs](https://docs.nvidia.com/nim/riva/asr/latest/overview.html).7d:Tdb8,"])</script><script>self.__next_f.push([1,"# Model Overview\n\n## Description:\nThis model is used to transcribe short-form audio files and is designed to be compatible with *OpenAI's sequential long-form transcription algorithm*. Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labeled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning. Whisper-large-v3 is one of the 5 configurations of the model with 1550M parameters.\u003cbr\u003e\nThis model version is optimized to run with NVIDIA TensorRT-LLM. This model is ready for commercial use.\n\n\n## Third-Party Community Consideration\nThis model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see the Whisper Model Card on GitHub.(https://github.com/openai/whisper/blob/main/model-card.md).\n\n### License/Terms of Use: \n\nGOVERNING TERMS: Use of this model is governed by the [NVIDIA Community Model License Agreement](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/).\nADDITIONAL INFORMATION: [MIT License](https://github.com/openai/whisper/blob/main/LICENSE).\n\n\n## References:\nWhisper [website](https://openai.com/index/whisper/) \u003cbr\u003e\nWhisper paper: \u003cbr\u003e\n```\n@misc{radford2022robust,\n      title={Robust Speech Recognition via Large-Scale Weak Supervision}, \n      author={Alec Radford and Jong Wook Kim and Tao Xu and Greg Brockman and Christine McLeavey and Ilya Sutskever},\n      year={2022},\n      eprint={2212.04356},\n      archivePrefix={arXiv},\n      primaryClass={eess.AS}\n}\n```\n\n## Model Architecture: \n**Architecture Type:** Transformer (Encoder-Decoder) \u003cbr\u003e\n**Network Architecture:** Whisper \u003cbr\u003e\n\n\n## Input: \n**Input Type(s):** Audio, Text-Prompt \u003cbr\u003e\n**Input Format(s):** Linear PCM 16-bit 1 channel (Audio), String (Text Prompt) \u003cbr\u003e\n**Input Parameters:** One-Dimensional (1D) \u003cbr\u003e\n\n## Output:\n**Output Type(s):** Text\n**Output Format:** String\n**Output Parameters:** 1D\n\n\n**Supported Hardware Microarchitecture Compatibility:** \u003cbr\u003e\n* NVIDIA Ampere \u003cbr\u003e\n* NVIDIA Blackwell \u003cbr\u003e\n\n\n**Supported Operating System(s):** \u003cbr\u003e\n* Linux \u003cbr\u003e\n\n## Model Version(s): \n**Large-v3:** Whisper large-v3 has the same architecture as the previous large and large-v2 models, except for the following minor differences:\n- The spectrogram input uses 128 Mel frequency bins instead of 80.\n- A new language token for Cantonese.\n\n## Training Dataset:\n\n**Data Collection Method by dataset:** [Hybrid: human, automatic] \u003cbr\u003e\n\n**Labeling Method by dataset:** [Automated] \u003cbr\u003e\n\n**Dataset License(s):** NA\n\n\n## Inference:\n**Engine:** Tensor(RT)-LLM, Triton \u003cbr\u003e\n**Test Hardware:**\n- A100\n- H100\n\nFor more detail on model usage, evaluation, training data set and implications, please refer to [Whisper Model Card](https://github.com/openai/whisper/blob/main/model-card.md).\n\n## Ethical Considerations:\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  \nPlease report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/)."])</script><script>self.__next_f.push([1,"7e:T985,"])</script><script>self.__next_f.push([1,"### Getting Started\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Instructions below demonstrate usage of \u003c%- name %\u003e model using Python gRPC client.\n\n### Prerequisites\n\nYou will need a system with Git and Python 3+ installed.\n\n### Install Riva Python Client\n\n```bash\npip install nvidia-riva-client\n```\n\n### Download Python Client\n\nDownload Python client code by cloning \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e.\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\n### Run Python Client\n\nMake sure you have a speech file in Mono, 16-bit audio in WAV, OPUS and FLAC formats. If you have generated the API key, it will be auto-populated in the command. Open a command terminal and execute below command to transcribe audio. Specifying `--language-code` as `multi` will enable auto language detection. If you know the source language, it is recommended to specify for better accuracy and latency. See \u003ca href=\"https://github.com/openai/whisper/blob/main/whisper/tokenizer.py#L10\"\u003eSupported Languages\u003c/a\u003e for the list of all available languages and corresponding code.\n\n```bash\npython python-clients/scripts/asr/transcribe_file_offline.py \\\n    --server grpc.nvcf.nvidia.com:443 --use-ssl \\\n    --metadata function-id \"\u003c%- nvcfFunctionId %\u003e\" \\\n    --metadata \"authorization\" \"Bearer \u003c%- apiKey %\u003e\" \\\n    --language-code en \\\n    --input-file \u003cpath_to_audio_file\u003e\n```\n\nBelow command demonstrates translation from French (fr) to English.\n\n```bash\npython python-clients/scripts/asr/transcribe_file_offline.py \\\n    --server grpc.nvcf.nvidia.com:443 --use-ssl \\\n    --metadata function-id \"\u003c%- nvcfFunctionId %\u003e\" \\\n    --metadata \"authorization\" \"Bearer \u003c%- apiKey %\u003e\" \\\n    --language-code fr \\\n    --custom-configuration \"task:translate\" \\\n    --input-file \u003cpath_to_audio_file\u003e\n```\n\n### Support for gRPC clients in other languages\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Proto files can be downloaded from \u003ca href=\"https://github.com/nvidia-riva/common/archive/refs/heads/main.zip\"\u003eRiva gRPC Proto files\u003c/a\u003e and compiled to target language using \u003ca href=\"https://grpc.io/docs/protoc-installation/\"\u003eProtoc compiler\u003c/a\u003e. Example Riva clients in C++ and Python languages are provided below.\n\n* \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e\n* \u003ca href=\"https://github.com/nvidia-riva/cpp-clients\"\u003eC++ Client Repository\u003c/a\u003e\n"])</script><script>self.__next_f.push([1,"7f:T77c,## Step 1. Generate API Key\n\n::generate-api-key\n\n## Step 2. Pull and Run the NIM\n\n```bash\n$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: \u003cPASTE_API_KEY_HERE\u003e\n```\n\nLaunch the Riva ASR NIM with Whisper Large v3 multilingual model with the command below. \nRefer [Supported Models](https://docs.nvidia.com/nim/riva/asr/latest/getting-started.html#supported-models) for full list of models.\n\n```bash\nexport NGC_API_KEY=\u003cPASTE_API_KEY_HERE\u003e\n\ndocker run -it --rm --name=whisper-large-v3 \\\n   --runtime=nvidia \\\n   --gpus '\"device=0\"' \\\n   --shm-size=8GB \\\n   -e NGC_API_KEY \\\n   -e NIM_HTTP_API_PORT=9000 \\\n   -e NIM_GRPC_API_PORT=50051 \\\n   -p 9000:9000 \\\n   -p 50051:50051 \\\n   -e NIM_TAGS_SELECTOR=name=whisper-large-v3 \\\n   nvcr.io/nim/nvidia/whisper-large-v3:latest\n```\n\n```{note}\nIt may take a up to 30 minutes depending on your network speed, for the container to be ready and start accepting requests from the time the docker container is started.\n```\n\n## Step 3. Test the NIM\n\nOpen a new terminal and run following command to check if the service is ready to handle inference requests\n\n```bash\ncurl -X 'GET' 'http://localhost:9000/v1/health/ready'\n```\n\nIf the service is ready, you get a response similar to the following.\n```bash\n{\"ready\":true}\n```\n\nInstall the Riva Python client package\n\n```bash\nsudo apt-get install python3-pip\npip install -U nvidia-riva-client\n```\n\nDownload Riva sample clients\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\nRun Speech to Text inference in offline mode. Riva ASR supports Mono, 16-bit audio in WAV, OPUS and FLAC formats.\n\n```bash\npython3 python-clients/scripts/asr/transcribe_file_offline.py --server 0.0.0.0:50051 \\\n   --language-code \u003cBCP-47 language code\u003e --input-file \u003cpath_to_speech_file\u003e\n```\n\nFor more details on getting started with this NIM, visit the [NVIDIA NIM Docs](https://docs.nvidia.com/nim/riva/asr/latest/overview.html).\n\n80:T97e,"])</script><script>self.__next_f.push([1,"# Model Overview\n\nRIVA Parakeet-CTC-0.6B-Unified ASR Vietnamese (around 600M parameters) [1] is trained on an ASR dataset with over 4,000 hours of Vietnamese (vi) speech. The model transcribes speech in Vietnamese, in upper case and lower case alphabets, along with punctuations (period, comma, and question mark), and spaces. \n\nThis model is ready for commercial/non-commercial use.\n\n## Terms of use\nGOVERNING TERMS: Use of this model is governed by the [NVIDIA Community Model License Agreement](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/).\n\n\n## References\n* [Fast Conformer](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fast-conformer)\n\n## Model Architecture\n\n**Architecture Type:** Parakeet-CTC (also known as FastConformer-CTC) which is an optimized version of Conformer model with 8x depthwise-separable convolutional downsampling with CTC loss. \u003cbr\u003e\n**Network Architecture:** Parakeet-CTC-0.6B \u003cbr\u003e\n\n## Input\n**Input Type(s):** Audio in Vietnamese + English \u003cbr\u003e\n**Input Format(s):** Linear PCM 16-bit 1 channel \u003cbr\u003e\n**Input Parameters:** One-Dimensional (1D)\n\n## Output\n**Output Type(s):** Text String in Vietnamese + English with Timestamps \u003cbr\u003e\n**Output Format(s):** String\n**Output Parameters:** One-Dimensional (1D)\n\n\n**Supported Operating System(s):**  \u003cbr\u003e\n* Linux \u003cbr\u003e\n\n## Inference\n**Engine:** Triton \u003cbr\u003e\n**Test Hardware:**  \u003cbr\u003e\n* NVIDIA A2 \u003cbr\u003e\n* NVIDIA A10 \u003cbr\u003e\n* NVIDIA A16 \u003cbr\u003e\n* NVIDIA A30 \u003cbr\u003e\n* NVIDIA A40 \u003cbr\u003e\n* NVIDIA A100 \u003cbr\u003e\n* NVIDIA H100 \u003cbr\u003e\n* NVIDIA L4 \u003cbr\u003e\n* NVIDIA L40 \u003cbr\u003e\n* GeForce RTX 40xx \u003cbr\u003e\n* GeForce RTX 50xx \u003cbr\u003e\n* Blackwell RTX 60xx \u003cbr\u003e\n\n\n## Ethical Considerations (For NVIDIA Models Only):\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety \u0026 Security, and Privacy Subcards.  Please report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/)."])</script><script>self.__next_f.push([1,"81:T9ca,"])</script><script>self.__next_f.push([1,"| Field | Response |\n| ----- | ----- |\n| Intended Task/Domain: | Speech to text transcription |\n| Model Type: | ASR |\n| Intended Users: | This model is intended for developers, researchers, academics, and industries building conversational based applications.    |\n| Output: | Transcribed text with timestamps and confidence scores |\n| Describe how this model works: | The model transcribes audio input into text in the input language. |\n| Name the adversely impacted groups this has been tested to deliver comparable outcomes regardless of: | Age, Gender, National Origin |\n| Technical Limitations \u0026 Mitigation: | Transcripts may not be 100% accurate. Accuracy varies based on the characteristics of the input audio (domain, use case, accent, noise, speech type, context of speech, etc.). |\n| Verified to have met prescribed NVIDIA quality standards: Performance Metrics: | Yes Word Error Rate (WER), Silence Robustness (characters per minute of silent audio), Latency (milliseconds), Throughput (total audio processed per unit of time). |\n| Potential Known Risks: | Not recommended for word-for-word transcription as accuracy varies based on the characteristics of the input audio (domain, use case, accent, noise, speech type, and context of speech). |\n| Licensing | **GOVERNING TERMS**: Your use of this API is governed by the [NVIDIA API Trial Service Terms of Use](https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Flegal%2FNVIDIA%2520API%2520Trial%2520Terms%2520of%2520Service.pdf\u0026data=05%7C02%7Cjaydar%40nvidia.com%7C95bb810010dc4a66c6da08dde4f09e53%7C43083d15727340c1b7db39efd9ccc17a%7C0%7C0%7C638918444317044724%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C\u0026sdata=vvzmA%2Bo2tdifKPXG%2FGjgN4TPVdoUTsXD4%2BE%2B1M%2F7CWI%3D\u0026reserved=0); and the use of this model is governed by the [NVIDIA Agreements | Enterprise Software | NVIDIA Community Model License](https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.nvidia.com%2Fen-us%2Fagreements%2Fenterprise-software%2Fnvidia-community-models-license%2F\u0026data=05%7C02%7Cjaydar%40nvidia.com%7C95bb810010dc4a66c6da08dde4f09e53%7C43083d15727340c1b7db39efd9ccc17a%7C0%7C0%7C638918444317058237%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C\u0026sdata=c%2B1IgzlTBZaDGUkIFi08wYwzVcmWVYnCCBwztkCBRTI%3D\u0026reserved=0). |"])</script><script>self.__next_f.push([1,"82:T698,| Field | Response |\n| ----- | ----- |\n| Model Application Field(s): | Speech to text transcription |\n| Describe the life-critical application (if present). | Not Applicable  |\n| Use Case Restrictions: | **GOVERNING TERMS**: Your use of this API is governed by the [NVIDIA API Trial Service Terms of Use](https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Fassets.ngc.nvidia.com%2Fproducts%2Fapi-catalog%2Flegal%2FNVIDIA%2520API%2520Trial%2520Terms%2520of%2520Service.pdf\u0026data=05%7C02%7Cjaydar%40nvidia.com%7C95bb810010dc4a66c6da08dde4f09e53%7C43083d15727340c1b7db39efd9ccc17a%7C0%7C0%7C638918444317044724%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C\u0026sdata=vvzmA%2Bo2tdifKPXG%2FGjgN4TPVdoUTsXD4%2BE%2B1M%2F7CWI%3D\u0026reserved=0); and the use of this model is governed by the [NVIDIA Agreements | Enterprise Software | NVIDIA Community Model License](https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.nvidia.com%2Fen-us%2Fagreements%2Fenterprise-software%2Fnvidia-community-models-license%2F\u0026data=05%7C02%7Cjaydar%40nvidia.com%7C95bb810010dc4a66c6da08dde4f09e53%7C43083d15727340c1b7db39efd9ccc17a%7C0%7C0%7C638918444317058237%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C\u0026sdata=c%2B1IgzlTBZaDGUkIFi08wYwzVcmWVYnCCBwztkCBRTI%3D\u0026reserved=0). |\n| Model and dataset restrictions: | The Principle of least privilege (PoLP) is applied limiting access for dataset generation and model development. Restrictions enforce dataset access during training, and dataset license constraints adhered to. |83:T68b,### Getting Started\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Instructions below demonstrate usage of \u003c%- name %\u003e model using Python gRPC client.\n\n### Prerequisites\n\nYou will need a system with Git and Python 3+ installed.\n\n### Install Riva Python Client\n\n```bash\npip install -U nvidia-riva-client\n```\n\n### Download Python Client\n\nDo"])</script><script>self.__next_f.push([1,"wnload Python client code by cloning \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e.\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\n### Run Python Client\n\nOpen a command terminal and execute below command to transcribe audio. Make sure you have a speech file in 16-bit Mono format in WAV/OGG/OPUS container. If you have generated the API key, it will be auto-populated in the command.\n\n```bash\npython python-clients/scripts/asr/transcribe_file.py \\\n    --server grpc.nvcf.nvidia.com:443 --use-ssl \\\n    --metadata function-id \"\u003c%- nvcfFunctionId %\u003e\" \\\n    --metadata \"authorization\" \"Bearer \u003c%- apiKey %\u003e\" \\\n    --language-code vi-VN \\\n    --input-file \u003cpath_to_audio_file\u003e\n```\n\n### Support for gRPC clients in other languages\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Proto files can be downloaded from \u003ca href=\"https://github.com/nvidia-riva/common/archive/refs/heads/main.zip\"\u003eRiva gRPC Proto files\u003c/a\u003e and compiled to target language using \u003ca href=\"https://grpc.io/docs/protoc-installation/\"\u003eProtoc compiler\u003c/a\u003e. Example Riva clients in C++ and Python languages are provided below.\n\n* \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e\n* \u003ca href=\"https://github.com/nvidia-riva/cpp-clients\"\u003eC++ Client Repository\u003c/a\u003e\n84:T714,## Step 1. Generate API Key\n\n::generate-api-key\n\n## Step 2. Pull and Run the NIM\n\n```bash\n$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: \u003cPASTE_API_KEY_HERE\u003e\n```\n\nRefer [Supported Models](https://docs.nvidia.com/nim/riva/asr/latest/getting-started.html#supported-models) for full list of models.\n\n```bash\nexport NGC_API_KEY=\u003cPASTE_API_KEY_HERE\u003e\n\ndocker run -it --rm --name=parakeet-ctc-0.6b-vi \\\n   --runtime=nvidia \\\n   --gpus '\"device=0\"' \\\n   --shm-size=8GB \\\n   -e NGC_API_KEY \\\n   -e NIM_HTTP_API_PORT=9000 \\\n   -e NIM_GRPC_API_PORT=50051 \\\n   -p 9000:9000 \\\n   -p 50051:50051 \\\n   -e NIM_TAGS_SELECTOR=mode=str,vad=silero,diarizer=sortformer \\\n   nvcr.io/nim/nvidia/parakeet-ctc-0.6b-vi:latest\n`"])</script><script>self.__next_f.push([1,"``\n\nIt may take a up to 30 minutes depending on your network speed, for the container to be ready and start accepting requests from the time the docker container is started.\n\n## Step 3. Test the NIM\n\nOpen a new terminal and run following command to check if the service is ready to handle inference requests\n\n```bash\ncurl -X 'GET' 'http://localhost:9000/v1/health/ready'\n```\n\nIf the service is ready, you get a response similar to the following.\n```bash\n{\"ready\":true}\n```\n\nInstall the Riva Python client package\n\n```bash\nsudo apt-get install python3-pip\npip install -U nvidia-riva-client\n```\n\nDownload Riva sample clients\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\nRun Speech to Text inference in streaming modes. Riva ASR supports Mono, 16-bit audio in WAV, OPUS and FLAC formats.\n\n```bash\npython3 python-clients/scripts/asr/transcribe_file.py --server 0.0.0.0:50051 --input-file \u003cpath_to_speech_file\u003e --language-code vi-VN\n```\n\n\nFor more details on getting started with this NIM, visit the [Riva ASR NIM Docs](https://docs.nvidia.com/nim/riva/asr/latest/overview.html).85:T419,Field                                                                                               |  Response\n:---------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------\nWhat is the language balance of the model validation data? | Mandarin Chinese: 49%, English: 51%\nWhat is the geographic origin language balance of the model validation data? | China: 47%, United States: 49%, Other: 4%\nWhat is the accent balance of the model validation data? | Mandarin Chinese: 49%, American English: 49%, Other Accented English: 2%\nParticipation considerations from adversely impacted groups ([protected classes](https://www.senate.ca.gov/content/protected-classes)) in model design and testing:  |  Age, Gender, Linguistic Background\nMeasures taken to mitigate against unwanted bias:                                       "])</script><script>self.__next_f.push([1,"            |  Used custom dataset to validate model performance across gender, age, and linguistic demographics86:Tec7,"])</script><script>self.__next_f.push([1,"# Speech Recognition: Parakeet CTC 0.6b Mandarin English Code Switch Model\n          \n## Description\n\nRIVA Parakeet-CTC-XL-0.6B-Unified ASR Mandarin (around 600M parameters) [1] is trained on ASR Set with over 17,000 hours of Mandarin (zh-CN) and English (en-US) speech. The model transcribes speech in Mandarin and English, in upper case and lower case alphabets, along with punctuations (period, comma, and question mark), spaces, and apostrophes. \n\nThis model is ready for commercial use.\n\n## License/Terms of Use \n\nGOVERNING TERMS: The use of this model is governed by the NVIDIA Community Model License (found at https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/).\n\n## References\n\n[1] [Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition](https://arxiv.org/abs/2305.05084) \u003cbr\u003e\n[2] [Fast-Conformer-CTC Model](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/models.html) \u003cbr\u003e\n[3] [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100) \u003cbr\u003e\n\n## Model Architecture\n\n**Architecture Type:** Parakeet-CTC (also known as FastConformer-CTC) [1], [2] which is an optimized version of Conformer model [3] with 8x depthwise-separable convolutional downsampling with CTC loss \u003cbr\u003e \n**Network Architecture:** Parakeet-CTC-XL-0.6B \u003cbr\u003e\n\n## Input \n\n**Input Type(s):** Audio \u003cbr\u003e\n**Input Format(s):** wav \u003cbr\u003e\n**Input Parameters:** 1-Dimension \u003cbr\u003e\n**Other Properties Related to Input:** Maximum Length in seconds specific to GPU Memory, No Pre-Processing Needed, Mono channel is required. \u003cbr\u003e\n\n## Output \n\n**Output Type(s):** Text \n**Output Format:** String (in Mandarin and English) \u003cbr\u003e\n**Output Parameters:** 1-Dimension \u003cbr\u003e\n**Other Properties Related to Output:** No Maximum Character Length, Does not handle special characters. \u003cbr\u003e\n\n\n**Supported Operating System(s):**  \u003cbr\u003e\n* Linux \u003cbr\u003e\n\n## Model Version(s): \n\nParakeet-CTC-XL-unified-0.6b_spe7k_zh-CN_3.0\n\n## Training \u0026 Evaluation \n\n### Training Dataset\n\n**Data Collection Method by dataset**\u003cbr\u003e\n\n* Human \u003cbr\u003e\n\n**Labeling Method by dataset** \u003cbr\u003e\n\n* Human \u003cbr\u003e\n\n**Properties:**  \u003cbr\u003e\n\nThis model is trained on over 17,000 hours of Mandarin (zh-CN) and English (en-US) speech, comprised of a dynamic blend of public and internal proprietary datasets normalized to have upper-cased, lower-cased, punctuated, and spoken forms in text.\n\n### Evaluation Dataset\n\n**Data Collection Method by dataset** \u003cbr\u003e\n\n* Human \u003cbr\u003e\n\n**Labeling Method by dataset** \u003cbr\u003e\n\n* Human \u003cbr\u003e\n\n**Properties:**\n\nA dynamic blend of public and internal proprietary datasets normalized to have upper-cased, lower-cased, punctuated, and spoken forms in text.\n\n## Inference\n\n**Engine:** Triton \u003cbr\u003e\n**Test Hardware:**  \u003cbr\u003e\n* NVIDIA A2 \u003cbr\u003e\n* NVIDIA A10 \u003cbr\u003e\n* NVIDIA A16 \u003cbr\u003e\n* NVIDIA A30 \u003cbr\u003e\n* NVIDIA A40 \u003cbr\u003e\n* NVIDIA A100 \u003cbr\u003e\n* NVIDIA H100 \u003cbr\u003e\n* NVIDIA L4 \u003cbr\u003e\n* NVIDIA L40 \u003cbr\u003e\n* GeForce RTX 40xx \u003cbr\u003e\n* GeForce RTX 50xx \u003cbr\u003e\n* Blackwell RTX 60xx \u003cbr\u003e\n\n\n## Ethical Considerations (For NVIDIA Models Only):\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety \u0026 Security, and Privacy Subcards.  Please report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/)."])</script><script>self.__next_f.push([1,"87:T9cd,"])</script><script>self.__next_f.push([1,"Field                                                                                                  |  Response\n:------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------\nIntended Applications \u0026 Domains:                                                                       |  Speech Transcription\nTypes:                                                                                                 |  Speech Transcription\nIntended Users:                                                                                        |  This model is intended for developers and data scientists building interactive call centers, virtual assistants, and language learning assistants\nOutput:                                                                                                |  Transcribed text with timestamps and confidence scores\nDescribe how the model works:                                                                          |  Model transcribes audio input into text for the input language\nName the adversely impacted groups this has been tested to deliver comparable outcomes regardless of:  |  Age, Gender, National Origin\nTechnical Limitations:                                                                                 |  Transcripts may not be 100% accurate. Accuracy varies based on the characteristics of input audio (Domain, Use Case, Accent, Noise, Speech Type, Context of speech, etc.)\nVerified to have met prescribed NVIDIA quality standards:                                                     |  Yes\nPerformance Metrics:                                                                                   |  Word Error Rate (WER), Silence Robustness (Characters/mins of silent audio), Latency (in milliseconds), Throughput (Total audio processed per unit of time)\nPotential Known Risks:                                                                                 |  Not recommended for word-for-word transcription as accuracy varies based on the characteristics of input audio (domain, use case, accent, noise, speech type, and context of speech)\nLicensing:                                                                                             |  [https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/)"])</script><script>self.__next_f.push([1,"88:T4d6,Field                                                                                                                              |  Response\n:----------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------\nGeneratable or reverse engineerable personally-identifiable information (PII)?                                                     |  None\nWas consent obtained for any personal data used?                                                                                   |  None\nHow often is dataset reviewed?                                                                                                     |  Before Release\nIs there data provenance for all datasets used in training?                                                                        |  Yes\nDoes data labeling (annotation, metadata) comply with privacy laws?                                                                |  Yes  \nIs data compliant with data subject requests for data correction or removal, if such a request was made?                           |  The data is compliant where applicable, but is not applicable for all data.89:T68b,### Getting Started\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Instructions below demonstrate usage of \u003c%- name %\u003e model using Python gRPC client.\n\n### Prerequisites\n\nYou will need a system with Git and Python 3+ installed.\n\n### Install Riva Python Client\n\n```bash\npip install -U nvidia-riva-client\n```\n\n### Download Python Client\n\nDownload Python client code by cloning \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e.\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\n### Run Python Client\n\nOpen a command terminal and execute below command to transcribe audio. Make sure you have a speech file in 16-bit Mono format in WAV/OGG/OPUS container. If you have generated the API key, it will be auto-populated in the command.\n"])</script><script>self.__next_f.push([1,"\n```bash\npython python-clients/scripts/asr/transcribe_file.py \\\n    --server grpc.nvcf.nvidia.com:443 --use-ssl \\\n    --metadata function-id \"\u003c%- nvcfFunctionId %\u003e\" \\\n    --metadata \"authorization\" \"Bearer \u003c%- apiKey %\u003e\" \\\n    --language-code zh-CN \\\n    --input-file \u003cpath_to_audio_file\u003e\n```\n\n### Support for gRPC clients in other languages\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Proto files can be downloaded from \u003ca href=\"https://github.com/nvidia-riva/common/archive/refs/heads/main.zip\"\u003eRiva gRPC Proto files\u003c/a\u003e and compiled to target language using \u003ca href=\"https://grpc.io/docs/protoc-installation/\"\u003eProtoc compiler\u003c/a\u003e. Example Riva clients in C++ and Python languages are provided below.\n\n* \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e\n* \u003ca href=\"https://github.com/nvidia-riva/cpp-clients\"\u003eC++ Client Repository\u003c/a\u003e\n8a:T71a,## Step 1. Generate API Key\n\n::generate-api-key\n\n## Step 2. Pull and Run the NIM\n\n```bash\n$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: \u003cPASTE_API_KEY_HERE\u003e\n```\n\nRefer [Supported Models](https://docs.nvidia.com/nim/riva/asr/latest/getting-started.html#supported-models) for full list of models.\n\n```bash\nexport NGC_API_KEY=\u003cPASTE_API_KEY_HERE\u003e\n\ndocker run -it --rm --name=parakeet-ctc-0.6b-zh-cn \\\n   --runtime=nvidia \\\n   --gpus '\"device=0\"' \\\n   --shm-size=8GB \\\n   -e NGC_API_KEY \\\n   -e NIM_HTTP_API_PORT=9000 \\\n   -e NIM_GRPC_API_PORT=50051 \\\n   -p 9000:9000 \\\n   -p 50051:50051 \\\n   -e NIM_TAGS_SELECTOR=mode=str,vad=silero,diarizer=sortformer \\\n   nvcr.io/nim/nvidia/parakeet-ctc-0.6b-zh-cn:latest\n```\n\nIt may take a up to 30 minutes depending on your network speed, for the container to be ready and start accepting requests from the time the docker container is started.\n\n## Step 3. Test the NIM\n\nOpen a new terminal and run following command to check if the service is ready to handle inference requests\n\n```bash\ncurl -X 'GET' 'http://localhost:9000/v1/health/ready'\n```\n\nIf the service is ready, you get a response similar to the following"])</script><script>self.__next_f.push([1,".\n```bash\n{\"ready\":true}\n```\n\nInstall the Riva Python client package\n\n```bash\nsudo apt-get install python3-pip\npip install -U nvidia-riva-client\n```\n\nDownload Riva sample clients\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\nRun Speech to Text inference in streaming modes. Riva ASR supports Mono, 16-bit audio in WAV, OPUS and FLAC formats.\n\n```bash\npython3 python-clients/scripts/asr/transcribe_file.py --server 0.0.0.0:50051 --input-file \u003cpath_to_speech_file\u003e --language-code zh-CN\n```\n\n\nFor more details on getting started with this NIM, visit the [Riva ASR NIM Docs](https://docs.nvidia.com/nim/riva/asr/latest/overview.html).8b:T43c,Field                                                                                               |  Response\n:---------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------\nWhat is the language balance of the model validation data? | Spanish: 50%, English 50%\nWhat is the geographic origin language balance of the model validation data? | America: 65%, Europe: 30%, Other: 5%\nWhat is the accent balance of the model validation data? | Latin American Spanish: 30%, European Spanish: 20%, American English: 40%, British English: 5%, Other Accented English: 5%\nParticipation considerations from adversely impacted groups ([protected classes](https://www.senate.ca.gov/content/protected-classes)) in model design and testing:  |  Age, Gender, Linguistic Background\nMeasures taken to mitigate against unwanted bias:                                                   |  Used custom dataset to validate model performance across gender, age, and linguistic demographics8c:Tea9,"])</script><script>self.__next_f.push([1,"# Speech Recognition: Parakeet CTC 0.6b Spanish English Code Switch Model\n\n## Description\nRIVA Parakeet-CTC-XL-0.6B-Unified ASR Spanish-English (around 600M parameters) [1] is trained on ASR Set with over 28,000 hours of Spanish (es-US) and English (en-US) speech. The model transcribes speech in Spanish and English, in upper case and lower case alphabets, along with punctuations (period, comma, and question mark), spaces, and apostrophes.\n\nThis model is ready for commercial use.\n\n## Terms of use\nGOVERNING TERMS: This trial is governed by the NVIDIA API Trial Terms of Service (found at https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf). \n\n## References\n* [Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition](https://arxiv.org/abs/2305.05084)\n* [Fast-Conformer-CTC Model](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/models.html)\n* [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100)\n\n## Model Architecture\n**Architecture Type:** Parakeet-CTC (also known as FastConformer-CTC) [1], [2] which is an optimized version of Conformer model [3] with 8x depthwise-separable convolutional downsampling with CTC loss \u003cbr\u003e\n**Network Architecture:** Parakeet-CTC-XXL-0.6B \u003cbr\u003e\n\n## Input\n**Input Type(s):** Audio \n**Input Format(s):** wav \n**Input Parameters:** 1-Dimension \n**Other Properties Related to Input:** Maximum Length in seconds specific to GPU Memory, No Pre-Processing Needed, Mono channel is required\n\n## Output\n**Output Type(s):** Transcription for what was spoken in input speech \u003cbr\u003e\n**Output Type(s):** Text \n**Output Format:** String (in Spanish and English) \n**Output Parameters:** 1-Dimension \n**Other Properties Related to Output:** No Maximum Character Length, Does not handle special characters\n\n**Supported Operating System(s):**  \u003cbr\u003e\n* Linux \u003cbr\u003e\n\n## Model Version\nParakeet-CTC-XL-unified-0.6b_spe1024_es-en-US_3.0 \u003cbr\u003e\n\n## Training \u0026 Evaluation\n\n### Training Dataset\n\n**Data Collection Method by dataset**\u003cbr\u003e\n\n* Human \u003cbr\u003e\n\n**Labeling Method by dataset** \u003cbr\u003e\n\n* Human \u003cbr\u003e\n\n**Properties:** \n\nThis model is trained on over 28,000 hours of Spanish (es-US) and English (en-US) speech, comprised of a dynamic blend of public and internal proprietary and customer datasets normalized to have upper-cased, lower-cased, punctuated, and spoken forms in text.\n\n### Evaluation Dataset\n\n**Data Collection Method by dataset** \u003cbr\u003e\n\n* Human \u003cbr\u003e\n\n**Labeling Method by dataset** \u003cbr\u003e\n\n* Human \u003cbr\u003e\n\n**Properties:**\nA dynamic blend of public and internal proprietary and customer datasets normalized to have upper-cased, lower-cased, punctuated, and spoken forms in text.\n\n## Inference\n**Engine:** Triton \u003cbr\u003e\n**Test Hardware:**  \u003cbr\u003e\n* NVIDIA A10 \u003cbr\u003e\n* NVIDIA A100 \u003cbr\u003e\n* NVIDIA A30 \u003cbr\u003e\n* NVIDIA H100 \u003cbr\u003e\n* NVIDIA Jetson Orin \u003cbr\u003e\n* NVIDIA L4 \u003cbr\u003e\n* NVIDIA L40 \u003cbr\u003e\n* NVIDIA Turing T4 \u003cbr\u003e\n* NVIDIA Volta V100 \u003cbr\u003e\n\n\n## Ethical Considerations (For NVIDIA Models Only):\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety \u0026 Security, and Privacy Subcards.  Please report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/)."])</script><script>self.__next_f.push([1,"8d:T996,"])</script><script>self.__next_f.push([1,"Field                                                                                                  |  Response\n:------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------\nIntended Applications \u0026 Domains:                                                                       |  Speech Transcription\nTypes:                                                                                                 |  Speech Transcription\nIntended Users:                                                                                        |  This model is intended for developers and data scientists building interactive call centers, virtual assistants, language learning assistants.\nOutput:                                                                                                |  Transcribed text with timestamps and confidence scores\nDescribe how the model works:                                                                          |  Model transcribes audio input into text for the input language\nName the adversely impacted groups this has been tested to deliver comparable outcomes regardless of:  |  Age, Gender, National Origin\nTechnical Limitations:                                                                                 |  Transcripts may not be 100% accurate. Accuracy varies based on the characteristics of input audio (Domain, Use Case, Accent, Noise, Speech Type, Context of speech, etc.)\nVerified to have met prescribed NVIDIA quality standards:                                                     |  Yes\nPerformance Metrics:                                                                                   |  Word Error Rate (WER), Silence Robustness (Characters/mins of silent audio), Latency (in milliseconds), Throughput (Total audio processed per unit of time)\nPotential Known Risks:                                                                                 |  Not recommended for word-for-word transcription as accuracy varies based on the characteristics of input audio (domain, use case, accent, noise, speech type, and context of speech)\nLicensing:                                                                                             |  [https://docs.nvidia.com/ai-foundation-models-community-license.pdf](https://docs.nvidia.com/ai-foundation-models-community-license.pdf)"])</script><script>self.__next_f.push([1,"8e:T82f,"])</script><script>self.__next_f.push([1,"Field                                                                                                                              |  Response\n:----------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------\nGeneratable or reverse engineerable personally-identifiable information (PII)?                                                     |  None\nWas consent obtained for any personal data used?                                                                                   |  Not Applicable\nProtected class data used to create this model?                                                                                    |  Age, Gender, Linguistic Background, National Origin\nHow often is dataset reviewed?                                                                                                     |  Before Release\nIs a mechanism in place to honor data subject right of access or deletion of personal data?                                        |  Not Applicable\nIf Personal data collected for the development of the model, was it collected directly by NVIDIA?                                  |  Not Applicable\nIf Personal data collected for the development of the model by NVIDIA, do you maintain or have access to disclosures made to data subjects?  |  Not Applicable\nIf Personal data collected for the development of this AI model, was it minimized to only what was required?                                 |  Not Applicable\nIs there provenance for all datasets used in training?                                                                                                                          |  Yes\nDoes data labeling (annotation, metadata) comply with privacy laws?                                                                |  Yes  \nIs data compliant with data subject requests for data correction or removal, if such a request was made?                           |  The data is compliant where applicable, but is not applicable for all data."])</script><script>self.__next_f.push([1,"8f:T68b,### Getting Started\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Instructions below demonstrate usage of \u003c%- name %\u003e model using Python gRPC client.\n\n### Prerequisites\n\nYou will need a system with Git and Python 3+ installed.\n\n### Install Riva Python Client\n\n```bash\npip install -U nvidia-riva-client\n```\n\n### Download Python Client\n\nDownload Python client code by cloning \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e.\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\n### Run Python Client\n\nOpen a command terminal and execute below command to transcribe audio. Make sure you have a speech file in 16-bit Mono format in WAV/OGG/OPUS container. If you have generated the API key, it will be auto-populated in the command.\n\n```bash\npython python-clients/scripts/asr/transcribe_file.py \\\n    --server grpc.nvcf.nvidia.com:443 --use-ssl \\\n    --metadata function-id \"\u003c%- nvcfFunctionId %\u003e\" \\\n    --metadata \"authorization\" \"Bearer \u003c%- apiKey %\u003e\" \\\n    --language-code es-US \\\n    --input-file \u003cpath_to_audio_file\u003e\n```\n\n### Support for gRPC clients in other languages\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Proto files can be downloaded from \u003ca href=\"https://github.com/nvidia-riva/common/archive/refs/heads/main.zip\"\u003eRiva gRPC Proto files\u003c/a\u003e and compiled to target language using \u003ca href=\"https://grpc.io/docs/protoc-installation/\"\u003eProtoc compiler\u003c/a\u003e. Example Riva clients in C++ and Python languages are provided below.\n\n* \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e\n* \u003ca href=\"https://github.com/nvidia-riva/cpp-clients\"\u003eC++ Client Repository\u003c/a\u003e\n90:T714,## Step 1. Generate API Key\n\n::generate-api-key\n\n## Step 2. Pull and Run the NIM\n\n```bash\n$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: \u003cPASTE_API_KEY_HERE\u003e\n```\n\nRefer [Supported Models](https://docs.nvidia.com/nim/riva/asr/latest/getting-started.html#supported-models) for full list of models.\n\n```bash\nexport NGC_API_KEY=\u003cPASTE_API_KEY_HERE\u003e\n\ndoc"])</script><script>self.__next_f.push([1,"ker run -it --rm --name=parakeet-ctc-0.6b-es \\\n   --runtime=nvidia \\\n   --gpus '\"device=0\"' \\\n   --shm-size=8GB \\\n   -e NGC_API_KEY \\\n   -e NIM_HTTP_API_PORT=9000 \\\n   -e NIM_GRPC_API_PORT=50051 \\\n   -p 9000:9000 \\\n   -p 50051:50051 \\\n   -e NIM_TAGS_SELECTOR=mode=str,vad=silero,diarizer=sortformer \\\n   nvcr.io/nim/nvidia/parakeet-ctc-0.6b-es:latest\n```\n\nIt may take a up to 30 minutes depending on your network speed, for the container to be ready and start accepting requests from the time the docker container is started.\n\n## Step 3. Test the NIM\n\nOpen a new terminal and run following command to check if the service is ready to handle inference requests\n\n```bash\ncurl -X 'GET' 'http://localhost:9000/v1/health/ready'\n```\n\nIf the service is ready, you get a response similar to the following.\n```bash\n{\"ready\":true}\n```\n\nInstall the Riva Python client package\n\n```bash\nsudo apt-get install python3-pip\npip install -U nvidia-riva-client\n```\n\nDownload Riva sample clients\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\nRun Speech to Text inference in streaming modes. Riva ASR supports Mono, 16-bit audio in WAV, OPUS and FLAC formats.\n\n```bash\npython3 python-clients/scripts/asr/transcribe_file.py --server 0.0.0.0:50051 --input-file \u003cpath_to_speech_file\u003e --language-code es-US\n```\n\n\nFor more details on getting started with this NIM, visit the [Riva ASR NIM Docs](https://docs.nvidia.com/nim/riva/asr/latest/overview.html).91:{\"name\":\"parakeet-ctc-0_6b-zh-tw\",\"type\":\"model\"}\n92:{\"name\":\"parakeet-tdt-0_6b-v2\",\"type\":\"model\"}\n93:{\"name\":\"parakeet-ctc-1_1b-asr\",\"type\":\"model\"}\n94:{\"name\":\"parakeet-ctc-0_6b-asr\",\"type\":\"model\"}\n95:{\"name\":\"canary-1b-asr\",\"type\":\"model\"}\n96:{\"name\":\"parakeet-1_1b-rnnt-multilingual-asr\",\"type\":\"model\"}\n97:{\"name\":\"conformer-ctc-asr\",\"type\":\"model\"}\n98:{\"name\":\"whisper-large-v3\",\"type\":\"model\"}\n99:{\"name\":\"canary-0-6b-turbo-asr\",\"type\":\"model\"}\n9a:{\"name\":\"parakeet-ctc-0_6b-vi\",\"type\":\"model\"}\n9b:{\"name\":\"parakeet-ctc-0_6b-zh-cn\",\"type\":\"model\"}\n9c:{\"name\":\"parakeet-ctc-0_6b-es\""])</script><script>self.__next_f.push([1,",\"type\":\"model\"}\n9d:{\"category\":\"speech\"}\n"])</script><script>self.__next_f.push([1,"33:[\"$\",\"$L4e\",null,{\"data\":[{\"endpoint\":{\"artifact\":{\"artifactType\":\"ENDPOINT\",\"name\":\"parakeet-ctc-0_6b-zh-tw\",\"displayName\":\"parakeet-ctc-0.6b-zh-tw\",\"publisher\":\"nvidia\",\"shortDescription\":\"Record-setting accuracy and performance for Mandarin Taiwanese English transcriptions.\",\"logo\":\"https://assets.ngc.nvidia.com/products/api-catalog/images/parakeet-ctc-0_6b-zh-tw.jpg\",\"labels\":[\"ASR\",\"NVIDIA NIM\",\"Streaming\",\"Taiwanese\",\"Speech-to-Text\"],\"attributes\":[{\"key\":\"AVAILABLE\",\"value\":\"true\"},{\"key\":\"PREVIEW\",\"value\":\"false\"}],\"updatedDate\":\"2025-10-16T01:36:31.358Z\",\"bias\":\"Field                                                                                               |  Response\\n:---------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------\\nWhat is the language balance of the model validation data? | Taiwanese Mandarin Chinese: 100%\\nWhat is the geographic origin language balance of the model validation data? | Taiwan: 100%\\nWhat is the accent balance of the model validation data? | Taiwanese Mandarin Chinese: 100%\\nParticipation considerations from adversely impacted groups ([protected classes](https://www.senate.ca.gov/content/protected-classes)) in model design and testing:  |  Age, Gender, Linguistic Background\\nMeasures taken to mitigate against unwanted bias:                                                   |  Used custom dataset to validate model performance across gender, age, and linguistic demographics\",\"canGuestDownload\":true,\"createdDate\":\"2025-10-16T01:36:31.358Z\",\"description\":\"$5b\",\"explainability\":\"$5c\",\"isPublic\":true,\"isReadOnly\":true,\"orgName\":\"qc69jvmznzxy\",\"privacy\":\"$5d\",\"safetyAndSecurity\":\"Field                                               |  Response\\n:---------------------------------------------------|:----------------------------------\\nModel Application Field(s):                               |  Speech Transcription\\nDescribe the life-critical impacts (if present).   |  Not Applicable\\nUse Case Restriction(s):                              |  Abide by [https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/)\\nModel and dataset restrictions:   | The Principle of Least Privilege (PoLP) is applied limiting access for dataset generation and model development. Restrictions enforce dataset access during training and dataset license constraints adhered to.\"},\"requestStatus\":{\"statusCode\":\"SUCCESS\",\"requestId\":\"4aad7977-e11b-421f-8c83-4c1e597a7d0a\"}},\"spec\":{\"namespace\":\"qc69jvmznzxy\",\"nvcfFunctionId\":\"8473f56d-51ef-473c-bb26-efd4f5def2bf\",\"createdDate\":\"2025-10-16T01:36:31.909Z\",\"attributes\":{\"apiDocsUrl\":\"https://docs.nvidia.com/nim/riva/asr/latest/protos.html\",\"termsOfUse\":\"\u003cb\u003eGOVERNING TERMS\u003c/b\u003e: Your use of this API is governed by the \u003ca href=\\\"https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA API Trial Service Terms of Use\u003c/a\u003e; and the use of this model is governed by the \u003ca href=\\\"https://docs.nvidia.com/ai-foundation-models-community-license.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA AI Foundation Models Community License\u003c/a\u003e.\\n\",\"showUnavailableBanner\":false,\"cta\":{\"text\":\"Run Anywhere - Notify Me\",\"url\":\"https://www.nvidia.com/en-us/ai/nim-notifyme/\"},\"usage\":\"$5e\",\"deploy\":[{\"label\":\"Linux with Docker\",\"filename\":\"linux.md\",\"contents\":\"$5f\"}]},\"artifactName\":\"parakeet-ctc-0_6b-zh-tw\"},\"config\":{\"name\":\"parakeet-ctc-0_6b-zh-tw\",\"type\":\"model\"}},{\"endpoint\":{\"artifact\":{\"artifactType\":\"ENDPOINT\",\"name\":\"parakeet-tdt-0_6b-v2\",\"displayName\":\"parakeet-tdt-0.6b-v2\",\"publisher\":\"nvidia\",\"shortDescription\":\"Accurate and optimized English transcriptions with punctuation and word timestamps\",\"logo\":\"https://assets.ngc.nvidia.com/products/api-catalog/images/parakeet-tdt-0_6b-v2.jpg\",\"labels\":[\"ASR\",\"English\",\"NVIDIA NIM\",\"NVIDIA Riva\",\"speech-to-text\"],\"attributes\":[{\"key\":\"AVAILABLE\",\"value\":\"true\"},{\"key\":\"PREVIEW\",\"value\":\"false\"}],\"updatedDate\":\"2025-07-30T19:12:46.669Z\",\"bias\":\"Field                                                                                               |  Response\\n:---------------------------------------------------------------------------------------------------|:---------------:\\nParticipation considerations from adversely impacted groups [protected classes](https://www.senate.ca.gov/content/protected-classes) in model design and testing  |  None\\nMeasures taken to mitigate against unwanted bias                                                   |  None\",\"canGuestDownload\":true,\"createdDate\":\"2025-07-30T19:12:46.669Z\",\"description\":\"$60\",\"explainability\":\"$61\",\"isPublic\":true,\"isReadOnly\":true,\"orgName\":\"qc69jvmznzxy\",\"privacy\":\"$62\",\"safetyAndSecurity\":\"Field                                               |  Response\\n:---------------------------------------------------:|:----------------------------------\\nModel Application(s)                               |  Speech to Text Transcription\\nDescribe the life critical impact   |  None\\nUse Case Restrictions                              | Abide by [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode.en) License\\nModel and dataset restrictions            |  The Principle of least privilege (PoLP) is applied limiting access for dataset generation and model development. Restrictions enforce dataset access during training, and dataset license constraints adhered to.\"},\"requestStatus\":{\"statusCode\":\"SUCCESS\",\"requestId\":\"9ecaab3c-7de2-4fef-8568-eaa15613010f\"}},\"spec\":{\"namespace\":\"qc69jvmznzxy\",\"updatedDate\":\"2025-07-31T07:26:14.239Z\",\"nvcfFunctionId\":\"d3fe9151-442b-4204-a70d-5fcc597fd610\",\"createdDate\":\"2025-07-30T19:12:46.961Z\",\"attributes\":{\"apiDocsUrl\":\"https://docs.nvidia.com/nim/riva/asr/latest/protos.html\",\"termsOfUse\":\"\u003cb\u003eGOVERNING TERMS\u003c/b\u003e: Your use of this API is governed by the \u003ca href=\\\"https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA API Trial Service Terms of Use\u003c/a\u003e; and the use of this model is governed by the \u003ca href=\\\"https://docs.nvidia.com/ai-foundation-models-community-license.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA AI Foundation Models Community License\u003c/a\u003e.\\n\",\"showUnavailableBanner\":false,\"cta\":{\"text\":\"Run Anywhere - Notify Me\",\"url\":\"https://www.nvidia.com/en-us/ai/nim-notifyme/\",\"nim_available_override_url\":\"https://catalog.ngc.nvidia.com/orgs/nim/teams/nvidia/containers/parakeet-tdt-0.6b-v2\"},\"usage\":\"$63\",\"deploy\":[{\"label\":\"Linux with Docker\",\"filename\":\"linux.md\",\"contents\":\"$64\"}],\"leaderboard\":[{\"name\":\"Throughput HF ASR Leaderboard\",\"url\":\"https://huggingface.co/spaces/hf-audio/open_asr_leaderboard\",\"position\":1}]},\"artifactName\":\"parakeet-tdt-0_6b-v2\"},\"config\":{\"name\":\"parakeet-tdt-0_6b-v2\",\"type\":\"model\"}},{\"endpoint\":{\"artifact\":{\"artifactType\":\"ENDPOINT\",\"name\":\"parakeet-ctc-1_1b-asr\",\"displayName\":\"parakeet-ctc-1.1b-asr\",\"publisher\":\"nvidia\",\"shortDescription\":\"Record-setting accuracy and performance for English transcription.\",\"logo\":\"https://assets.ngc.nvidia.com/products/api-catalog/images/parakeet-ctc-1_1b-asr.jpg\",\"labels\":[\"ASR\",\"English\",\"NVIDIA NIM\",\"Streaming\",\"batch\",\"Speech-to-Text\"],\"attributes\":[{\"key\":\"AVAILABLE\",\"value\":\"true\"},{\"key\":\"PREVIEW\",\"value\":\"false\"}],\"updatedDate\":\"2025-06-26T20:47:19.082Z\",\"bias\":\"Field                                                                                               |  Response\\n:---------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------\\nParticipation considerations from adversely impacted groups ([protected classes](https://www.senate.ca.gov/content/protected-classes)) in model design and testing:  |  Age, Gender, Linguistic Background\\nMeasures taken to mitigate against unwanted bias:                                                   |  Used custom dataset to validate model performance across gender, age, and linguistic demographics\",\"canGuestDownload\":true,\"createdDate\":\"2024-08-06T06:27:13.068Z\",\"description\":\"$65\",\"explainability\":\"$66\",\"isPublic\":true,\"isReadOnly\":true,\"orgName\":\"qc69jvmznzxy\",\"privacy\":\"$67\",\"safetyAndSecurity\":\"Field                                               |  Response\\n:---------------------------------------------------|:----------------------------------\\nModel Application(s):                               |  Speech Transcription\\nDescribe the life-critical impacts (if present).   |  Not Applicable for Licensed Use Cases per [NVIDIA RIVA License Agreement](https://developer.nvidia.com/riva/ga/license)\\nUse Case Restriction(s):                              |  Abide by https://developer.nvidia.com/riva/ga/license\\nModel and Dataset Restriction(s):                       |  Dataset access restrictions\\nDescribe access restrictions (if any):   | The Principle of Least Privilege (PoLP) is applied limiting access for dataset generation and model development. Restrictions enforce dataset access during training and dataset license constraints adhered to.\"},\"requestStatus\":{\"statusCode\":\"SUCCESS\",\"requestId\":\"554222b6-94de-4c8c-b98c-7808952c9176\"}},\"spec\":{\"namespace\":\"qc69jvmznzxy\",\"updatedDate\":\"2025-12-23T05:50:48.694Z\",\"nvcfFunctionId\":\"1598d209-5e27-4d3c-8079-4751568b1081\",\"createdDate\":\"2024-08-06T06:27:13.397Z\",\"attributes\":{\"apiDocsUrl\":\"https://docs.nvidia.com/nim/riva/asr/latest/protos.html\",\"termsOfUse\":\"\u003cb\u003eGOVERNING TERMS\u003c/b\u003e: Your use of this API is governed by the \u003ca href=\\\"https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA API Trial Service Terms of Use\u003c/a\u003e; and the use of this model is governed by the \u003ca href=\\\"https://docs.nvidia.com/ai-foundation-models-community-license.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA AI Foundation Models Community License\u003c/a\u003e.\\n\",\"showUnavailableBanner\":false,\"cta\":{\"text\":\"Run Anywhere - Notify Me\",\"url\":\"https://www.nvidia.com/en-us/ai/nim-notifyme/\"},\"usage\":\"$68\",\"deploy\":[{\"label\":\"Linux with Docker\",\"filename\":\"linux.md\",\"contents\":\"$69\"}]},\"artifactName\":\"parakeet-ctc-1_1b-asr\"},\"config\":{\"name\":\"parakeet-ctc-1_1b-asr\",\"type\":\"model\"}},{\"endpoint\":{\"artifact\":{\"artifactType\":\"ENDPOINT\",\"name\":\"parakeet-ctc-0_6b-asr\",\"displayName\":\"parakeet-ctc-0.6b-asr\",\"publisher\":\"nvidia\",\"shortDescription\":\"State-of-the-art accuracy and speed for English transcriptions.\",\"logo\":\"https://assets.ngc.nvidia.com/products/api-catalog/images/parakeet-ctc-0_6b-asr.jpg\",\"labels\":[\"ASR\",\"Batch\",\"English\",\"Fast\",\"NVIDIA NIM\",\"Run-on-RTX\",\"Streaming\",\"Speech-to-Text\"],\"attributes\":[{\"key\":\"AVAILABLE\",\"value\":\"true\"},{\"key\":\"PREVIEW\",\"value\":\"false\"}],\"updatedDate\":\"2025-06-13T14:13:00.782Z\",\"bias\":\"Field                                                                                               |  Response\\n:---------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------\\nWhat is the language balance of the model validation data? | English: 100%\\nWhat is the geographic origin language balance of the model validation data? | United States: 80%, United Kingdom: 10%, Others: 10%\\nWhat is the accent balance of the model validation data? | American English: 80%, British English: 10%, Others: 10%\\nParticipation considerations from adversely impacted groups ([protected classes](https://www.senate.ca.gov/content/protected-classes)) in model design and testing:  |  Age, Gender, Linguistic Background\\nMeasures taken to mitigate against unwanted bias:                                                   |  Used custom dataset to validate model performance across gender, age, and linguistic demographics\",\"canGuestDownload\":true,\"createdDate\":\"2024-08-06T06:27:13.040Z\",\"description\":\"$6a\",\"explainability\":\"$6b\",\"isPublic\":true,\"isReadOnly\":true,\"orgName\":\"qc69jvmznzxy\",\"privacy\":\"$6c\",\"safetyAndSecurity\":\"Field                                               |  Response\\n:---------------------------------------------------|:----------------------------------\\nModel Application(s):                               |  Speech Transcription\\nDescribe the life-critical impacts (if present).   |  Not Applicable\\nUse Case Restriction(s):                              |  Abide by https://developer.nvidia.com/riva/ga/license\\nDescribe access restrictions (if any):   | The Principle of Least Privilege (PoLP) is applied limiting access for dataset generation and model development. Restrictions enforce dataset access during training and dataset license constraints adhered to.\"},\"requestStatus\":{\"statusCode\":\"SUCCESS\",\"requestId\":\"f734ac7c-2216-4d43-896d-a420e483bddc\"}},\"spec\":{\"namespace\":\"qc69jvmznzxy\",\"updatedDate\":\"2025-09-10T15:00:18.599Z\",\"nvcfFunctionId\":\"d8dd4e9b-fbf5-4fb0-9dba-8cf436c8d965\",\"createdDate\":\"2024-08-06T06:27:13.383Z\",\"attributes\":{\"apiDocsUrl\":\"https://docs.nvidia.com/nim/riva/asr/latest/protos.html\",\"termsOfUse\":\"\u003cb\u003eGOVERNING TERMS\u003c/b\u003e: Your use of this API is governed by the \u003ca href=\\\"https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA API Trial Service Terms of Use\u003c/a\u003e; and the use of this model is governed by the \u003ca href=\\\"https://docs.nvidia.com/ai-foundation-models-community-license.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA AI Foundation Models Community License\u003c/a\u003e.\\n\",\"showUnavailableBanner\":false,\"cta\":{\"text\":\"Run Anywhere - Notify Me\",\"url\":\"https://www.nvidia.com/en-us/ai/nim-notifyme/\",\"nim_available_override_url\":\"https://catalog.ngc.nvidia.com/orgs/nim/teams/nvidia/containers/parakeet-0-6b-ctc-en-us\"},\"usage\":\"$6d\",\"deploy\":[{\"label\":\"Linux with Docker\",\"filename\":\"linux.md\",\"contents\":\"$6e\"},{\"label\":\"Windows on RTX AI PCs (Beta)\",\"filename\":\"wsl2.md\",\"contents\":\"$6f\"}]},\"artifactName\":\"parakeet-ctc-0_6b-asr\"},\"config\":{\"name\":\"parakeet-ctc-0_6b-asr\",\"type\":\"model\"}},{\"endpoint\":{\"artifact\":{\"artifactType\":\"ENDPOINT\",\"name\":\"canary-1b-asr\",\"displayName\":\"canary-1b-asr\",\"publisher\":\"nvidia\",\"shortDescription\":\"Multi-lingual model supporting speech-to-text recognition and translation.\",\"logo\":\"https://assets.ngc.nvidia.com/products/api-catalog/images/canary-1b-asr.jpg\",\"labels\":[\"Automatic Speech Recognition\",\"Automatic Speech Translation\",\"NVIDIA NIM\",\"NVIDIA Riva\"],\"attributes\":[{\"key\":\"AVAILABLE\",\"value\":\"true\"},{\"key\":\"PREVIEW\",\"value\":\"false\"}],\"updatedDate\":\"2025-04-10T15:18:38.670Z\",\"bias\":\"$70\",\"canGuestDownload\":true,\"createdDate\":\"2025-02-18T18:49:02.046Z\",\"description\":\"$71\",\"explainability\":\"$72\",\"isPublic\":true,\"isReadOnly\":true,\"orgName\":\"qc69jvmznzxy\",\"privacy\":\"$73\",\"safetyAndSecurity\":\"$74\"},\"requestStatus\":{\"statusCode\":\"SUCCESS\",\"requestId\":\"cbff7125-a960-494d-a74d-27d03554f19c\"}},\"spec\":{\"namespace\":\"qc69jvmznzxy\",\"updatedDate\":\"2025-10-29T07:01:04.531Z\",\"nvcfFunctionId\":\"b0e8b4a5-217c-40b7-9b96-17d84e666317\",\"createdDate\":\"2025-02-18T18:49:02.357Z\",\"attributes\":{\"apiDocsUrl\":\"https://docs.nvidia.com/nim/riva/asr/latest/protos.html\",\"termsOfUse\":\"\u003cb\u003eGOVERNING TERMS\u003c/b\u003e: Your use of this API is governed by the \u003ca href=\\\"https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA API Trial Service Terms of Use\u003c/a\u003e; and the use of this model is governed by the \u003ca href=\\\"https://docs.nvidia.com/ai-foundation-models-community-license.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA AI Foundation Models Community License\u003c/a\u003e.\\n\",\"showUnavailableBanner\":false,\"cta\":{\"text\":\"Run Anywhere - Notify Me\",\"url\":\"https://www.nvidia.com/en-us/ai/nim-notifyme/\",\"nim_available_override_url\":\"https://catalog.ngc.nvidia.com/orgs/nim/teams/nvidia/containers/canary-1b\"},\"usage\":\"$75\",\"deploy\":[{\"label\":\"Linux with Docker\",\"filename\":\"linux.md\",\"contents\":\"$76\"}]},\"artifactName\":\"canary-1b-asr\"},\"config\":{\"name\":\"canary-1b-asr\",\"type\":\"model\"}},{\"endpoint\":{\"artifact\":{\"artifactType\":\"ENDPOINT\",\"name\":\"parakeet-1_1b-rnnt-multilingual-asr\",\"displayName\":\"parakeet-1.1b-rnnt-multilingual-asr\",\"publisher\":\"nvidia\",\"shortDescription\":\"High accuracy and optimized performance for transcription in 25 languages\",\"logo\":\"https://assets.ngc.nvidia.com/products/api-catalog/images/parakeet-1_1b-rnnt-multilingual-asr.jpg\",\"labels\":[\"Automatic Speech Recognition\",\"NVIDIA NIM\",\"NVIDIA Riva\",\"Speech-to-Text\"],\"attributes\":[{\"key\":\"AVAILABLE\",\"value\":\"true\"},{\"key\":\"PREVIEW\",\"value\":\"false\"}],\"updatedDate\":\"2025-04-30T17:29:30.445Z\",\"bias\":\"$77\",\"canGuestDownload\":true,\"createdDate\":\"2025-04-30T17:29:30.445Z\",\"description\":\"$78\",\"explainability\":\"$79\",\"isPublic\":true,\"isReadOnly\":true,\"orgName\":\"qc69jvmznzxy\",\"privacy\":\"$7a\",\"safetyAndSecurity\":\"Field                                               |  Response\\n:---------------------------------------------------|:----------------------------------\\nModel Application(s):                               |  Speech Transcription\\nDescribe the life-critical impacts (if present).   |  Not Applicable\\nUse Case Restriction(s):                              |  Abide by [https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/)\\nDescribe access restrictions (if any):   | The Principle of Least Privilege (PoLP) is applied limiting access for dataset generation and model development. Restrictions enforce dataset access during training and dataset license constraints adhered to.\"},\"requestStatus\":{\"statusCode\":\"SUCCESS\",\"requestId\":\"50c19cce-6860-4b79-be6f-f23d56efa47c\"}},\"spec\":{\"namespace\":\"qc69jvmznzxy\",\"updatedDate\":\"2025-12-23T05:50:49.470Z\",\"nvcfFunctionId\":\"71203149-d3b7-4460-8231-1be2543a1fca\",\"createdDate\":\"2025-04-30T17:29:30.749Z\",\"attributes\":{\"apiDocsUrl\":\"https://docs.nvidia.com/nim/riva/asr/latest/protos.html\",\"termsOfUse\":\"\u003cb\u003eGOVERNING TERMS\u003c/b\u003e: Your use of this API is governed by the \u003ca href=\\\"https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA API Trial Service Terms of Use\u003c/a\u003e; and the use of this model is governed by the \u003ca href=\\\"https://docs.nvidia.com/ai-foundation-models-community-license.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA AI Foundation Models Community License\u003c/a\u003e.\\n\",\"showUnavailableBanner\":false,\"cta\":{\"text\":\"Run Anywhere - Notify Me\",\"url\":\"https://www.nvidia.com/en-us/ai/nim-notifyme/\"},\"usage\":\"$7b\",\"deploy\":[{\"label\":\"Linux with Docker\",\"filename\":\"linux.md\",\"contents\":\"$7c\"}]},\"artifactName\":\"parakeet-1_1b-rnnt-multilingual-asr\"},\"config\":{\"name\":\"parakeet-1_1b-rnnt-multilingual-asr\",\"type\":\"model\"}},{\"endpoint\":\"$undefined\",\"spec\":\"$undefined\",\"config\":{\"name\":\"conformer-ctc-asr\",\"type\":\"model\"}},{\"endpoint\":{\"artifact\":{\"artifactType\":\"ENDPOINT\",\"name\":\"whisper-large-v3\",\"displayName\":\"whisper-large-v3\",\"publisher\":\"openai\",\"shortDescription\":\"Robust Speech Recognition via Large-Scale Weak Supervision.\",\"logo\":\"https://assets.ngc.nvidia.com/products/api-catalog/images/whisper-large-v3.jpg\",\"labels\":[\"ASR\",\"AST\",\"Multilingual\",\"NVIDIA NIM\",\"NVIDIA Riva\",\"OpenAI\",\"batch\",\"Speech-to-Text\",\"whisper\"],\"attributes\":[{\"key\":\"AVAILABLE\",\"value\":\"true\"},{\"key\":\"PREVIEW\",\"value\":\"false\"}],\"updatedDate\":\"2025-04-10T15:18:41.346Z\",\"canGuestDownload\":true,\"createdDate\":\"2025-02-18T19:47:35.847Z\",\"description\":\"$7d\",\"isPublic\":true,\"isReadOnly\":true,\"orgName\":\"qc69jvmznzxy\"},\"requestStatus\":{\"statusCode\":\"SUCCESS\",\"requestId\":\"fdf944e0-b618-49c1-8cba-525241e41ce3\"}},\"spec\":{\"namespace\":\"qc69jvmznzxy\",\"updatedDate\":\"2025-08-01T11:46:35.143Z\",\"nvcfFunctionId\":\"b702f636-f60c-4a3d-a6f4-f3568c13bd7d\",\"createdDate\":\"2025-02-18T19:47:36.161Z\",\"attributes\":{\"apiDocsUrl\":\"https://docs.nvidia.com/nim/riva/asr/latest/protos.html\",\"termsOfUse\":\"\u003cb\u003eGOVERNING TERMS\u003c/b\u003e: Your use of this API is governed by the \u003ca href=\\\"https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA API Trial Service Terms of Use\u003c/a\u003e; and the use of this model is governed by the \u003ca href=\\\"https://docs.nvidia.com/ai-foundation-models-community-license.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA AI Foundation Models Community License\u003c/a\u003e.\\n\",\"showUnavailableBanner\":false,\"cta\":{\"text\":\"Run Anywhere - Notify Me\",\"url\":\"https://www.nvidia.com/en-us/ai/nim-notifyme/\",\"nim_available_override_url\":\"https://catalog.ngc.nvidia.com/orgs/nim/teams/nvidia/containers/riva-asr\"},\"usage\":\"$7e\",\"deploy\":[{\"label\":\"Linux with Docker\",\"filename\":\"linux.md\",\"contents\":\"$7f\"}]},\"artifactName\":\"whisper-large-v3\"},\"config\":{\"name\":\"whisper-large-v3\",\"type\":\"model\"}},{\"endpoint\":\"$undefined\",\"spec\":\"$undefined\",\"config\":{\"name\":\"canary-0-6b-turbo-asr\",\"type\":\"model\"}},{\"endpoint\":{\"artifact\":{\"artifactType\":\"ENDPOINT\",\"name\":\"parakeet-ctc-0_6b-vi\",\"displayName\":\"parakeet-ctc-0.6b-vi\",\"publisher\":\"nvidia\",\"shortDescription\":\"Accurate and optimized Vietnamese-English transcriptions with punctuation and word timestamps.\",\"logo\":\"https://assets.ngc.nvidia.com/products/api-catalog/images/parakeet-ctc-0_6b-vi.jpg\",\"labels\":[\"ASR\",\"NVIDIA NIM\",\"Streaming\",\"Vietnamese\",\"Speech-to-Text\"],\"attributes\":[{\"key\":\"AVAILABLE\",\"value\":\"true\"},{\"key\":\"PREVIEW\",\"value\":\"false\"}],\"updatedDate\":\"2025-09-08T15:59:08.704Z\",\"bias\":\"| Field Participation considerations from adversely impacted groups [protected classes](https://www.senate.ca.gov/content/protected-classes) in model design and testing: | Response None |\\n| ----- | ----- |\\n| Measures taken to mitigate against unwanted bias: | None  |\",\"canGuestDownload\":true,\"createdDate\":\"2025-09-08T15:59:08.704Z\",\"description\":\"$80\",\"explainability\":\"$81\",\"isPublic\":true,\"isReadOnly\":true,\"orgName\":\"qc69jvmznzxy\",\"privacy\":\"| Field | Response |\\n| ----- | ----- |\\n| Generatable or reverse engineerable personal data? | None |\\n| Personal data used to create this model? | None |\\n| How often is the dataset reviewed? | Before Release |\\n| Is there provenance for all datasets used in training? | Yes |\\n| Does data labeling (annotation, metadata) comply with privacy laws? | Yes |\\n| Is data compliant with data subject requests for data correction or removal, if such a request was made? | The data is compliant where applicable, but is not applicable for all data. |\\n| Applicable Privacy Policy | [https://www.nvidia.com/en-us/about-nvidia/privacy-policy/](https://www.nvidia.com/en-us/about-nvidia/privacy-policy/)  |\",\"safetyAndSecurity\":\"$82\"},\"requestStatus\":{\"statusCode\":\"SUCCESS\",\"requestId\":\"7c9be825-48a5-444b-a78f-49ec84a3ab6c\"}},\"spec\":{\"namespace\":\"qc69jvmznzxy\",\"updatedDate\":\"2025-12-23T05:50:48.044Z\",\"nvcfFunctionId\":\"f3dff2bb-99f9-403d-a5f1-f574a757deb0\",\"createdDate\":\"2025-09-08T15:59:10.537Z\",\"attributes\":{\"apiDocsUrl\":\"https://docs.nvidia.com/nim/riva/asr/latest/protos.html\",\"termsOfUse\":\"\u003cb\u003eGOVERNING TERMS\u003c/b\u003e: Your use of this API is governed by the \u003ca href=\\\"https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA API Trial Service Terms of Use\u003c/a\u003e; and the use of this model is governed by the \u003ca href=\\\"https://docs.nvidia.com/ai-foundation-models-community-license.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA AI Foundation Models Community License\u003c/a\u003e.\\n\",\"showUnavailableBanner\":false,\"cta\":{\"text\":\"Run Anywhere - Notify Me\",\"url\":\"https://www.nvidia.com/en-us/ai/nim-notifyme/\"},\"usage\":\"$83\",\"deploy\":[{\"label\":\"Linux with Docker\",\"filename\":\"linux.md\",\"contents\":\"$84\"}]},\"artifactName\":\"parakeet-ctc-0_6b-vi\"},\"config\":{\"name\":\"parakeet-ctc-0_6b-vi\",\"type\":\"model\"}},{\"endpoint\":{\"artifact\":{\"artifactType\":\"ENDPOINT\",\"name\":\"parakeet-ctc-0_6b-zh-cn\",\"displayName\":\"parakeet-ctc-0.6b-zh-cn\",\"publisher\":\"nvidia\",\"shortDescription\":\"Record-setting accuracy and performance for Mandarin English transcriptions.\",\"logo\":\"https://assets.ngc.nvidia.com/products/api-catalog/images/parakeet-ctc-0_6b-zh-cn.jpg\",\"labels\":[\"ASR\",\"Mandarin\",\"NVIDIA NIM\",\"Streaming\",\"Speech-to-Text\"],\"attributes\":[{\"key\":\"AVAILABLE\",\"value\":\"true\"},{\"key\":\"PREVIEW\",\"value\":\"false\"}],\"updatedDate\":\"2025-09-09T10:36:17.833Z\",\"bias\":\"$85\",\"canGuestDownload\":true,\"createdDate\":\"2025-09-09T10:36:17.833Z\",\"description\":\"$86\",\"explainability\":\"$87\",\"isPublic\":true,\"isReadOnly\":true,\"orgName\":\"qc69jvmznzxy\",\"privacy\":\"$88\",\"safetyAndSecurity\":\"Field                                               |  Response\\n:---------------------------------------------------|:----------------------------------\\nModel Application(s):                               |  Speech Transcription\\nDescribe the life-critical impacts (if present).   |  Not Applicable\\nUse Case Restriction(s):                              |  Abide by [https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/)\\nDescribe access restrictions (if any):   | The Principle of Least Privilege (PoLP) is applied limiting access for dataset generation and model development. Restrictions enforce dataset access during training and dataset license constraints adhered to.\"},\"requestStatus\":{\"statusCode\":\"SUCCESS\",\"requestId\":\"632f5507-3d24-450b-983e-9535ad5be764\"}},\"spec\":{\"namespace\":\"qc69jvmznzxy\",\"nvcfFunctionId\":\"9add5ef7-322e-47e0-ad7a-5653fb8d259b\",\"createdDate\":\"2025-09-09T10:36:18.315Z\",\"attributes\":{\"apiDocsUrl\":\"https://docs.nvidia.com/nim/riva/asr/latest/protos.html\",\"termsOfUse\":\"\u003cb\u003eGOVERNING TERMS\u003c/b\u003e: Your use of this API is governed by the \u003ca href=\\\"https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA API Trial Service Terms of Use\u003c/a\u003e; and the use of this model is governed by the \u003ca href=\\\"https://docs.nvidia.com/ai-foundation-models-community-license.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA AI Foundation Models Community License\u003c/a\u003e.\\n\",\"showUnavailableBanner\":false,\"cta\":{\"text\":\"Run Anywhere - Notify Me\",\"url\":\"https://www.nvidia.com/en-us/ai/nim-notifyme/\"},\"usage\":\"$89\",\"deploy\":[{\"label\":\"Linux with Docker\",\"filename\":\"linux.md\",\"contents\":\"$8a\"}]},\"artifactName\":\"parakeet-ctc-0_6b-zh-cn\"},\"config\":{\"name\":\"parakeet-ctc-0_6b-zh-cn\",\"type\":\"model\"}},{\"endpoint\":{\"artifact\":{\"artifactType\":\"ENDPOINT\",\"name\":\"parakeet-ctc-0_6b-es\",\"displayName\":\"parakeet-ctc-0.6b-es\",\"publisher\":\"nvidia\",\"shortDescription\":\"Accurate and optimized Spanish English transcriptions with punctuation and word timestamps.\",\"logo\":\"https://assets.ngc.nvidia.com/products/api-catalog/images/parakeet-ctc-0_6b-es.jpg\",\"labels\":[\"ASR\",\"NVIDIA NIM\",\"Spanish\",\"Streaming\",\"Speech-to-Text\"],\"attributes\":[{\"key\":\"AVAILABLE\",\"value\":\"true\"},{\"key\":\"PREVIEW\",\"value\":\"false\"}],\"updatedDate\":\"2025-09-09T10:10:03.570Z\",\"bias\":\"$8b\",\"canGuestDownload\":true,\"createdDate\":\"2025-09-09T10:10:03.570Z\",\"description\":\"$8c\",\"explainability\":\"$8d\",\"isPublic\":true,\"isReadOnly\":true,\"orgName\":\"qc69jvmznzxy\",\"privacy\":\"$8e\",\"safetyAndSecurity\":\"Field                                               |  Response\\n:---------------------------------------------------|:----------------------------------\\nModel Application(s):                               |  Speech Transcription\\nDescribe the life-critical impacts (if present).   |  Not Applicable\\nUse Case Restriction(s):                              |  Abide by https://developer.nvidia.com/riva/ga/license\\nDescribe access restrictions (if any):   | The Principle of Least Privilege (PoLP) is applied limiting access for dataset generation and model development. Restrictions enforce dataset access during training and dataset license constraints adhered to.\"},\"requestStatus\":{\"statusCode\":\"SUCCESS\",\"requestId\":\"4eac35c1-490b-4e25-9cca-d99fd687e9dc\"}},\"spec\":{\"namespace\":\"qc69jvmznzxy\",\"updatedDate\":\"2025-10-16T03:00:09.465Z\",\"nvcfFunctionId\":\"None\",\"createdDate\":\"2025-09-09T10:10:04.376Z\",\"attributes\":{\"apiDocsUrl\":\"https://docs.nvidia.com/nim/riva/asr/latest/protos.html\",\"termsOfUse\":\"\u003cb\u003eGOVERNING TERMS\u003c/b\u003e: Your use of this API is governed by the \u003ca href=\\\"https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA API Trial Service Terms of Use\u003c/a\u003e; and the use of this model is governed by the \u003ca href=\\\"https://docs.nvidia.com/ai-foundation-models-community-license.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA AI Foundation Models Community License\u003c/a\u003e.\\n\",\"showUnavailableBanner\":false,\"cta\":{\"text\":\"Run Anywhere - Notify Me\",\"url\":\"https://www.nvidia.com/en-us/ai/nim-notifyme/\"},\"usage\":\"$8f\",\"deploy\":[{\"label\":\"Linux with Docker\",\"filename\":\"linux.md\",\"contents\":\"$90\"}]},\"artifactName\":\"parakeet-ctc-0_6b-es\"},\"config\":{\"name\":\"parakeet-ctc-0_6b-es\",\"type\":\"model\"}}],\"items\":[\"$91\",\"$92\",\"$93\",\"$94\",\"$95\",\"$96\",\"$97\",\"$98\",\"$99\",\"$9a\",\"$9b\",\"$9c\"],\"mode\":\"$undefined\",\"params\":\"$9d\",\"slotTitle\":[[\"$\",\"div\",null,{\"className\":\"mb-2 flex items-start gap-2 max-xs:justify-between\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-ml font-medium leading-body tracking-less text-manitoulinLightWhite mb-0\",\"children\":\"Automatic Speech Recognition (ASR)\"}],\"$undefined\"]}],[\"$\",\"p\",null,{\"className\":\"text-md font-normal text-manitoulinLightGray mb-0\",\"children\":\"Low Latency NVIDIA Nemotron Speech transcription models for your agentic AI workflows.\"}],\" \"]}]\n"])</script><script>$RC("B:3","S:3")</script><script>$RC("B:2","S:2")</script><script>self.__next_f.push([1,"9e:T1a3b,"])</script><script>self.__next_f.push([1,"# Model Overview\n\n\n### Description:\nThe Riva-Translate-4B-Instruct-v1.1 Neural Machine Translation model translates text in 12 languages. The supported languages are: English(en), German(de), European Spanish(es-ES), LATAM Spanish(es-US), France(fr), Brazillian Portugese(pt-BR), Russian(ru), Simplified Chinese(zh-CN), Traditional Chinese(zh-TW), Japanese(ja),Korean(ko), Arabic(ar). It supports both sentence and document level translation. The model delivers a significant performance uplift compared to all previous in-house NMT models. This model is ready for commercial/non-commercial use. \u003cbr\u003e\n\n### License/Terms of Use\nGOVERNING TERMS: This trial service is governed by the [NVIDIA API Trial Terms of Service](https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf). Use of this model is governed by the [NVIDIA Community Model License](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) ADDITIONAL INFORMATION: [Apache 2.0](https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md). \u003cbr\u003e\n\n\n### Deployment Geography:\nGlobal \u003cbr\u003e\n\n### Use Case: \u003cbr\u003e\nTranslators, marketers, and web developers who deliver content in multiple languages. \u003cbr\u003e\n\n### Release Date:  \u003cbr\u003e\nBuild.nvidia.com 12/11/2025 via [Link](https://build.nvidia.com/nvidia/riva-translate-4b-instruct-v1_1) \u003cbr\u003e\n\n\n## References(s):\n[1] Vaswani, Ashish, et al. \"Attention is all you need.\" arXiv preprint arXiv:1706.03762 (2017).\n[2] https://github.com/openai/tiktoken\n[3] https://en.wikipedia.org/wiki/BLEU\n[4] https://github.com/mjpost/sacreBLEU\n[5] https://github.com/Unbabel/COMET\n[6] NVIDIA NeMo Toolkit\n\u003cbr\u003e \n\n## Model Architecture:\n**Architecture Type:** Transformer \u003cbr\u003e\n\n**Network Architecture:** Decoder-only \u003cbr\u003e\n\nThis model was developed based on Transformer architecture originally presented in \"Attention Is All You Need\" paper [1]. It is a fine-tuned version of a 4B Base model that was pruned and distilled from nvidia/Mistral-NeMo-Minitron-8B-Base using our LLM compression technique. The model was trained using a multi-stage CPT and SFT. It uses tiktoken [2] as the tokenizer. The model supports a context length of 8K tokens.\n\u003cbr\u003e \n\n\n## Input: \u003cbr\u003e\n**Input Type(s):** Text \u003cbr\u003e\n**Input Format:** String \u003cbr\u003e\n**Input Parameters:** One-Dimensional (1D) \u003cbr\u003e\n**Other Properties Related to Input:** This model supports a context length of 8K. \u003cbr\u003e\n\n## Output: \u003cbr\u003e\n**Output Type(s):** Text \u003cbr\u003e\n**Output Format:** String \u003cbr\u003e\n**Output Parameters:** One-Dimensional (1D) \u003cbr\u003e\n**Other Properties Related to Output:** This model supports a context length of 8K. \u003cbr\u003e\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA’s hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. \u003cbr\u003e\n\n## Prompt Format: \u003cbr\u003e\nWe recommend using the following prompt template, which was used to fine-tune the model. The model may not perform optimally without it.\n```\n\u003cs\u003eSystem\nYou are an expert at translating text from {Source_language} to {Target_language}.\u003c/s\u003e\n\u003cs\u003eUser\nWhat is the {Target_language} translation of the sentence: {Input_Sentence}?\u003c/s\u003e\n\u003cs\u003eAssistant\\n\n\u003cbr\u003e\n```\n\n## Performance: \nCOMET score of any2en and en2any direction for Flores-101 dataset (Higher score is better)\n\n| Language               | Eng -\u003e Language | Language -\u003e Eng |\n|------------------------|-----------------|-----------------|\n| German                 |     0.663       |       0.7575    |\n| European Spanish       |     0.7475      |       0.7317    |\n| Latin American Spanish |     0.7472      |       0.7318    |\n| French                 |     0.824       |       0.8154    |\n| Brazil Portuguese      |     0.894       |       0.8466    |\n| Russian                |     0.7234      |       0.6427    |\n| Simplified Chinese     |     0.6609      |       0.701     |\n| Traditional Chinese    |     0.6319      |       0.6745    |\n| Japanese               |     0.7263      |       0.6664    |\n| Korean                 |     0.712       |       0.6801    |\n| Arabic                 |     0.6888      |       0.7073    |\n\n\n## Software Integration:\n**Runtime Engine(s):** NeMo Framework 24.09 \u003cbr\u003e \n\n**Supported Hardware Microarchitecture Compatibility:** \u003cbr\u003e\n* NVIDIA Ampere \u003cbr\u003e\n* NVIDIA Blackwell \u003cbr\u003e\n* NVIDIA Hopper \u003cbr\u003e\n* NVIDIA Lovelace \u003cbr\u003e\n\n**Supported Operating System(s):** \u003cbr\u003e\n* Linux \u003cbr\u003e\n\n\n## Model Version(s):\nRiva-Translate-4B-Instruct-v1.1 \u003cbr\u003e\n\n# Training \u0026 Evaluation: \n### Training Dataset:\n\nData Collection Method by dataset:  \u003cbr\u003e\n* Hybrid: Human, Synthetic \u003cbr\u003e\n\nLabeling Method by dataset:  \u003cbr\u003e\n* Automated \u003cbr\u003e\n\n**Properties:** This model is trained on open-sourced datasets and synthetic datasets of text parallel corpora generated via back-translation and monolingual datasets. Each entry in the parallel corpus consists of a text in the source language and its translation in the target language. The monolingual datasets contain texts from each of the 12 target language domains. See bias subcard for language distribution.  \u003cbr\u003e\n\n\n## Evaluation Dataset:\n\n**Link:** We used Flores101 [1], NTREX-128 [2], FRMT [3https://www.statmt.org/wmt19/translation-task.html], WMT 19 [4], WMT20 [5] to evaluate the model.\n\u003cbr\u003e\n\nData Collection Method by dataset:  \u003cbr\u003e\n* Automated \u003cbr\u003e\n\nLabeling Method by dataset:  \u003cbr\u003e\n* Automated \u003cbr\u003e\n\n## References:\nFor more information about these datasets, please see the links below.\n[1] https://aclanthology.org/2022.tacl-1.30.pdf\n[2] https://aclanthology.org/2022.sumeval-1.4.pdf\n[3] https://aclanthology.org/2023.tacl-1.39.pdf\n[4] https://www.statmt.org/wmt19/translation-task.html\n[5] https://www.statmt.org/wmt20/translation-task.html\n\n# Inference:\n**Acceleration Engine:** TensorRT-LLM \u003cbr\u003e\n**Test Hardware:** \u003cbr\u003e  \n* A100 \u003cbr\u003e\n* A10G \u003cbr\u003e\n* H100 \u003cbr\u003e\n* L40S \u003cbr\u003e\n\n## Ethical Considerations:\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. \u003cbr\u003e \n\nFor more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety \u0026 Security, and Privacy Subcards. \u003cbr\u003e\n\nPlease report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).  \u003cbr\u003e"])</script><script>self.__next_f.push([1,"9f:Ta1a,"])</script><script>self.__next_f.push([1,"Field                                                                                                  |  Response\n:------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------\nIntended Task/Domain:                                                                                  |  Text-to-Text translation\nModel Type:                                                                                            |  Decoder-only Transformer\nIntended Users:                                                                                        |  Translators, marketers, and web developers who deliver content in multiple languages.\nOutput:                                                                                                |  Text translated into the target language\nDescribe how the model works:                                                                          |  The model takes text in the source language as the input and translate it into the target languages in the text form.\nName the adversely impacted groups this has been tested to deliver comparable outcomes regardless of:  |  Not Applicable\nTechnical Limitations \u0026 Mitigation:                                                                    |  Accuracy varies based on the characteristics of the input (domain, use case, noise, context, etc.). Grammatical errors and semantic issues may be present. As a potential mitigation, users can adjust the prompt to improve translation quality.\nVerified to have met prescribed NVIDIA quality standards:  |  Yes\nPerformance Metrics:                                                                                   |  BLEU and COMET scores.\nPotential Known Risks:                                                                                 |  Translations may not be 100% accurate. This is not recommended for word-for-word translation.\nLicensing:                                                                                             |  GOVERNING TERMS: This trial service is governed by the [NVIDIA API Trial Terms of Service](https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf). Use of this model is governed by the [NVIDIA Community Model License](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) ADDITIONAL INFORMATION: [Apache 2.0](https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md)."])</script><script>self.__next_f.push([1,"a0:T53e,Field                                                                                                                              |  Response\n:----------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------\nGeneratable or reverse engineerable personal data?                                                     |  No\nPersonal data used to create this model?                                                                                       |  No\nHow often is dataset reviewed?                                                                                                     |  Dataset is initially reviewed upon addition, and subsequent reviews are conducted as needed or upon request for changes.\nIs there provenance for all datasets used in training?                                                                                |  Yes.\nDoes data labeling (annotation, metadata) comply with privacy laws?                                                                |  Yes.\nIs data compliant with data subject requests for data correction or removal, if such a request was made?                           |  Yes\nApplicable Privacy Policy        | [Policy](https://www.nvidia.com/en-us/about-nvidia/privacy-policy/)a1:T4bd,from openai import OpenAI\n\nclient = OpenAI(\n  base_url = \"https://integrate.api.nvidia.com/v1\",\n  api_key = \"$NVIDIA_API_KEY\"\n)\n\u003c% if (request.tools) { %\u003e\ncompletion = client.chat.completions.create(\n  model=\"\u003c%- request.model %\u003e\",\n  messages=\u003c%- JSON.stringify(request.messages) %\u003e,\n  temperature=\u003c%- request.temperature %\u003e,\n  top_p=\u003c%- request.top_p %\u003e,\n  max_tokens=\u003c%- request.max_tokens %\u003e,\n  stream=\u003c%- request.stream?.toString()[0].toUpperCase() + request.stream?.toString().slice(1) %\u003e,\n  tools=\u003c%- JSON.stringify(request.tools) %\u003e,\n  \u003c% if (request.tool_choice) { %\u003etool_choice=\u003c%- JSON.stringify(request.tool_choice) %\u003e\u003c% } %\u003e\n)\u003c% } else { %\u003e\ncompletion = client.chat.completions."])</script><script>self.__next_f.push([1,"create(\n  model=\"\u003c%- request.model %\u003e\",\n  messages=\u003c%- JSON.stringify(request.messages) %\u003e,\n  temperature=\u003c%- request.temperature %\u003e,\n  top_p=\u003c%- request.top_p %\u003e,\n  max_tokens=\u003c%- request.max_tokens %\u003e,\n  stream=\u003c%- request.stream?.toString()[0].toUpperCase() + request.stream?.toString().slice(1) %\u003e\n)\u003c% } %\u003e\n\u003c% if (request.stream) { %\u003e\nfor chunk in completion:\n  if chunk.choices[0].delta.content is not None:\n    print(chunk.choices[0].delta.content, end=\"\")\n\u003c% } else { %\u003e\nprint(completion.choices[0].message)\n\u003c% } %\u003e\na2:T504,import OpenAI from 'openai';\n\nconst openai = new OpenAI({\n  apiKey: '$NVIDIA_API_KEY',\n  baseURL: 'https://integrate.api.nvidia.com/v1',\n})\n \u003c% if (request.tools) { %\u003e\nasync function main() {\n  const completion = await openai.chat.completions.create({\n    model: \"\u003c%- request.model %\u003e\",\n    messages: \u003c%- JSON.stringify(request.messages) %\u003e,\n    temperature: \u003c%- request.temperature %\u003e,\n    top_p: \u003c%- request.top_p %\u003e,\n    max_tokens: \u003c%- request.max_tokens %\u003e,\n    stream: \u003c%- request.stream %\u003e,\n    \u003c% if (request.tools) { %\u003etools: \u003c%- JSON.stringify(request.tools) %\u003e,\u003c% } %\u003e\n    \u003c% if (request.tool_choice) { %\u003etool_choice: \u003c%- JSON.stringify(request.tool_choice) %\u003e,\u003c% } %\u003e\n  })\u003c% } else { %\u003e\nasync function main() {\n  const completion = await openai.chat.completions.create({\n    model: \"\u003c%- request.model %\u003e\",\n    messages: \u003c%- JSON.stringify(request.messages) %\u003e,\n    temperature: \u003c%- request.temperature %\u003e,\n    top_p: \u003c%- request.top_p %\u003e,\n    max_tokens: \u003c%- request.max_tokens %\u003e,\n    stream: \u003c%- request.stream %\u003e\n  })\u003c% } %\u003e\n   \u003c% if (request.stream) { %\u003e\n  for await (const chunk of completion) {\n    process.stdout.write(chunk.choices[0]?.delta?.content || '')\n  }\n  \u003c% } else { %\u003e\n  process.stdout.write(completion.choices[0]?.message?.content);\n  \u003c% } %\u003e\n}\n\nmain();a3:T685,\u003c% if (request.tools) { %\u003e\n  \"curl https://integrate.api.nvidia.com/v1/chat/completions \\\\\\n\n    -H \\\"Content-Type: application/json\\\" \\\\\\n\n    -H \\\"Authorization: Bearer $NVIDIA_API_KEY\\\" \\\\\\n\n    -d '{\\n\n      \\\"model\\\": \\\""])</script><script>self.__next_f.push([1,"nvidia/riva-translate-4b-instruct-v1.1\\\",\\n\n      \\\"messages\\\": \u003c%- JSON.stringify(request.messages).replaceAll(\\\"'\\\", \\\"'\\\\\\\"'\\\\\\\"'\\\") %\u003e,\\n\n      \\\"temperature\\\": \u003c%- request.temperature %\u003e,\\n\n      \\\"top_p\\\": \u003c%- request.top_p %\u003e,\\n\n      \\\"max_tokens\\\": \u003c%- request.max_tokens %\u003e,\\n\n      \\\"stream\\\": \u003c%- request.stream %\u003e\n      \u003c% if (request.tools) { %\u003e,\\n    \\\"tools\\\": \u003c%- JSON.stringify(request.tools).replaceAll(\\\"'\\\", \\\"'\\\\\\\"'\\\\\\\"'\\\") %\u003e\u003c% } %\u003e\n      \u003c% if (request.tool_choice) { %\u003e,\\n    \\\"tool_choice\\\": \u003c%- JSON.stringify(request.tool_choice).replaceAll(\\\"'\\\", \\\"'\\\\\\\"'\\\\\\\"'\\\") %\u003e\u003c% } %\u003e\n    }'\\n\"\u003c% } else { %\u003e\n  \"curl https://integrate.api.nvidia.com/v1/chat/completions \\\\\\n\n    -H \\\"Content-Type: application/json\\\" \\\\\\n\n    -H \\\"Authorization: Bearer $NVIDIA_API_KEY\\\" \\\\\\n\n    -d '{\\n\n      \\\"model\\\": \\\"nvidia/riva-translate-4b-instruct-v1.1\\\",\\n\n      \\\"messages\\\": \u003c%- JSON.stringify(request.messages).replaceAll(\\\"'\\\", \\\"'\\\\\\\"'\\\\\\\"'\\\") %\u003e,\\n\n      \\\"temperature\\\": \u003c%- request.temperature %\u003e,\\n\n      \\\"top_p\\\": \u003c%- request.top_p %\u003e,\\n\n      \\\"max_tokens\\\": \u003c%- request.max_tokens %\u003e,\\n\n      \\\"stream\\\": \u003c%- request.stream %\u003e\n      \u003c% if (request.tools) { %\u003e,\\n    \\\"tools\\\": \u003c%- JSON.stringify(request.tools).replaceAll(\\\"'\\\", \\\"'\\\\\\\"'\\\\\\\"'\\\") %\u003e\u003c% } %\u003e\n      \u003c% if (request.tool_choice) { %\u003e,\\n    \\\"tool_choice\\\": \u003c%- JSON.stringify(request.tool_choice).replaceAll(\\\"'\\\", \\\"'\\\\\\\"'\\\\\\\"'\\\") %\u003e\u003c% } %\u003e\n    }'\\n\"\u003c% } %\u003ea4:Tea8,"])</script><script>self.__next_f.push([1,"## Description:\nThe Riva Translate 1.6B Multilingual Neural Machine Translation model translates text in any to any directions across the 37 supported languages, including non-English centric translation (such as French to Chinese, etc). The Supported languages are: English(en), Czech(cs), Danish (da), German(de), Greek(el), European Spanish(es-ES), LATAM Spanish(es-US), Finnish(fi), France(fr), Hungarian(hu), Italian(it), Lithuanian(lt), Latvian(lv),Dutch(nl), Norwegian(no), Polish(pl), European Portuguese(pt-PT), Brazillian Portuguese(pt-BR), Romanian(ro), Russian(ru), Slovak(sk), Swedish(sv), Simplified Chinese(zh-CN), Traditional Chinese(zh-TW), Japanese(ja), Hindi(hi), Korean(ko), Estonian(et), Slovenian(sl), Bulgarian(bg), Ukrainian(uk), Croatian(hr), Arabic(ar), Vietnamese(vi), Turkish(tr), Indonesian(id), Thai(th). This model is ready for commercial use. \n\n\n## Model Architecture\n\nArchitecture Type: Transformer\n\nNetwork Architecture: Megatron\n\nThe model is based on Transformer architecture originally presented in \"Attention Is All You Need\" paper [1]. In this particular instance, the model has 24 layers in the encoder and 24 layers in the decoder. It is using SentencePiece tokenizer [2].\n\n\n## Input: \n**Input Type(s):** Text String \u003cbr\u003e\n**Input Format(s):** List \u003cbr\u003e\n\n**Other Properties Related to Input:** No Pre-Processing Needed; No Tokenization required; 512 Character Text String Limit (No non-textual characters) \u003cbr\u003e\n\n\n## Output: \n**Output Type(s):** Text String \u003cbr\u003e\n**Output Format:** List \u003cbr\u003e\n**Output Parameters:** Selected Language \u003cbr\u003e\n**Other Properties Related to Output:** Outputs are not tokenized or processed to hide sensitive input information \u003cbr\u003e\n\n# Training \u0026 Evaluation Dataset: \n\n** Data Collection Method by dataset \u003cbr\u003e\n* Human \u003cbr\u003e\n\n** Labeling Method by dataset \u003cbr\u003e\n* Automated \u003cbr\u003e\n\n**Properties (Quantity, Dataset Descriptions, Sensor(s)):** This model is trained on open-sourced datasets and synthetic datasets of text parallel corpora generated via back-translation. \u003cbr\u003e\n\n## References:\n[1] Vaswani, Ashish, et al. \"Attention is all you need.\" arXiv preprint arXiv:1706.03762 (2017).\n[2] https://github.com/google/sentencepiece\n[3] https://en.wikipedia.org/wiki/BLEU\n[4] https://github.com/mjpost/sacreBLEU\n[5] NVIDIA NeMo Toolkit\n\n## Software Integration\n**Runtime Engine(s):** \n* Riva 2.19.0 or Higher \u003cbr\u003e\n\n**Supported Operating System(s):**  \u003cbr\u003e\n* Linux \u003cbr\u003e\n\n## Model Version(s): \nriva-translate-1.6b\u003cbr\u003e\n\n# Inference\n**Engine:** Triton \u003cbr\u003e\n**Test Hardware:**  \u003cbr\u003e\n* NVIDIA H100 GPU\n* NVIDIA A100 GPU\n* NVIDIA L40 GPU\n\n\n## Ethical Considerations (For NVIDIA Models Only):\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety \u0026 Security, and Privacy Subcards.  Please report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).\n\n## GOVERNING TERMS: \nThis trial is governed by the NVIDIA API Trial Terms of Service (found at https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf). The use of this model is governed by the AI Foundation Models Community License Agreement (found at NVIDIA Agreements | Enterprise Software | NVIDIA AI Foundation Models Community License Agreement)."])</script><script>self.__next_f.push([1,"a5:T810,"])</script><script>self.__next_f.push([1,"Field                                                                                                  |  Response\n:------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------\nIntended Applications \u0026 Domains:                                                                       | Text and Document Translation\nTypes:                                                                                                 | Text translation\nIntended Users:                                                                                        | This model is intended for our developers to perform text and document translation.\nOutput:                                                                                                | Text (in Target Language)\nDescribe how the model works:                                                                          | Model translates text in one language into in target language.\nName the adversely impacted groups this has been tested to deliver comparable outcomes regardless of:  | Not Applicable\nTechnical Limitations:                                                                                 | Translations may not be 100% accurate. Accuracy varies based on the characteristics of input (Domain, Use Case, Noise, Context, etcetera). Grammar errors and semantic issues may be present.\nVerified to have met prescribed NVIDIA quality standards:                                                     | Yes\nPerformance Metrics:                                                                                   | BiLingual Evaluation Understudy (BLEU) Scores, Crosslingual Optimized Metric for Evaluation of Translation (COMET)\nPotential Known Risks:                                                                                 | None.\nLicensing:                                                                                             | [Riva](https:developer.nvidia.com/riva/ga/license)"])</script><script>self.__next_f.push([1,"a6:T7c0,Field                                                                                                                              |  Response\n:----------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------\nGeneratable or reverse engineerable personally-identifiable information (PII)?                                                     | None\nWas consent obtained for any PII used?                                                                                             | Not Applicable\nProtected class data used to create this model?                                                                                    | Not Applicable\nHow often is dataset reviewed?                                                                                                     | Before Release\nIs a mechanism in place to honor data subject right of access or deletion of personal data?                                        | No\nIf PII collected for the development of the model, was it collected directly by NVIDIA?                                            | Not Applicable\nIf PII collected for the development of the model by NVIDIA, do you maintain or have access to disclosures made to data subjects?  | Not Applicable\nIf PII collected for the development of this AI model, was it minimized to only what was required?                                 | Not Applicable\nIs there provenance for all datasets used in training?                                                                                                                          | Yes\nDoes data labeling (annotation, metadata) comply with privacy laws?                                                                | Yes\nIs data compliant with data subject requests for data correction or removal, if such a request was made?                           | No, not possible with externally-sourced data.a7:T7a1,### Getting Started\n\nRiva uses \u003ca href=\"https://"])</script><script>self.__next_f.push([1,"grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Instructions below demonstrate usage of \u003c%- name %\u003e model using Python gRPC client.\n\n### Prerequisites\n\nYou will need a system with Git and Python 3+ installed.\n\n### Install Riva Python Client\n\n```bash\npip install nvidia-riva-client\n```\n\n### Download Python Client\n\nDownload Python client code by cloning \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e.\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\n### Run Python Client\n\nOpen a command terminal and execute below command to translate text. If you have generated the API key, it will be auto-populated in the command.\n\n```bash\npython python-clients/scripts/nmt/nmt.py \\\n    --server grpc.nvcf.nvidia.com:443 --use-ssl \\\n    --metadata function-id \"\u003c%- nvcfFunctionId %\u003e\" \\\n    --metadata \"authorization\" \"Bearer \u003c%- apiKey %\u003e\" \\\n    --text \"This is an example text for Riva text translation\" \\\n    --source-language-code en \\\n    --target-language-code de\n```\n\nList of supported source and target languages can be queried using below command.\n\n```bash\npython python-clients/scripts/nmt/nmt.py \\\n    --server grpc.nvcf.nvidia.com:443 --use-ssl \\\n    --metadata function-id \"\u003c%- nvcfFunctionId %\u003e\" \\\n    --metadata \"authorization\" \"Bearer \u003c%- apiKey %\u003e\" \\\n    --list-models\n```\n\n### Support for gRPC clients in other languages\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Proto files can be downloaded from \u003ca href=\"https://github.com/nvidia-riva/common/archive/refs/heads/main.zip\"\u003eRiva gRPC Proto files\u003c/a\u003e and compiled to target language using \u003ca href=\"https://grpc.io/docs/protoc-installation/\"\u003eProtoc compiler\u003c/a\u003e. Example Riva clients in C++ and Python languages are provided below.\n\n* \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e\n* \u003ca href=\"https://github.com/nvidia-riva/cpp-clients\"\u003eC++ Client Repository\u003c/a\u003e\na8:T708,## Step 1. Generate API Key\n\n::generate-api-key\n\n## Step 2. Pull and Run the NIM\n\n```bash\n$ docker login nvcr.io\nUsername: $oauthtoken\n"])</script><script>self.__next_f.push([1,"Password: \u003cPASTE_API_KEY_HERE\u003e\n```\n\nThis command launches NIM container on any of the supported GPUs.\n\n```bash\nexport NGC_API_KEY=\u003cPASTE_API_KEY_HERE\u003e\n\nexport CONTAINER_NAME=riva-translate-1_6b\ndocker run -it --rm --name=$CONTAINER_NAME \\\n  --runtime=nvidia \\\n  --gpus '\"device=0\"' \\\n  --shm-size=8GB \\\n  -e NGC_API_KEY=$NGC_API_KEY \\\n  -e NIM_HTTP_API_PORT=9000 \\\n  -e NIM_GRPC_API_PORT=50051 \\\n  -p 9000:9000 \\\n  -p 50051:50051 \\\n  nvcr.io/nim/nvidia/riva-translate-1_6b:latest\n```\n\nIt may take a up to 30 minutes depending on your network speed, for the container to be ready and start accepting requests from the time the docker container is started.\n\n## Step 3. Test the NIM\n\nOpen a new terminal and run following command to check if the service is ready to handle inference requests\n\n```bash\ncurl -X 'GET' 'http://localhost:9000/v1/health/ready'\n```\n\nIf the service is ready, you get a response similar to the following.\n```bash\n{\"ready\":true}\n```\n\nInstall the Riva Python client package\n\n```bash\nsudo apt-get install python3-pip\npip install -U nvidia-riva-client\n```\n\nDownload Riva sample clients\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\nRun Text to Text translation inference\n\n```bash\npython3 python-clients/scripts/nmt/nmt.py --server 0.0.0.0:50051 --text \"This will become German words\" --source-language-code en --target-language-code de\n```\n\nAbove command will translate the text from English to German and output will be as shown below.\n\n```bash\n## Das werden deutsche Wörter\n```\n\nFor more details on getting started with this NIM, visit the [Riva NIM Docs](https://docs.nvidia.com/nim/riva/nmt/latest/overview.html).\na9:T572,Field                                                                                               |  Response\n:---------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------\nWhat is the language balance of the model validation data? | ~3% per language a"])</script><script>self.__next_f.push([1,"cross following languages and dialects: Arabic, Danish, Czech, German, American English, British English, Latin American Spanish, European Spanish, Metropole French, Canadian French, Hebrew, Hindi, Italian, Japanese, Korean, Norwegian Bokmal, Dutch, Polish, European Portuguese, Brazilian Portuguese, Russian, Swedish, Thai, Turkish, Mandarin Chinese.  \nWhat is the geographic origin language balance of the model validation data? | Middle East, United States, Europe, East Asia, South Asia, Africa, Latin America, Southeast Asia\nWhat is the accent balance of the model validation data? | ~3% per language.\nParticipation considerations from adversely impacted groups ([protected classes](https://www.senate.ca.gov/content/protected-classes)) in model design and testing:  |  Age, Gender, Linguistic Background\nMeasures taken to mitigate against unwanted bias:                                                   |  Used a custom dataset to validate model performance across demographic groups (gender, age, and language).aa:T1a25,"])</script><script>self.__next_f.push([1,"# Speech Recognition/Translation: Canary\n          \n## Description\n\nRIVA Canary-1b-Flash (around 1B parameters) [1][2] is a multi-lingual multi-tasking model that supports automatic speech-to-text recognition (ASR) and automatic speech-to-text translation (AST) between all non-English and English in both directions for the following languages: Arabic (ar-AR), Danish (da-DK), Czech (cs-CZ), English (en-US), Spanish (es-US, es-ES), German (de-DE), French (fr-FR), Hindi (hi-IN), Italian (it-IT), Portuguese (pt-BR, pt-PT), Japanese (ja-JP), Korean (ko-KR), Dutch (nl-NL), Norwegian (nb-NO), Polish (pl-PO), Russian (ru-RU), Swedish (sv-SE), Turkish (tr-TR), and Mandarin (zh-CN) with punctuation and capitalization (PnC). Bulgarian (bg-BG), Greek (el-GR), Estonian (et-EE), Finnish (fi-FI), Croatian (hr-HR), Hungarian (hu-HU), Indonesian (id-ID), Lithuanian (lt-LT), Latvian (lv-LV), Romanian (ro-RO), Slovak (sk-SK), Slovenian (sl-SI), Ukrainian (uk-UA) and Vietnamese (vi-VN) are also supported as target languages for translation from English. British English (en-GB), Canadian French (fr-CA), and Norwegian Nynorsk (nn-NO) are supported for ASR and translation to English. Hebrew (he-IL) is supported for ASR. Thai (th-TH) is supported for ASR and translation target from English.\n\nThis model is ready for commercial use. \n\n### License/Terms of Use\n \nGOVERNING TERMS: The NIM container is governed by the [NVIDIA Software License Agreement](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/) and [Product-Specific Terms for AI Products](https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/). Use of this model is governed by the [NVIDIA Community Model License](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/).\n\n\n## Deployment Geography:\nGlobal\n\n## Use Case:\nThis model serves developers, researchers, academics, and industries building applications that require speech-to-text capabilities, including but not limited to: conversational AI, voice assistants, transcription services, subtitle generation, and voice analytics platforms.\n\n#Release Date:\nBuild.Nvidia.com [10/15/2025] via [https://build.nvidia.com/nvidia/canary-1b-asr] \nHugging Face [10/15/2025] via [URL] \nNGC [10/15/2025] via [https://build.stg.ngc.nvidia.com/nvidia/canary-1b-asr] \n\n\n## References\n\n[1] [Less is More: Accurate Speech Recognition \u0026 Translation without Web-Scale Data](https://arxiv.org/pdf/2406.19674) \u003cbr\u003e\n[2] [Training and Inference Efficiency of Encoder-Decoder Speech Models](https://arxiv.org/pdf/2503.05931) \u003cbr\u003e\n[3] [New Standard for Speech Recognition and Translation from the NVIDIA NeMo Canary Model](https://developer.nvidia.com/blog/new-standard-for-speech-recognition-and-translation-from-the-nvidia-nemo-canary-model/) \u003cbr\u003e\n[4] [Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition](https://arxiv.org/abs/2305.05084) \u003cbr\u003e\n[6] [Attention Is All You Need](https://arxiv.org/abs/1706.03762) \u003cbr\u003e\n\n## Model Architecture\n\n**Architecture Type:** This model was developed based on the Canary Flash Architecture [2]. An encoder-decoder model with FastConformer [4] encoder and Transformer decoder [5]  \u003cbr\u003e\n**Network Architecture:** 42 layer encoder, 8 layer decoder, Number of model parameters: 918M \u003cbr\u003e\n\n\n## Input\n \n**Input Type(s):** Audio \u003cbr\u003e\n**Input Format:** wav \u003cbr\u003e\n**Input Parameters:** One-Dimensional (1D) \u003cbr\u003e\n**Other Properties Related to Input:** Mono channel is required \u003cbr\u003e\n\n## Output\n \n**Output Type(s):** Text \u003cbr\u003e\n**Output Format:** String \u003cbr\u003e\n**Output Parameters:** One-Dimensional (1D) \u003cbr\u003e\n**Other Properties Related to Output:** No Maximum Character Length, Does not handle special characters \u003cbr\u003e\n\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA’s hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.\n\n## Software Integration\n\n**Runtime Engine(s):** \n* Riva 2.23.0 or higher \u003cbr\u003e\n\n**Supported Hardware Microarchitecture Compatibility:** \u003cbr\u003e\n* NVIDIA Ampere \u003cbr\u003e\n* NVIDIA Hopper \u003cbr\u003e\n* NVIDIA Jetson \u003cbr\u003e\n* NVIDIA Turing \u003cbr\u003e\n* NVIDIA Volta \u003cbr\u003e\n\n**[Preferred/Supported] Operating System(s):** \u003cbr\u003e\n* Linux \u003cbr\u003e\n* Linux 4 Tegra \u003cbr\u003e\n\nThe integration of foundation and fine-tuned models into AI systems requires additional testing using use-case-specific data to ensure safe and effective deployment. Following the V-model methodology, iterative testing and validation at both unit and system levels are essential to mitigate risks, meet technical and functional requirements, and ensure compliance with safety and ethical standards before deployment.\n\n## Model Version(s): \n\nCanary-1B-Flash 2.0\n\n# Training \u0026 Evaluation \n\n## Training Dataset\n\n**Data Modality**\n\n[Audio]\u003cbr\u003e\n[Text]\n\n**Audio Training Data Size**\n\n[10,000 to 1 Million Hours]\n\n**Data Collection Method by dataset** \u003cbr\u003e\n\n* Human \u003cbr\u003e\n\n**Labeling Method by dataset**\u003cbr\u003e\n\n* ASR: Human \u003cbr\u003e\n* AST: Human, Synthetic \u003cbr\u003e\n\n**Properties:**  \u003cbr\u003e\n\nMixture of organic ASR data aligned with human voices and machine generated translations to create AST pairings.\n\n## Evaluation Dataset\n\n**Data Collection Method by dataset** \u003cbr\u003e\n\n* Human \u003cbr\u003e\n\n**Labeling Method by dataset**\u003cbr\u003e\n\n* Hybrid: Human, Synthetic \u003cbr\u003e \n\n**Properties:**\n\nA dynamic blend of public and internal proprietary and customer datasets aligning text with human audio data.\n\n## Inference\n\n**Acceleration Engine:** Triton \u003cbr\u003e\n**Test Hardware:** \u003cbr\u003e\n* NVIDIA A10 \n* NVIDIA A100 \n* NVIDIA A30 \n* NVIDIA H100 \n* NVIDIA L4 \n* NVIDIA L40 \n* NVIDIA Turing T4\n* NVIDIA Volta V100\n\n## Get Help\n\n### Enterprise Support\n\nGet access to knowledge base articles and support cases or [submit a ticket](https://www.nvidia.com/en-us/data-center/products/ai-enterprise-suite/support/).\n\n## Ethical Considerations\n\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  \n\nFor more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety \u0026 Security, and Privacy Subcards.  \n\nPlease report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/)."])</script><script>self.__next_f.push([1,"ab:Tc68,"])</script><script>self.__next_f.push([1,"Field                                                                                                  |  Response\n:------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------\nIntended Applications \u0026 Domains:                                                                       |  FastConformer-Encoder Transformer-Decoder Model for Automatic Speech Recognition (ASR) and Translation (AST)\nModel Type:                                                                                            |  Speech Recognition and Speech Translation\nIntended Users:                                                                                        |  This model is intended for developers and data scientists who are building interactive call-center applications, virtual assistants, and language-learning assistants.\nOutput:                                                                                                |  Text Sequence\nDescribe how the model works:                                                                          |  Model transcribes audio input into text for the input language or translates audio input into target language based on  probability of next word and audio input.\nName the adversely impacted groups this has been tested to deliver comparable outcomes regardless of:  |  Age, Gender, National Origin\nTechnical Limitations:                                                                                 |  Transcripts may not be 100% accurate. \u003cbr\u003e  Translations may be technically correct but lack fluency. \u003cbr\u003e  Translations may show bias towards grammatical gender in some European languages. \u003cbr\u003e  Translations may show bias towards formal-informal tense in Korean and Japanese. \u003cbr\u003e  Dialects not represented in training will likely see poor performance on both ASR and AST.\nVerified to have met prescribed NVIDIA quality standards:                                              |  Yes\nPerformance Metrics:                                                                                   |  Word Error Rate (WER) of the ASR transcription. Bilingual Evaluation Understudy (BLEU) of the AST translation.\nPotential Known Risks:                                                                                 |  If an audio sample occurs outside the domain of training, there is a higher likelihood of mistranslation/poor transcription. Dialects not represented in training will likely see poor performance on both ASR and AST.\nLicensing:                                                                                             |  The NIM container is governed by the [NVIDIA Software License Agreement](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/) and [Product-Specific Terms for AI Products.](https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/) Use of this model is governed by the [NVIDIA Community Model License.](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/)"])</script><script>self.__next_f.push([1,"ac:T505,Field                                                                                                                              |  Response\n:----------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------\nGeneratable or reverse engineerable personal data?                                                                                 |  No\nPersonal data used to create this model?                                                                                           |  No\nHow often is dataset reviewed?                                                                                                     |  Before Release\nIs there provenance for all datasets used in training?                                                                             |  Yes\nIs data compliant with data subject requests for data correction or removal, if such a request was made?                           |  The data is compliant where applicable, but is not applicable for all data.\nApplicable Privacy Policy                                                                                                          | https://www.nvidia.com/en-us/about-nvidia/privacy-policy/ad:T447,Field                                               |  Response\n:---------------------------------------------------|:----------------------------------\nModel Application Field(s):                         |  Automatic Speech Recognition \u0026 Translation\nDescribe the life-critical impacts (if present).    |  Not Applicable\nUse Case Restriction(s):                            |  Abide by the [NVIDIA Software License Agreement](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/) and [Product-Specific Terms for AI Products](https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/). Use of this model is governed by the [NVIDIA Community Model License](https://w"])</script><script>self.__next_f.push([1,"ww.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/).\nDescribe access restrictions (if any):              | The Principle of Least Privilege (PoLP) is applied limiting access for dataset generation and model development. Restrictions enforce dataset access during training and dataset license constraints adhered to.ae:T9a8,"])</script><script>self.__next_f.push([1,"### Getting Started\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Instructions below demonstrate usage of \u003c%- name %\u003e model using Python gRPC client.\n\n### Prerequisites\n\nYou will need a system with Git and Python 3+ installed.\n\n### Install Riva Python Client\n\n```bash\npip install nvidia-riva-client\n```\n\n### Download Python Client\n\nDownload Python client code by cloning \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e.\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\n### Run Python Client\n\nMake sure you have a speech file in Mono, 16-bit audio in WAV, OPUS and FLAC formats. If you have generated the API key, it will be auto-populated in the command. Open a command terminal and execute below command to transcribe audio. If you know the source language, it is recommended to pass `source_language` in custom configuration parameter.\n\nBelow command demonstrates transcription of English audio file.\n\n```bash\npython python-clients/scripts/asr/transcribe_file_offline.py \\\n    --server grpc.nvcf.nvidia.com:443 --use-ssl \\\n    --metadata function-id \"\u003c%- nvcfFunctionId %\u003e\" \\\n    --metadata \"authorization\" \"Bearer \u003c%- apiKey %\u003e\" \\\n    --language-code en-US \\\n    --input-file \u003cpath_to_audio_file\u003e\n```\n\nBelow command demonstrates translation from English audio to Hindi.\n\n```bash\npython python-clients/scripts/asr/transcribe_file_offline.py \\\n    --server grpc.nvcf.nvidia.com:443 --use-ssl \\\n    --metadata function-id \"\u003c%- nvcfFunctionId %\u003e\" \\\n    --metadata \"authorization\" \"Bearer \u003c%- apiKey %\u003e\" \\\n    --language-code en-US \\\n    --custom-configuration \"target_language:hi-IN,task:translate\" \\\n    --input-file \u003cpath_to_audio_file\u003e\n```\n\nOne can transcribe and translate supported languages by changing the source language via `--language-code` and target language via `target_language` parameter.\n\n### Support for gRPC clients in other programming languages\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Proto files can be downloaded from \u003ca href=\"https://github.com/nvidia-riva/common/archive/refs/heads/main.zip\"\u003eRiva gRPC Proto files\u003c/a\u003e and compiled to target language using \u003ca href=\"https://grpc.io/docs/protoc-installation/\"\u003eProtoc compiler\u003c/a\u003e. Example Riva clients in C++ and Python languages are provided below.\n\n* \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e\n* \u003ca href=\"https://github.com/nvidia-riva/cpp-clients\"\u003eC++ Client Repository\u003c/a\u003e\n"])</script><script>self.__next_f.push([1,"af:T6ec,## Step 1. Generate API Key\n\n::generate-api-key\n\n## Step 2. Pull and Run the NIM\n\n```bash\n$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: \u003cPASTE_API_KEY_HERE\u003e\n```\n\nRefer to [Supported Models](https://docs.nvidia.com/nim/riva/asr/latest/getting-started.html#supported-models) for full list of models.\n\n```bash\nexport NGC_API_KEY=\u003cPASTE_API_KEY_HERE\u003e\n\ndocker run -it --rm --name=canary-1b \\\n   --runtime=nvidia \\\n   --gpus '\"device=0\"' \\\n   --shm-size=8GB \\\n   -e NGC_API_KEY \\\n   -e NIM_HTTP_API_PORT=9000 \\\n   -e NIM_GRPC_API_PORT=50051 \\\n   -p 9000:9000 \\\n   -p 50051:50051 \\\n   -e NIM_TAGS_SELECTOR=name=canary-1b \\\n   nvcr.io/nim/nvidia/canary-1b:latest\n```\n\nIt may take a up to 30 minutes depending on your network speed, for the container to be ready and start accepting requests from the time the docker container is started.\n\n## Step 3. Test the NIM\n\nOpen a new terminal and run following command to check if the service is ready to handle inference requests\n\n```bash\ncurl -X 'GET' 'http://localhost:9000/v1/health/ready'\n```\n\nIf the service is ready, you get a response similar to the following.\n```bash\n{\"ready\":true}\n```\n\nInstall the Riva Python client package\n\n```bash\nsudo apt-get install python3-pip\npip install nvidia-riva-client\n```\n\nDownload Riva sample clients\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\nRun Speech to Text inference in offline modes. Riva ASR supports Mono, 16-bit audio in WAV, OPUS and FLAC formats.\n\n```bash\npython3 python-clients/scripts/asr/transcribe_file_offline.py --server 0.0.0.0:50051 --input-file \u003cpath_to_speech_file\u003e --language-code en-US\n```\n\n\nFor more details on getting started with this NIM, visit the [Riva ASR NIM Docs](https://docs.nvidia.com/nim/riva/asr/latest/overview.html).\nb0:Tdb8,"])</script><script>self.__next_f.push([1,"# Model Overview\n\n## Description:\nThis model is used to transcribe short-form audio files and is designed to be compatible with *OpenAI's sequential long-form transcription algorithm*. Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labeled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning. Whisper-large-v3 is one of the 5 configurations of the model with 1550M parameters.\u003cbr\u003e\nThis model version is optimized to run with NVIDIA TensorRT-LLM. This model is ready for commercial use.\n\n\n## Third-Party Community Consideration\nThis model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see the Whisper Model Card on GitHub.(https://github.com/openai/whisper/blob/main/model-card.md).\n\n### License/Terms of Use: \n\nGOVERNING TERMS: Use of this model is governed by the [NVIDIA Community Model License Agreement](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/).\nADDITIONAL INFORMATION: [MIT License](https://github.com/openai/whisper/blob/main/LICENSE).\n\n\n## References:\nWhisper [website](https://openai.com/index/whisper/) \u003cbr\u003e\nWhisper paper: \u003cbr\u003e\n```\n@misc{radford2022robust,\n      title={Robust Speech Recognition via Large-Scale Weak Supervision}, \n      author={Alec Radford and Jong Wook Kim and Tao Xu and Greg Brockman and Christine McLeavey and Ilya Sutskever},\n      year={2022},\n      eprint={2212.04356},\n      archivePrefix={arXiv},\n      primaryClass={eess.AS}\n}\n```\n\n## Model Architecture: \n**Architecture Type:** Transformer (Encoder-Decoder) \u003cbr\u003e\n**Network Architecture:** Whisper \u003cbr\u003e\n\n\n## Input: \n**Input Type(s):** Audio, Text-Prompt \u003cbr\u003e\n**Input Format(s):** Linear PCM 16-bit 1 channel (Audio), String (Text Prompt) \u003cbr\u003e\n**Input Parameters:** One-Dimensional (1D) \u003cbr\u003e\n\n## Output:\n**Output Type(s):** Text\n**Output Format:** String\n**Output Parameters:** 1D\n\n\n**Supported Hardware Microarchitecture Compatibility:** \u003cbr\u003e\n* NVIDIA Ampere \u003cbr\u003e\n* NVIDIA Blackwell \u003cbr\u003e\n\n\n**Supported Operating System(s):** \u003cbr\u003e\n* Linux \u003cbr\u003e\n\n## Model Version(s): \n**Large-v3:** Whisper large-v3 has the same architecture as the previous large and large-v2 models, except for the following minor differences:\n- The spectrogram input uses 128 Mel frequency bins instead of 80.\n- A new language token for Cantonese.\n\n## Training Dataset:\n\n**Data Collection Method by dataset:** [Hybrid: human, automatic] \u003cbr\u003e\n\n**Labeling Method by dataset:** [Automated] \u003cbr\u003e\n\n**Dataset License(s):** NA\n\n\n## Inference:\n**Engine:** Tensor(RT)-LLM, Triton \u003cbr\u003e\n**Test Hardware:**\n- A100\n- H100\n\nFor more detail on model usage, evaluation, training data set and implications, please refer to [Whisper Model Card](https://github.com/openai/whisper/blob/main/model-card.md).\n\n## Ethical Considerations:\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  \nPlease report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/)."])</script><script>self.__next_f.push([1,"b1:T985,"])</script><script>self.__next_f.push([1,"### Getting Started\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Instructions below demonstrate usage of \u003c%- name %\u003e model using Python gRPC client.\n\n### Prerequisites\n\nYou will need a system with Git and Python 3+ installed.\n\n### Install Riva Python Client\n\n```bash\npip install nvidia-riva-client\n```\n\n### Download Python Client\n\nDownload Python client code by cloning \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e.\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\n### Run Python Client\n\nMake sure you have a speech file in Mono, 16-bit audio in WAV, OPUS and FLAC formats. If you have generated the API key, it will be auto-populated in the command. Open a command terminal and execute below command to transcribe audio. Specifying `--language-code` as `multi` will enable auto language detection. If you know the source language, it is recommended to specify for better accuracy and latency. See \u003ca href=\"https://github.com/openai/whisper/blob/main/whisper/tokenizer.py#L10\"\u003eSupported Languages\u003c/a\u003e for the list of all available languages and corresponding code.\n\n```bash\npython python-clients/scripts/asr/transcribe_file_offline.py \\\n    --server grpc.nvcf.nvidia.com:443 --use-ssl \\\n    --metadata function-id \"\u003c%- nvcfFunctionId %\u003e\" \\\n    --metadata \"authorization\" \"Bearer \u003c%- apiKey %\u003e\" \\\n    --language-code en \\\n    --input-file \u003cpath_to_audio_file\u003e\n```\n\nBelow command demonstrates translation from French (fr) to English.\n\n```bash\npython python-clients/scripts/asr/transcribe_file_offline.py \\\n    --server grpc.nvcf.nvidia.com:443 --use-ssl \\\n    --metadata function-id \"\u003c%- nvcfFunctionId %\u003e\" \\\n    --metadata \"authorization\" \"Bearer \u003c%- apiKey %\u003e\" \\\n    --language-code fr \\\n    --custom-configuration \"task:translate\" \\\n    --input-file \u003cpath_to_audio_file\u003e\n```\n\n### Support for gRPC clients in other languages\n\nRiva uses \u003ca href=\"https://grpc.io/\"\u003egRPC\u003c/a\u003e APIs. Proto files can be downloaded from \u003ca href=\"https://github.com/nvidia-riva/common/archive/refs/heads/main.zip\"\u003eRiva gRPC Proto files\u003c/a\u003e and compiled to target language using \u003ca href=\"https://grpc.io/docs/protoc-installation/\"\u003eProtoc compiler\u003c/a\u003e. Example Riva clients in C++ and Python languages are provided below.\n\n* \u003ca href=\"https://github.com/nvidia-riva/python-clients\"\u003ePython Client Repository\u003c/a\u003e\n* \u003ca href=\"https://github.com/nvidia-riva/cpp-clients\"\u003eC++ Client Repository\u003c/a\u003e\n"])</script><script>self.__next_f.push([1,"b2:T77c,## Step 1. Generate API Key\n\n::generate-api-key\n\n## Step 2. Pull and Run the NIM\n\n```bash\n$ docker login nvcr.io\nUsername: $oauthtoken\nPassword: \u003cPASTE_API_KEY_HERE\u003e\n```\n\nLaunch the Riva ASR NIM with Whisper Large v3 multilingual model with the command below. \nRefer [Supported Models](https://docs.nvidia.com/nim/riva/asr/latest/getting-started.html#supported-models) for full list of models.\n\n```bash\nexport NGC_API_KEY=\u003cPASTE_API_KEY_HERE\u003e\n\ndocker run -it --rm --name=whisper-large-v3 \\\n   --runtime=nvidia \\\n   --gpus '\"device=0\"' \\\n   --shm-size=8GB \\\n   -e NGC_API_KEY \\\n   -e NIM_HTTP_API_PORT=9000 \\\n   -e NIM_GRPC_API_PORT=50051 \\\n   -p 9000:9000 \\\n   -p 50051:50051 \\\n   -e NIM_TAGS_SELECTOR=name=whisper-large-v3 \\\n   nvcr.io/nim/nvidia/whisper-large-v3:latest\n```\n\n```{note}\nIt may take a up to 30 minutes depending on your network speed, for the container to be ready and start accepting requests from the time the docker container is started.\n```\n\n## Step 3. Test the NIM\n\nOpen a new terminal and run following command to check if the service is ready to handle inference requests\n\n```bash\ncurl -X 'GET' 'http://localhost:9000/v1/health/ready'\n```\n\nIf the service is ready, you get a response similar to the following.\n```bash\n{\"ready\":true}\n```\n\nInstall the Riva Python client package\n\n```bash\nsudo apt-get install python3-pip\npip install -U nvidia-riva-client\n```\n\nDownload Riva sample clients\n\n```bash\ngit clone https://github.com/nvidia-riva/python-clients.git\n```\n\nRun Speech to Text inference in offline mode. Riva ASR supports Mono, 16-bit audio in WAV, OPUS and FLAC formats.\n\n```bash\npython3 python-clients/scripts/asr/transcribe_file_offline.py --server 0.0.0.0:50051 \\\n   --language-code \u003cBCP-47 language code\u003e --input-file \u003cpath_to_speech_file\u003e\n```\n\nFor more details on getting started with this NIM, visit the [NVIDIA NIM Docs](https://docs.nvidia.com/nim/riva/asr/latest/overview.html).\n\nb3:{\"name\":\"riva-translate-4b-instruct-v1_1\",\"type\":\"model\"}\nb4:{\"name\":\"riva-translate-1_6b\",\"type\":\"model\"}\nb5:{\"name\":\"ca"])</script><script>self.__next_f.push([1,"nary-1b-asr\",\"type\":\"model\"}\nb6:{\"name\":\"canary-0-6b-turbo-asr\",\"type\":\"model\"}\nb7:{\"name\":\"whisper-large-v3\",\"type\":\"model\"}\n"])</script><script>self.__next_f.push([1,"35:[\"$\",\"$L4e\",null,{\"data\":[{\"endpoint\":{\"artifact\":{\"artifactType\":\"ENDPOINT\",\"name\":\"riva-translate-4b-instruct-v1_1\",\"displayName\":\"riva-translate-4b-instruct-v1_1\",\"publisher\":\"nvidia\",\"shortDescription\":\"Translation model in 12 languages with few-shots example prompts capability.\",\"logo\":\"https://assets.ngc.nvidia.com/products/api-catalog/images/riva-translate-4b-instruct.jpg\",\"labels\":[\"neural machine translation\",\"nvidia nim\",\"Text Translation\"],\"attributes\":[{\"key\":\"AVAILABLE\",\"value\":\"false\"},{\"key\":\"PREVIEW\",\"value\":\"true\"}],\"updatedDate\":\"2025-12-12T18:02:51.590Z\",\"bias\":\"Field                                                                                               |  Response\\n:---------------------------------------------------------------------------------------------------|:---------------\\nParticipation considerations from adversely impacted groups [protected classes](https://www.senate.ca.gov/content/protected-classes) in model design and testing:  |  None\\nBias Metric (If Measured):                                                   |  BLEU and COMET score\\nMeasures taken to mitigate against unwanted bias:                                                   |  We balanced the number of training data in each language domain.\",\"canGuestDownload\":true,\"createdDate\":\"2025-12-12T18:02:51.590Z\",\"description\":\"$9e\",\"explainability\":\"$9f\",\"isPublic\":true,\"isReadOnly\":true,\"orgName\":\"qc69jvmznzxy\",\"privacy\":\"$a0\"},\"requestStatus\":{\"statusCode\":\"SUCCESS\",\"requestId\":\"95e550e8-2aa1-4e49-9ffb-d8b9f96024af\"}},\"spec\":{\"openAPISpec\":{\"openapi\":\"3.1.0\",\"info\":{\"title\":\"NVIDIA NIM API for nvidia/riva-translate-4b-instruct-v1.1\",\"description\":\"The NVIDIA NIM REST API. Please see https://docs.api.nvidia.com/nim/reference/nvidia-riva-translate-4b-instruct-v1.1 for more details.\",\"version\":\"1.0.0\",\"termsOfService\":\"https://www.nvidia.com/en-us/data-center/products/nvidia-ai-enterprise/eula/\",\"contact\":{\"name\":\"NVIDIA Enterprise Support\",\"url\":\"https://www.nvidia.com/en-us/support/enterprise/\"},\"license\":{\"name\":\"NVIDIA Open Model License Agreement\",\"url\":\"https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/\"}},\"servers\":[{\"url\":\"https://integrate.api.nvidia.com/v1/\"}],\"paths\":{\"/chat/completions\":{\"post\":{\"operationId\":\"create_chat_completion_v1_chat_completions_post\",\"tags\":[\"Chat\"],\"summary\":\"Creates a model response for the given chat conversation.\",\"description\":\"Given a list of messages comprising a conversation, the model will return a response. Compatible with OpenAI. See https://platform.openai.com/docs/api-reference/chat/create\",\"requestBody\":{\"content\":{\"application/json\":{\"schema\":{\"$ref\":\"#/components/schemas/ChatRequest\"}}},\"required\":true},\"responses\":{\"200\":{\"description\":\"Invocation is fulfilled\",\"content\":{\"application/json\":{\"schema\":{\"$ref\":\"#/components/schemas/ChatCompletion\"}},\"text/event-stream\":{\"schema\":{\"$ref\":\"#/components/schemas/ChatCompletionChunk\"}}}},\"202\":{\"description\":\"Result is pending. Client should poll using the requestId.\\n\",\"content\":{\"application/json\":{\"example\":{},\"schema\":{}}},\"headers\":{\"NVCF-REQID\":{\"description\":\"requestId required for pooling\",\"schema\":{\"type\":\"string\",\"format\":\"uuid\"}},\"NVCF-STATUS\":{\"description\":\"Invocation status\",\"schema\":{\"type\":\"string\"}}}},\"422\":{\"description\":\"Validation failed, provided entity could not be processed.\",\"content\":{\"application/json\":{\"schema\":{\"$ref\":\"#/components/schemas/Errors\"},\"example\":{\"type\":\"urn:nvcf-worker-service:problem-details:unprocessable-entity\",\"title\":\"Unprocessable Entity\",\"status\":422,\"detail\":\"string\",\"instance\":\"/v2/nvcf/pexec/functions/4a58c6cb-a9b4-4014-99de-3e704d4ae687\",\"requestId\":\"3fa85f64-5717-4562-b3fc-2c963f66afa6\"}}}},\"500\":{\"description\":\"The invocation ended with an error.\",\"content\":{\"application/json\":{\"schema\":{\"$ref\":\"#/components/schemas/Errors\"},\"example\":{\"type\":\"urn:nvcf-worker-service:problem-details:internal-server-error\",\"title\":\"Internal Server Error\",\"status\":500,\"detail\":\"string\",\"instance\":\"/v2/nvcf/pexec/functions/4a58c6cb-a9b4-4014-99de-3e704d4ae687\",\"requestId\":\"3fa85f64-5717-4562-b3fc-2c963f66afa6\"}}}}},\"x-nvai-meta\":{\"name\":\"Create chat completion\",\"returns\":\"Returns a [chat completion](/docs/api-reference/chat/object) object, or a streamed sequence of [chat completion chunk](/docs/api-reference/chat/streaming) objects if the request is streamed.\\n\",\"path\":\"create\",\"examples\":[{\"name\":\"Translate from English to simplified Chinese: The GRACE mission is a collaboration between the NASA and German Aerospace Center.\",\"requestJson\":\"{\\n      \\\"model\\\": \\\"nvidia/riva-translate-4b-instruct-v1.1\\\",\\n      \\\"messages\\\": [\\n        {\\n          \\\"role\\\": \\\"system\\\",\\n          \\\"content\\\": \\\"en-zh-cn\\\"\\n        },\\n        {\\n          \\\"role\\\": \\\"user\\\",\\n          \\\"content\\\": \\\"The GRACE mission is a collaboration between the NASA and German Aerospace Center.?\\\"\\n        }\\n      ],\\n      \\\"temperature\\\": 0.0,\\n      \\\"max_tokens\\\": 128,\\n      \\\"seed\\\": 42,\\n      \\\"stream\\\": true\\n}\\n\",\"responseJson\":\"{\\n  \\\"id\\\": \\\"id-123\\\",\\n  \\\"object\\\": \\\"chat.completion\\\",\\n  \\\"created\\\": 1677652288,\\n  \\\"model\\\": \\\"nvidia/riva-translate-4b-instruct-v1.1\\\",\\n  \\\"system_fingerprint\\\": \\\"fp_44709d6fcb\\\",\\n  \\\"choices\\\": [{\\n    \\\"index\\\": 0,\\n    \\\"message\\\": {\\n      \\\"role\\\": \\\"assistant\\\",\\n      \\\"content\\\": \\\"GRACE任务是美国宇航局和德国航空航天中心之间的合作项目。\\\"\\n    },\\n    \\\"finish_reason\\\": \\\"stop\\\"\\n  }],\\n  \\\"usage\\\": {\\n    \\\"prompt_tokens\\\": 42,\\n    \\\"completion_tokens\\\": 18,\\n    \\\"total_tokens\\\": 60\\n  }\\n}\\n\"},{\"name\":\"Translate from Latin American Spanish to English: La tecnología de inteligencia artificial está cambiando nuestra forma de vida y trabajo.\",\"requestJson\":\"{\\n      \\\"model\\\": \\\"nvidia/riva-translate-4b-instruct-v1.1\\\",\\n      \\\"messages\\\": [\\n        {\\n          \\\"role\\\": \\\"system\\\",\\n          \\\"content\\\": \\\"es-us-en\\\"\\n        },\\n        {\\n          \\\"role\\\": \\\"user\\\",\\n          \\\"content\\\": \\\"La tecnología de inteligencia artificial está cambiando nuestra forma de vida y trabajo.\\\"\\n        }\\n      ],\\n      \\\"temperature\\\": 0.0,\\n      \\\"max_tokens\\\": 128,\\n      \\\"seed\\\": 42,\\n      \\\"stream\\\": true\\n}\\n\",\"responseJson\":\"{\\n  \\\"id\\\": \\\"id-123\\\",\\n  \\\"object\\\": \\\"chat.completion\\\",\\n  \\\"created\\\": 1677652288,\\n  \\\"model\\\": \\\"nvidia/riva-translate-4b-instruct-v1.1\\\",\\n  \\\"system_fingerprint\\\": \\\"fp_44709d6fcb\\\",\\n  \\\"choices\\\": [{\\n    \\\"index\\\": 0,\\n    \\\"message\\\": {\\n      \\\"role\\\": \\\"assistant\\\",\\n      \\\"content\\\": \\\"Artificial intelligence technology is changing our way of life and work.\\\"\\n    },\\n    \\\"finish_reason\\\": \\\"stop\\\"\\n  }],\\n  \\\"usage\\\": {\\n    \\\"prompt_tokens\\\": 45,\\n    \\\"completion_tokens\\\": 12,\\n    \\\"total_tokens\\\": 57\\n  }\\n}\\n\"}],\"templates\":[{\"title\":\"No Streaming\",\"requestEjs\":{\"python\":\"$a1\",\"langChain\":\"from langchain_nvidia_ai_endpoints import ChatNVIDIA\\n\\nclient = ChatNVIDIA(\\n  model=\\\"\u003c%- request.model %\u003e\\\",\\n  api_key=\\\"$NVIDIA_API_KEY\\\", \\n  temperature=\u003c%- request.temperature %\u003e,\\n  top_p=\u003c%- request.top_p %\u003e,\\n  max_tokens=\u003c%- request.max_tokens %\u003e,\\n)\\n\u003c% if (request.stream) { %\u003e\\nfor chunk in client.stream(\u003c%- JSON.stringify(request.messages) %\u003e): \\n  print(chunk.content, end=\\\"\\\")\\n\u003c% } else { %\u003e\\nresponse = client.invoke(\u003c%- JSON.stringify(request.messages) %\u003e)\\nprint(response.content)\\n\u003c% } %\u003e\\n  \\n\",\"node.js\":\"$a2\",\"curl\":\"$a3\"},\"response\":\"{\\n  \\\"id\\\": \\\"chatcmpl-123\\\",\\n  \\\"object\\\": \\\"chat.completion\\\",\\n  \\\"created\\\": 1677652288,\\n  \\\"model\\\": \\\"nvidia/riva-translate-4b-instruct-v1.1\\\",\\n  \\\"system_fingerprint\\\": \\\"fp_44709d6fcb\\\",\\n  \\\"choices\\\": [{\\n    \\\"index\\\": 0,\\n    \\\"message\\\": {\\n      \\\"role\\\": \\\"assistant\\\",\\n      \\\"content\\\": \\\"\\\\n\\\\nHello there, how may I assist you today?\\\",\\n    },\\n    \\\"finish_reason\\\": \\\"stop\\\"\\n  }],\\n  \\\"usage\\\": {\\n    \\\"prompt_tokens\\\": 9,\\n    \\\"completion_tokens\\\": 12,\\n    \\\"total_tokens\\\": 21\\n  }\\n}\\n\"}]}}}},\"security\":[{\"Token\":[]}],\"components\":{\"securitySchemes\":{\"Token\":{\"type\":\"http\",\"scheme\":\"bearer\"}},\"schemas\":{\"Errors\":{\"properties\":{\"type\":{\"type\":\"string\",\"description\":\"Error type\"},\"title\":{\"type\":\"string\",\"description\":\"Error title\"},\"status\":{\"type\":\"integer\",\"description\":\"Error status code\"},\"detail\":{\"type\":\"string\",\"description\":\"Detailed information about the error\"},\"instance\":{\"type\":\"string\",\"description\":\"Function instance used to invoke the request\"},\"requestId\":{\"type\":\"string\",\"format\":\"uuid\",\"description\":\"UUID of the request\"}},\"type\":\"object\",\"required\":[\"type\",\"title\",\"status\",\"detail\",\"instance\",\"requestId\"],\"title\":\"InvokeError\"},\"ChatCompletion\":{\"properties\":{\"id\":{\"description\":\"A unique identifier for the completion.\",\"format\":\"uuid\",\"title\":\"Id\",\"type\":\"string\"},\"choices\":{\"description\":\"The list of completion choices the model generated for the input prompt.\",\"items\":{\"$ref\":\"#/components/schemas/Choice\"},\"title\":\"Choices\",\"type\":\"array\"},\"usage\":{\"allOf\":[{\"$ref\":\"#/components/schemas/Usage\"}],\"description\":\"Usage statistics for the completion request.\"}},\"required\":[\"id\",\"choices\",\"usage\"],\"title\":\"ChatCompletion\",\"type\":\"object\"},\"ChatCompletionChunk\":{\"properties\":{\"id\":{\"description\":\"A unique identifier for the completion.\",\"format\":\"uuid\",\"title\":\"Id\",\"type\":\"string\"},\"choices\":{\"description\":\"The list of completion choices the model generated for the input prompt.\",\"items\":{\"$ref\":\"#/components/schemas/ChoiceChunk\"},\"title\":\"Choices\",\"type\":\"array\"}},\"required\":[\"id\",\"choices\"],\"title\":\"ChatCompletionChunk\",\"type\":\"object\"},\"ChatRequest\":{\"additionalProperties\":false,\"properties\":{\"model\":{\"type\":\"string\",\"title\":\"Model\",\"default\":\"nvidia/riva-translate-4b-instruct-v1.1\"},\"messages\":{\"description\":\"A list of messages comprising the conversation so far. The roles of the messages must be alternating between `user` and `assistant`. The last input message should have role `user`. A message with the the `system` role is optional, and must be the very first message if it is present; `context` is also optional, but must come before a user question.\",\"examples\":[[{\"content\":\"I am going to Paris, what should I see?\",\"role\":\"user\"}]],\"items\":{\"$ref\":\"#/components/schemas/Message\"},\"title\":\"Messages\",\"type\":\"array\"},\"temperature\":{\"default\":0,\"description\":\"The sampling temperature to use for text generation. The higher the temperature value is, the less deterministic the output text will be. It is not recommended to modify both temperature and top_p in the same call.\",\"maximum\":1,\"minimum\":0,\"title\":\"Temperature\",\"type\":\"number\"},\"top_p\":{\"default\":0.9,\"description\":\"The top-p sampling mass used for text generation. The top-p value determines the probability mass that is sampled at sampling time. For example, if top_p = 0.2, only the most likely tokens (summing to 0.2 cumulative probability) will be sampled. It is not recommended to modify both temperature and top_p in the same call.\",\"maximum\":1,\"exclusiveMinimum\":0,\"title\":\"Top P\",\"type\":\"number\"},\"frequency_penalty\":{\"type\":\"number\",\"maximum\":2,\"minimum\":-2,\"default\":0,\"title\":\"Frequency Penalty\",\"description\":\"Indicates how much to penalize new tokens based on their existing frequency in the text so far, decreasing model likelihood to repeat the same line verbatim.\"},\"presence_penalty\":{\"type\":\"number\",\"maximum\":2,\"minimum\":-2,\"default\":0,\"title\":\"Presence Penalty\",\"description\":\"Positive values penalize new tokens based on whether they appear in the text so far, increasing model likelihood to talk about new topics.\"},\"max_tokens\":{\"default\":512,\"description\":\"The maximum number of tokens to generate in any given call. Note that the model is not aware of this value, and generation will simply stop at the number of tokens specified.\",\"maximum\":4096,\"minimum\":1,\"title\":\"Max Tokens\",\"type\":\"integer\"},\"stream\":{\"default\":false,\"description\":\"If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events (SSE) as they become available (JSON responses are prefixed by `data: `), with the stream terminated by a `data: [DONE]` message.\",\"title\":\"Stream\",\"type\":\"boolean\"},\"stop\":{\"anyOf\":[{\"items\":{\"type\":\"string\"},\"type\":\"array\"},{\"type\":\"string\"},{\"type\":\"null\"}],\"title\":\"Stop\",\"description\":\"A string or a list of strings where the API will stop generating further tokens. The returned text will not contain the stop sequence.\"}},\"required\":[\"messages\"],\"title\":\"ChatRequest\",\"type\":\"object\"},\"Choice\":{\"properties\":{\"index\":{\"description\":\"The index of the choice in the list of choices (always 0).\",\"title\":\"Index\",\"type\":\"integer\"},\"message\":{\"allOf\":[{\"$ref\":\"#/components/schemas/Message\"}],\"description\":\"A chat completion message generated by the model.\",\"examples\":[{\"content\":\"Ah, Paris, the City of Light! There are so many amazing things to see and do in this beautiful city ...\",\"role\":\"assistant\"}]},\"finish_reason\":{\"anyOf\":[{\"enum\":[\"stop\",\"length\"],\"type\":\"string\"},{\"type\":\"null\"}],\"default\":null,\"description\":\"The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, or `length` if the maximum number of tokens specified in the request was reached.\",\"examples\":[\"stop\"],\"title\":\"Finish Reason\"}},\"required\":[\"index\",\"message\"],\"title\":\"Choice\",\"type\":\"object\"},\"ChoiceChunk\":{\"properties\":{\"index\":{\"description\":\"The index of the choice in the list of choices (always 0).\",\"title\":\"Index\",\"type\":\"integer\"},\"delta\":{\"allOf\":[{\"$ref\":\"#/components/schemas/Message\"}],\"description\":\"A chat completion delta generated by streamed model responses.\",\"examples\":[{\"content\":\"Ah,\",\"role\":\"assistant\"}]},\"finish_reason\":{\"anyOf\":[{\"enum\":[\"stop\",\"length\"],\"type\":\"string\"},{\"type\":\"null\"}],\"default\":null,\"description\":\"The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, or `length` if the maximum number of tokens specified in the request was reached. Will be `null` if the model has not finished generating.\",\"title\":\"Finish Reason\"}},\"required\":[\"index\",\"delta\"],\"title\":\"ChoiceChunk\",\"type\":\"object\"},\"Message\":{\"additionalProperties\":false,\"properties\":{\"role\":{\"description\":\"The role of the message author.\",\"enum\":[\"system\",\"context\",\"user\",\"assistant\"],\"title\":\"Role\",\"type\":\"string\"},\"content\":{\"description\":\"The contents of the message.\",\"title\":\"Content\",\"anyOf\":[{\"type\":\"string\"},{\"type\":\"null\"}]}},\"required\":[\"role\",\"content\"],\"title\":\"Message\",\"type\":\"object\"},\"Usage\":{\"properties\":{\"completion_tokens\":{\"description\":\"Number of tokens in the generated completion.\",\"examples\":[25],\"title\":\"Completion Tokens\",\"type\":\"integer\"},\"prompt_tokens\":{\"description\":\"Number of tokens in the prompt.\",\"examples\":[9],\"title\":\"Prompt Tokens\",\"type\":\"integer\"},\"total_tokens\":{\"description\":\"Total number of tokens used in the request (prompt + completion).\",\"examples\":[34],\"title\":\"Total Tokens\",\"type\":\"integer\"}},\"required\":[\"completion_tokens\",\"prompt_tokens\",\"total_tokens\"],\"title\":\"Usage\",\"type\":\"object\"}}}},\"namespace\":\"qc69jvmznzxy\",\"updatedDate\":\"2025-12-13T22:00:27.303Z\",\"nvcfFunctionId\":\"b5cca41a-de17-4c7a-a6c8-03937cfb07b9\",\"createdDate\":\"2025-12-12T18:02:51.936Z\",\"attributes\":{\"apiDocsUrl\":\"https://docs.api.nvidia.com/nim/reference/nvidia-riva-translate-4b-instruct-v1_1\",\"termsOfUse\":\"\u003cb\u003eGOVERNING TERMS\u003c/b\u003e: This trial service is governed by the \u003ca href=\\\"https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA API Trial Terms of Service\u003c/a\u003e. Use of this model is governed by the \u003ca href=\\\"https://docs.nvidia.com/ai-foundation-models-community-license.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA Community Model License\u003c/a\u003e. Additional Information: \u003ca href=\\\"https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eApache 2.0\u003c/a\u003e.\\n\",\"showUnavailableBanner\":false,\"cta\":{\"text\":\"Apply to Self-Host\",\"url\":\"https://www.nvidia.com/en-us/ai/nim-notifyme/\"}},\"artifactName\":\"riva-translate-4b-instruct-v1_1\"},\"config\":{\"name\":\"riva-translate-4b-instruct-v1_1\",\"type\":\"model\"}},{\"endpoint\":{\"artifact\":{\"artifactType\":\"ENDPOINT\",\"name\":\"riva-translate-1_6b\",\"displayName\":\"riva-translate-1.6b\",\"publisher\":\"nvidia\",\"shortDescription\":\"Enable smooth global interactions in 36 languages.\",\"logo\":\"https://assets.ngc.nvidia.com/products/api-catalog/images/riva-translate-1_6b.jpg\",\"labels\":[\"NVIDIA NIM\",\"Neural machine translation\",\"Text Translation\"],\"attributes\":[{\"key\":\"AVAILABLE\",\"value\":\"true\"},{\"key\":\"PREVIEW\",\"value\":\"false\"}],\"updatedDate\":\"2025-06-26T19:40:20.620Z\",\"bias\":\"Field                                                                                               |  Response\\n:---------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------\\nParticipation considerations from adversely impacted groups ([protected classes](https://www.senate.ca.gov/content/protected-classes)) in model design and testing:  |  Not Applicable\\nMeasures taken to mitigate against unwanted bias:                                                   |    Sourced diverse dataset from East Asia, South Asia, Latin America, and North America annotated by different gender.\\\" See https://www.arxiv-vanity.com/papers/2106.03193/ for more details.\",\"canGuestDownload\":true,\"createdDate\":\"2025-06-26T19:40:20.620Z\",\"description\":\"$a4\",\"explainability\":\"$a5\",\"isPublic\":true,\"isReadOnly\":true,\"orgName\":\"qc69jvmznzxy\",\"privacy\":\"$a6\",\"safetyAndSecurity\":\"Field                                               |  Response\\n:---------------------------------------------------|:----------------------------------\\nModel Application(s):                               | Language Translation\\nDescribe the life-critical impacts (if present).   | Not Applicable\\nUse Case Restriction(s):                              | Abide by https://developer.nvidia.com/riva/ga/license\\nModel and Dataset Restriction(s):                       | The Principle of least privilege (PoLP) is applied limiting access for dataset generation and model development. Restrictions enforce dataset access during training, and dataset license constraints adhered to.\"},\"requestStatus\":{\"statusCode\":\"SUCCESS\",\"requestId\":\"472f41e1-f890-4c5a-acc5-63b6a8412d04\"}},\"spec\":{\"namespace\":\"qc69jvmznzxy\",\"updatedDate\":\"2025-07-30T19:23:51.548Z\",\"nvcfFunctionId\":\"0778f2eb-b64d-45e7-acae-7dd9b9b35b4d\",\"createdDate\":\"2025-06-26T19:40:20.900Z\",\"attributes\":{\"apiDocsUrl\":\"https://docs.nvidia.com/nim/riva/nmt/latest/protos.html\",\"termsOfUse\":\"\u003cb\u003eGOVERNING TERMS\u003c/b\u003e: Your use of this API is governed by the \u003ca href=\\\"https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA API Trial Service Terms of Use\u003c/a\u003e; and the use of this model is governed by the \u003ca href=\\\"https://docs.nvidia.com/ai-foundation-models-community-license.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA AI Foundation Models Community License\u003c/a\u003e.\\n\",\"showUnavailableBanner\":false,\"cta\":{\"text\":\"Run Anywhere - Notify Me\",\"url\":\"https://www.nvidia.com/en-us/ai/nim-notifyme/\"},\"usage\":\"$a7\",\"deploy\":[{\"label\":\"Linux with Docker\",\"filename\":\"linux.md\",\"contents\":\"$a8\"}]},\"artifactName\":\"riva-translate-1_6b\"},\"config\":{\"name\":\"riva-translate-1_6b\",\"type\":\"model\"}},{\"endpoint\":{\"artifact\":{\"artifactType\":\"ENDPOINT\",\"name\":\"canary-1b-asr\",\"displayName\":\"canary-1b-asr\",\"publisher\":\"nvidia\",\"shortDescription\":\"Multi-lingual model supporting speech-to-text recognition and translation.\",\"logo\":\"https://assets.ngc.nvidia.com/products/api-catalog/images/canary-1b-asr.jpg\",\"labels\":[\"Automatic Speech Recognition\",\"Automatic Speech Translation\",\"NVIDIA NIM\",\"NVIDIA Riva\"],\"attributes\":[{\"key\":\"AVAILABLE\",\"value\":\"true\"},{\"key\":\"PREVIEW\",\"value\":\"false\"}],\"updatedDate\":\"2025-04-10T15:18:38.670Z\",\"bias\":\"$a9\",\"canGuestDownload\":true,\"createdDate\":\"2025-02-18T18:49:02.046Z\",\"description\":\"$aa\",\"explainability\":\"$ab\",\"isPublic\":true,\"isReadOnly\":true,\"orgName\":\"qc69jvmznzxy\",\"privacy\":\"$ac\",\"safetyAndSecurity\":\"$ad\"},\"requestStatus\":{\"statusCode\":\"SUCCESS\",\"requestId\":\"cbff7125-a960-494d-a74d-27d03554f19c\"}},\"spec\":{\"namespace\":\"qc69jvmznzxy\",\"updatedDate\":\"2025-10-29T07:01:04.531Z\",\"nvcfFunctionId\":\"b0e8b4a5-217c-40b7-9b96-17d84e666317\",\"createdDate\":\"2025-02-18T18:49:02.357Z\",\"attributes\":{\"apiDocsUrl\":\"https://docs.nvidia.com/nim/riva/asr/latest/protos.html\",\"termsOfUse\":\"\u003cb\u003eGOVERNING TERMS\u003c/b\u003e: Your use of this API is governed by the \u003ca href=\\\"https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA API Trial Service Terms of Use\u003c/a\u003e; and the use of this model is governed by the \u003ca href=\\\"https://docs.nvidia.com/ai-foundation-models-community-license.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA AI Foundation Models Community License\u003c/a\u003e.\\n\",\"showUnavailableBanner\":false,\"cta\":{\"text\":\"Run Anywhere - Notify Me\",\"url\":\"https://www.nvidia.com/en-us/ai/nim-notifyme/\",\"nim_available_override_url\":\"https://catalog.ngc.nvidia.com/orgs/nim/teams/nvidia/containers/canary-1b\"},\"usage\":\"$ae\",\"deploy\":[{\"label\":\"Linux with Docker\",\"filename\":\"linux.md\",\"contents\":\"$af\"}]},\"artifactName\":\"canary-1b-asr\"},\"config\":{\"name\":\"canary-1b-asr\",\"type\":\"model\"}},{\"endpoint\":\"$undefined\",\"spec\":\"$undefined\",\"config\":{\"name\":\"canary-0-6b-turbo-asr\",\"type\":\"model\"}},{\"endpoint\":{\"artifact\":{\"artifactType\":\"ENDPOINT\",\"name\":\"whisper-large-v3\",\"displayName\":\"whisper-large-v3\",\"publisher\":\"openai\",\"shortDescription\":\"Robust Speech Recognition via Large-Scale Weak Supervision.\",\"logo\":\"https://assets.ngc.nvidia.com/products/api-catalog/images/whisper-large-v3.jpg\",\"labels\":[\"ASR\",\"AST\",\"Multilingual\",\"NVIDIA NIM\",\"NVIDIA Riva\",\"OpenAI\",\"batch\",\"Speech-to-Text\",\"whisper\"],\"attributes\":[{\"key\":\"AVAILABLE\",\"value\":\"true\"},{\"key\":\"PREVIEW\",\"value\":\"false\"}],\"updatedDate\":\"2025-04-10T15:18:41.346Z\",\"canGuestDownload\":true,\"createdDate\":\"2025-02-18T19:47:35.847Z\",\"description\":\"$b0\",\"isPublic\":true,\"isReadOnly\":true,\"orgName\":\"qc69jvmznzxy\"},\"requestStatus\":{\"statusCode\":\"SUCCESS\",\"requestId\":\"fdf944e0-b618-49c1-8cba-525241e41ce3\"}},\"spec\":{\"namespace\":\"qc69jvmznzxy\",\"updatedDate\":\"2025-08-01T11:46:35.143Z\",\"nvcfFunctionId\":\"b702f636-f60c-4a3d-a6f4-f3568c13bd7d\",\"createdDate\":\"2025-02-18T19:47:36.161Z\",\"attributes\":{\"apiDocsUrl\":\"https://docs.nvidia.com/nim/riva/asr/latest/protos.html\",\"termsOfUse\":\"\u003cb\u003eGOVERNING TERMS\u003c/b\u003e: Your use of this API is governed by the \u003ca href=\\\"https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA API Trial Service Terms of Use\u003c/a\u003e; and the use of this model is governed by the \u003ca href=\\\"https://docs.nvidia.com/ai-foundation-models-community-license.pdf\\\" rel=\\\"noreferrer\\\" target=\\\"_blank\\\"\u003eNVIDIA AI Foundation Models Community License\u003c/a\u003e.\\n\",\"showUnavailableBanner\":false,\"cta\":{\"text\":\"Run Anywhere - Notify Me\",\"url\":\"https://www.nvidia.com/en-us/ai/nim-notifyme/\",\"nim_available_override_url\":\"https://catalog.ngc.nvidia.com/orgs/nim/teams/nvidia/containers/riva-asr\"},\"usage\":\"$b1\",\"deploy\":[{\"label\":\"Linux with Docker\",\"filename\":\"linux.md\",\"contents\":\"$b2\"}]},\"artifactName\":\"whisper-large-v3\"},\"config\":{\"name\":\"whisper-large-v3\",\"type\":\"model\"}}],\"items\":[\"$b3\",\"$b4\",\"$b5\",\"$b6\",\"$b7\"],\"mode\":\"$undefined\",\"params\":\"$9d\",\"slotTitle\":[[\"$\",\"div\",null,{\"className\":\"mb-2 flex items-start gap-2 max-xs:justify-between\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-ml font-medium leading-body tracking-less text-manitoulinLightWhite mb-0\",\"children\":\"Neural Machine Translation (NMT) \u0026 Audio Speech Translation (AST)\"}],\"$undefined\"]}],[\"$\",\"p\",null,{\"className\":\"text-md font-normal text-manitoulinLightGray mb-0\",\"children\":\"Enable seamless multilingual global communication across dozens of languages with NVIDIA Nemotron Speech models.\"}],\" \"]}]\n"])</script><script>$RC("B:4","S:4")</script><noscript><img src="https://build.nvidia.com/akam/13/pixel_58fbb61e?a=dD02MzYzNjEyMzg4MjkwN2I0NjA0MmIxYTgxMzI3YTgzNzY0YTFjYWI4JmpzPW9mZg==" style="visibility: hidden; position: absolute; left: -999px; top: -999px;" /></noscript><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/launch-f1e5e96c44d5.min.js.下载" id="adobe-analytics" data-nscript="afterInteractive"></script><script async="true" id="OptanonWrapper" data-nscript="afterInteractive">function OptanonWrapper() {
                  const event = new Event('bannerLoaded');
                  window.dispatchEvent(event);
                }</script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/ot-custom.js.下载" async="true" id="GlobalPrivacyControl" data-nscript="afterInteractive"></script><next-route-announcer style="position: absolute;"><template shadowrootmode="open"><div aria-live="assertive" id="__next-route-announcer__" role="alert" style="position: absolute; border: 0px; height: 1px; margin: -1px; padding: 0px; width: 1px; clip: rect(0px, 0px, 0px, 0px); overflow: hidden; white-space: nowrap; overflow-wrap: normal;">Explore Speech Models | Try NVIDIA NIM APIs</div></template></next-route-announcer><script>
var script = document.createElement('script');
script.src = 'https://www.googletagmanager.com/gtag/js?id=AW-1041695361'; 
script.async = true;
document.body.appendChild(script);

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'AW-1041695361');
</script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/js" async=""></script><script>
_linkedin_partner_id = "84497";
window._linkedin_data_partner_ids = window._linkedin_data_partner_ids || [];
window._linkedin_data_partner_ids.push(_linkedin_partner_id);
var s = document.getElementsByTagName("script")[0];
var b = document.createElement("script");
b.type = "text/javascript";
b.async = true;
b.src = "https://snap.licdn.com/li.lms-analytics/insight.min.js";
s.parentNode.insertBefore(b, s);

</script><script>
!function(w,d){if(!w.rdt){var p=w.rdt=function()
{p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)}
;p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_fp03cfcwb8mz');rdt('track', 'PageVisit'); 
</script><script>
(function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:3655182,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><script>
 (function(w,d,t,r,u) { var f,n,i; w[u]=w[u]||[],f=function() { var o={ti:"56337120", enableAutoSpaTracking: true}; o.q=w[u],w[u]=new UET(o),w[u].push("pageLoad") }, n=d.createElement(t),n.src=r,n.async=1,n.onload=n.onreadystatechange=function() { var s=this.readyState; s&&s!=="loaded"&&s!=="complete"||(f(),n.onload=n.onreadystatechange=null) }, i=d.getElementsByTagName(t)[0],i.parentNode.insertBefore(n,i) }) (window,document,"script","//bat.bing.com/bat.js","uetq"); 
</script><script>
   !function(f,b,e,v,n,t,s)

  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?

  n.callMethod.apply(n,arguments):n.queue.push(arguments)};

  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';

  n.queue=[];t=b.createElement(e);t.async=!0;

  t.src=v;s=b.getElementsByTagName(e)[0];

  s.parentNode.insertBefore(t,s)}(window, document,'script',

  'https://connect.facebook.net/en_US/fbevents.js');

  fbq('init', '161755414605325');

  fbq('track', 'PageView');
</script><script>
var script = document.createElement('script');
script.src = 'https://www.googletagmanager.com/gtag/js?id=AW-974166441'; 
script.async = true;
document.body.appendChild(script);

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'AW-974166441');
</script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/js(1)" async=""></script><script>_satellite["_runScript1"](function(event, target, Promise) {
window.ClickOmniTrack=function(obj,eventVal,prefix,secName){  
  try
   { s.eVar42=s.prop42='';
      secName = prefix+secName+":"+s.pageName ;
     secName=secName.toLowerCase();
      s.events=eventVal;s.linkTrackEvents=eventVal;
      s.eVar10=s.prop10=secName;
      s.linkTrackVars='events,eVar10,prop10';
    if(_satellite.getVar('browserDoNotTrack')=="disabled"){
      s.tl(true,'o',secName); }
	    s.linkTrackVars="None"
	    s.linkTrackEvents="None"
	 }catch(error){}}

window.ClickOmniSearch=function(obj,eventVal,prefix,secName,userType){  
  try
   { s.eVar42=s.prop42='';
    	var srchKeyword=secName;
      secName = prefix+secName+":"+s.pageName ;
     secName=secName.toLowerCase();
      s.events=eventVal;s.linkTrackEvents=eventVal;
      s.eVar10=s.prop10=secName;
    	s.eVar20=s.prop20=srchKeyword;
    s.eVar166=userType;
      s.linkTrackVars='events,eVar10,prop10,eVar20,prop20,eVar166';
    	if(_satellite.getVar('browserDoNotTrack')=="disabled"){
      s.tl(true,'o',secName); }
	    s.linkTrackVars="None"
	    s.linkTrackEvents="None"
	 }catch(error){}}
window.ClickOmniPart=function(obj,eventVal,prefix,secName,partner){  
  try
   { s.eVar42=s.prop42=partner;
      secName = prefix+secName+":"+s.pageName ;
     secName=secName.toLowerCase();
      s.events=eventVal;s.linkTrackEvents=eventVal;
      s.eVar10=s.prop10=secName;
      s.eVar15=s.prop15=partner.toLowerCase().replace(/.+\/\/|www.|\..+/g, '');
      s.linkTrackVars='events,eVar10,prop10,prop15,eVar15,eVar42,prop42';
    	if(_satellite.getVar('browserDoNotTrack')=="disabled"){
      s.tl(true,'o',secName); }
	    s.linkTrackVars="None"
	    s.linkTrackEvents="None"
	 }catch(error){}}

window.ClickOmniTrackNav=function(obj,eventVal,prefix,secName){  
  try
   {  s.eVar42=s.prop42='';
     var secN=secName.lastIndexOf('/')+1;
      secName=secName.substring(secN);
      secName=secName.toLowerCase();
      secName=secName.replace(/\//g,':').replace('#','');
      s.eVar11=s.prop11=secName;
      secName = prefix+secName+":"+s.pageName;
      s.events=eventVal;s.linkTrackEvents=eventVal;
      s.eVar10=s.prop10=secName; 
      s.linkTrackVars='events,eVar10,prop10,prop11,eVar11';
    	//if(_satellite.getVar('browserDoNotTrack')=="disabled"){
        //console.log('Nav Do not track',_satellite.getVar('browserDoNotTrack'));
        s.tl(true,'o',secName); //}
	    s.linkTrackVars="None"
	    s.linkTrackEvents="None"
	 }catch(error){}}


window.ClickOmniProd=function(obj,eventVal,prefix,secName,pVal){  
try
   {   s.eVar42=s.prop42='';
     var secN=secName.lastIndexOf('/')+1;
      secName=secName.substring(secN);
      secName=secName.toLowerCase();
      secName=secName.replace(/\//g,':').replace('#','');
      secName = prefix+secName+":"+s.pageName;
      s.events=eventVal;s.linkTrackEvents=eventVal;
      s.eVar10=s.prop10=secName; 
      s.products=";"+pVal;
      s.linkTrackVars='events,eVar10,prop10,products';
    	if(_satellite.getVar('browserDoNotTrack')=="disabled"){
        s.tl(true,'o',secName); }
	    s.linkTrackVars="None"
	    s.linkTrackEvents="None"
	  }catch(error){}}


window.ClkTrkFl=function(obj,eventVal,secName){  
  try
   { s.eVar42=s.prop42='';
      secName = "filter:"+secName+":"+s.pageName ;
     secName=secName.toLowerCase();
      s.events=eventVal;s.linkTrackEvents=eventVal;
      s.eVar10=s.prop10=secName;
      s.linkTrackVars='events,eVar10,prop10';
    if(_satellite.getVar('browserDoNotTrack')=="disabled"){
      s.tl(true,'o',secName); }
	    s.linkTrackVars="None"
	    s.linkTrackEvents="None"
	 }catch(error){}}

window.ClickOmniTrackMUt=function(obj,eventVal,prefix,secName,modelName,userType){  
  try
   {
    if(typeof s!='undefined'){
      secName = prefix+secName+":"+s.pageName ;
      secName=secName.toLowerCase();
      s.events=eventVal;s.linkTrackEvents=eventVal;
      s.eVar10=s.prop10=secName;
	  s.eVar165=modelName;
      s.eVar166=userType;
	  s.linkTrackVars='events,eVar10,prop10,eVar165,eVar166';
      s.tl(obj,'o',secName); 
	    s.linkTrackVars="None"
	    s.linkTrackEvents="None"}
	 }catch(error){console.log("Bt DownTrack"+error)}}
window.ClickOmniTrackMn=function(obj,eventVal,prefix,secName,modelName){  
  try
   {
    if(typeof s!='undefined'){
      secName = prefix+secName+":"+s.pageName ;
      secName=secName.toLowerCase();
      s.events=eventVal;s.linkTrackEvents=eventVal;
      s.eVar10=s.prop10=secName;
	  s.eVar165=modelName;
	  s.linkTrackVars='events,eVar10,prop10,eVar165';
      s.tl(obj,'o',secName); 
	    s.linkTrackVars="None"
	    s.linkTrackEvents="None"}
	 }catch(error){console.log("Bt DownTrack"+error)}}
window.ClickOmniTrackUt=function(obj,eventVal,prefix,secName,userType){  
  try
   {
    if(typeof s!='undefined'){
      secName = prefix+secName+":"+s.pageName ;
      secName=secName.toLowerCase();
      s.events=eventVal;s.linkTrackEvents=eventVal;
      s.eVar10=s.prop10=secName;
	  s.eVar166=userType;
	  s.linkTrackVars='events,eVar10,prop10,eVar166';
      s.tl(obj,'o',secName); 
	    s.linkTrackVars="None"
	    s.linkTrackEvents="None"}
	 }catch(error){console.log("Bt DownTrack"+error)}}
});</script><div id="onetrust-consent-sdk" data-nosnippet="true"><div class="onetrust-pc-dark-filter ot-hide ot-fade-in"></div><div id="onetrust-pc-sdk" class="otPcCenter ot-hide ot-fade-in otRelFont" lang="en" aria-label="Preference center" role="region"><div role="dialog" aria-modal="true" style="height: 100%;" aria-label="Cookie Settings"><!-- Close Button --><div class="ot-pc-header"><!-- Logo Tag --><div class="ot-pc-logo"><img alt="Company Logo" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/logo_nvidia.png"></div><button id="close-pc-btn-handler" class="ot-close-icon" aria-label="Close preference center" style="background-image: url(&quot;https://cdn.cookielaw.org/logos/static/ot_close.svg&quot;);"></button></div><!-- Close Button --><div id="ot-pc-content" class="ot-pc-scrollbar"><div class="ot-optout-signal ot-hide"><div class="ot-optout-icon"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true"><path class="ot-floating-button__svg-fill" d="M14.588 0l.445.328c1.807 1.303 3.961 2.533 6.461 3.688 2.015.93 4.576 1.746 7.682 2.446 0 14.178-4.73 24.133-14.19 29.864l-.398.236C4.863 30.87 0 20.837 0 6.462c3.107-.7 5.668-1.516 7.682-2.446 2.709-1.251 5.01-2.59 6.906-4.016zm5.87 13.88a.75.75 0 00-.974.159l-5.475 6.625-3.005-2.997-.077-.067a.75.75 0 00-.983 1.13l4.172 4.16 6.525-7.895.06-.083a.75.75 0 00-.16-.973z" fill="#FFF" fill-rule="evenodd"></path></svg></div><span></span></div><p role="heading" aria-level="2" id="ot-pc-title">Cookie Settings</p><div id="ot-pc-desc">We and our third-party partners (including social media, advertising, and analytics partners) use cookies and other tracking technologies to collect, store, monitor, and process certain information about you when you visit our website. The information collected might relate to you, your preferences, or your device. We use that information to make the site work and with your consent to analyze performance and traffic on our website, to provide a more personalized web experience, and assist in our marketing efforts.<br><br>Click on the different category headings below to find out more and change the settings according to your preference. You cannot opt out of Required Cookies as they are deployed to ensure the proper functioning of our website (such as prompting the cookie banner and remembering your settings, etc.). By clicking "Save and Accept" at the bottom, you consent to the use of cookies and other tools as described in our <a href="https://www.nvidia.com/en-us/about-nvidia/cookie-policy/" target="_self">Cookie Policy</a> in accordance with your settings. For more information about our privacy practices, please see our <a href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" target="_self">Privacy Policy</a>.</div><section class="ot-sdk-row ot-cat-grp"><!-- Groups sections starts --><!-- Group section ends --><!-- Accordion Group section starts --><!-- Accordion Group section ends --><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="C0001"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-C0001" aria-labelledby="ot-header-id-C0001 ot-status-id-C0001"></button><!-- Accordion header --><div class="ot-acc-hdr ot-always-active-group"><div class="ot-plus-minus"><span></span><span></span></div><p role="heading" aria-level="4" class="ot-cat-header" id="ot-header-id-C0001">Required Cookies</p><div id="ot-status-id-C0001" class="ot-always-active">Always Active</div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-C0001">These cookies enable core functionality such as security, network management, and accessibility. These cookies are required for the site to function and cannot be turned off.</p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" dir="ltr" aria-label="Required Cookies - Cookie Details button opens Cookie List menu" data-parent-id="C0001">Cookies Details</button></div></div></div><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="C0002"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-C0002" aria-labelledby="ot-header-id-C0002"></button><!-- Accordion header --><div class="ot-acc-hdr"><div class="ot-plus-minus"><span></span><span></span></div><p role="heading" aria-level="4" class="ot-cat-header" id="ot-header-id-C0002">Performance Cookies</p><div class="ot-tgl"><input type="checkbox" name="ot-group-id-C0002" id="ot-group-id-C0002" class="category-switch-handler" data-optanongroupid="C0002" checked="" aria-labelledby="ot-header-id-C0002"> <label class="ot-switch" for="ot-group-id-C0002"><span class="ot-switch-nob"></span> <span class="ot-label-txt">Performance Cookies</span></label> </div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-C0002">These cookies are used to provide quantitative measures of our website visitors, such as the number of times you visit, time on page, your mouse movements, scrolling, clicks and keystroke activity on the websites; other browsing, search, or product research behavior; and what brought you to our site. These cookies may store a unique ID so that our system will remember you when you return.  Information collected with these cookies is used to measure and find ways to improve website performance.</p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" dir="ltr" aria-label="Performance Cookies - Cookie Details button opens Cookie List menu" data-parent-id="C0002">Cookies Details</button></div></div></div><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="C0003"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-C0003" aria-labelledby="ot-header-id-C0003"></button><!-- Accordion header --><div class="ot-acc-hdr"><div class="ot-plus-minus"><span></span><span></span></div><p role="heading" aria-level="4" class="ot-cat-header" id="ot-header-id-C0003">Personalization Cookies</p><div class="ot-tgl"><input type="checkbox" name="ot-group-id-C0003" id="ot-group-id-C0003" class="category-switch-handler" data-optanongroupid="C0003" checked="" aria-labelledby="ot-header-id-C0003"> <label class="ot-switch" for="ot-group-id-C0003"><span class="ot-switch-nob"></span> <span class="ot-label-txt">Personalization Cookies</span></label> </div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-C0003">These cookies collect data about how you have interacted with our website to help us improve your web experience, such as which pages you have visited. These cookies   may store a unique ID so that our system will remember you when you return.  They may be set by us or by third party providers whose services we have added to our pages. These cookies enable us to provide enhanced website functionality and personalization as well as make the marketing messages we send to you more relevant to your interests. If you do not allow these cookies, then some or all of these services may not function properly.</p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" dir="ltr" aria-label="Personalization Cookies - Cookie Details button opens Cookie List menu" data-parent-id="C0003">Cookies Details</button></div></div></div><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="C0004"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-C0004" aria-labelledby="ot-header-id-C0004"></button><!-- Accordion header --><div class="ot-acc-hdr"><div class="ot-plus-minus"><span></span><span></span></div><p role="heading" aria-level="4" class="ot-cat-header" id="ot-header-id-C0004">Advertising Cookies</p><div class="ot-tgl"><input type="checkbox" name="ot-group-id-C0004" id="ot-group-id-C0004" class="category-switch-handler" data-optanongroupid="C0004" checked="" aria-labelledby="ot-header-id-C0004"> <label class="ot-switch" for="ot-group-id-C0004"><span class="ot-switch-nob"></span> <span class="ot-label-txt">Advertising Cookies</span></label> </div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-C0004">These cookies record your visit to our websites, the pages you have visited and the links you have followed to influence the advertisements that you see on other websites. These cookies and the information they collect may be managed by other companies, including our advertising partners, and may be used to build a profile of your interests and show you relevant advertising on other sites. We and our advertising partners will use this information to make our websites and the advertising displayed on it,  more relevant to your interests.</p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" dir="ltr" aria-label="Advertising Cookies - Cookie Details button opens Cookie List menu" data-parent-id="C0004">Cookies Details</button></div></div></div></section></div><section id="ot-pc-lst" class="ot-hide ot-hosts-ui ot-pc-scrollbar"><div id="ot-pc-hdr"><div id="ot-lst-title"><button class="ot-link-btn back-btn-handler" aria-label="Back"><svg id="ot-back-arw" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 444.531 444.531" xml:space="preserve"><title>Back Button</title><g><path fill="#656565" d="M213.13,222.409L351.88,83.653c7.05-7.043,10.567-15.657,10.567-25.841c0-10.183-3.518-18.793-10.567-25.835
                    l-21.409-21.416C323.432,3.521,314.817,0,304.637,0s-18.791,3.521-25.841,10.561L92.649,196.425
                    c-7.044,7.043-10.566,15.656-10.566,25.841s3.521,18.791,10.566,25.837l186.146,185.864c7.05,7.043,15.66,10.564,25.841,10.564
                    s18.795-3.521,25.834-10.564l21.409-21.412c7.05-7.039,10.567-15.604,10.567-25.697c0-10.085-3.518-18.746-10.567-25.978
                    L213.13,222.409z"></path></g></svg></button><p role="heading" aria-level="3">Cookie List</p></div><div class="ot-lst-subhdr"><div class="ot-search-cntr"><p role="status" class="ot-scrn-rdr"></p><input id="vendor-search-handler" type="text" name="vendor-search-handler" placeholder="Search…" aria-label="Cookie list search"> <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 -30 110 110" aria-hidden="true"><title>Search Icon</title><path fill="#2e3644" d="M55.146,51.887L41.588,37.786c3.486-4.144,5.396-9.358,5.396-14.786c0-12.682-10.318-23-23-23s-23,10.318-23,23
            s10.318,23,23,23c4.761,0,9.298-1.436,13.177-4.162l13.661,14.208c0.571,0.593,1.339,0.92,2.162,0.92
            c0.779,0,1.518-0.297,2.079-0.837C56.255,54.982,56.293,53.08,55.146,51.887z M23.984,6c9.374,0,17,7.626,17,17s-7.626,17-17,17
            s-17-7.626-17-17S14.61,6,23.984,6z"></path></svg></div><div class="ot-fltr-cntr"><button id="filter-btn-handler" aria-label="Filter Cookie List" aria-haspopup="true" aria-expanded="false"><svg role="presentation" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 402.577 402.577" xml:space="preserve"><title>Filter Icon</title><g><path fill="#fff" d="M400.858,11.427c-3.241-7.421-8.85-11.132-16.854-11.136H18.564c-7.993,0-13.61,3.715-16.846,11.136
      c-3.234,7.801-1.903,14.467,3.999,19.985l140.757,140.753v138.755c0,4.955,1.809,9.232,5.424,12.854l73.085,73.083
      c3.429,3.614,7.71,5.428,12.851,5.428c2.282,0,4.66-0.479,7.135-1.43c7.426-3.238,11.14-8.851,11.14-16.845V172.166L396.861,31.413
      C402.765,25.895,404.093,19.231,400.858,11.427z"></path></g></svg></button></div><div id="ot-anchor"></div><section id="ot-fltr-modal"><div id="ot-fltr-cnt"><div id="ot-filter-list-header"></div><button id="clear-filters-handler">Clear</button><div class="ot-fltr-scrlcnt ot-pc-scrollbar" role="group" aria-labelledby="ot-filter-list-header"><ul class="ot-fltr-opts"><li class="ot-fltr-opt"><div class="ot-chkbox"><input id="chkbox-id" type="checkbox" class="category-filter-handler"> <label for="chkbox-id"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div></li></ul><div class="ot-fltr-btns"><button id="filter-apply-handler">Apply</button> <button id="filter-cancel-handler">Cancel</button></div></div></div></section></div></div><section id="ot-lst-cnt" class="ot-host-cnt ot-pc-scrollbar"><div id="ot-sel-blk"><div class="ot-sel-all"><div class="ot-sel-all-hdr"><span class="ot-consent-hdr">Consent</span> <span class="ot-li-hdr">Leg.Interest</span></div><div class="ot-sel-all-chkbox"><div class="ot-chkbox" id="ot-selall-hostcntr"><input id="select-all-hosts-groups-handler" type="checkbox"> <label for="select-all-hosts-groups-handler"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div><div class="ot-chkbox" id="ot-selall-vencntr"><input id="select-all-vendor-groups-handler" type="checkbox"> <label for="select-all-vendor-groups-handler"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div><div class="ot-chkbox" id="ot-selall-licntr"><input id="select-all-vendor-leg-handler" type="checkbox"> <label for="select-all-vendor-leg-handler"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div></div></div></div><div class="ot-sdk-row"><div class="ot-sdk-column"><ul id="ot-host-lst"></ul></div></div></section></section><div class="ot-pc-footer ot-pc-scrollbar"><div class="ot-btn-container"><button class="ot-pc-refuse-all-handler" ot-button-order="1">Decline All</button> <button class="save-preference-btn-handler onetrust-close-btn-handler" ot-button-order="2">Save and Accept</button></div><!-- Footer logo --><div class="ot-pc-footer-logo"><a href="https://www.onetrust.com/products/cookie-consent/" target="_blank" rel="noopener noreferrer" aria-label="Powered by OneTrust Opens in a new Tab"><img alt="Powered by Onetrust" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/powered_by_logo.svg" title="Powered by OneTrust Opens in a new Tab"></a></div></div><!-- Cookie subgroup container --><!-- Vendor list link --><!-- Cookie lost link --><!-- Toggle HTML element --><!-- Checkbox HTML --><!-- plus minus--><!-- Arrow SVG element --><!-- Accordion basic element --><span class="ot-scrn-rdr" aria-atomic="true" aria-live="polite"></span><!-- Vendor Service container and item template --></div><iframe class="ot-text-resize" sandbox="allow-same-origin" title="onetrust-text-resize" style="position: absolute; top: -50000px; width: 100em;" aria-hidden="true" src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/saved_resource.html"></iframe></div></div><script>
var script = document.createElement('script');
script.src = 'https://www.googletagmanager.com/gtag/js?id=AW-1041695361'; 
script.async = true;
document.body.appendChild(script);

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'AW-1041695361');
</script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/js" async=""></script><script>
!function(w,d){if(!w.rdt){var p=w.rdt=function()
{p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)}
;p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_fp03cfcwb8mz');rdt('track', 'PageVisit'); 
</script><script>
(function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:3655182,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><script>
 (function(w,d,t,r,u) { var f,n,i; w[u]=w[u]||[],f=function() { var o={ti:"56337120", enableAutoSpaTracking: true}; o.q=w[u],w[u]=new UET(o),w[u].push("pageLoad") }, n=d.createElement(t),n.src=r,n.async=1,n.onload=n.onreadystatechange=function() { var s=this.readyState; s&&s!=="loaded"&&s!=="complete"||(f(),n.onload=n.onreadystatechange=null) }, i=d.getElementsByTagName(t)[0],i.parentNode.insertBefore(n,i) }) (window,document,"script","//bat.bing.com/bat.js","uetq"); 
</script><script>
   !function(f,b,e,v,n,t,s)

  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?

  n.callMethod.apply(n,arguments):n.queue.push(arguments)};

  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';

  n.queue=[];t=b.createElement(e);t.async=!0;

  t.src=v;s=b.getElementsByTagName(e)[0];

  s.parentNode.insertBefore(t,s)}(window, document,'script',

  'https://connect.facebook.net/en_US/fbevents.js');

  fbq('init', '161755414605325');

  fbq('track', 'PageView');
</script><script>
var script = document.createElement('script');
script.src = 'https://www.googletagmanager.com/gtag/js?id=AW-974166441'; 
script.async = true;
document.body.appendChild(script);

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'AW-974166441');
</script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/js(1)" async=""></script><script>
var script = document.createElement('script');
script.src = 'https://www.googletagmanager.com/gtag/js?id=AW-1041695361'; 
script.async = true;
document.body.appendChild(script);

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'AW-1041695361');
</script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/js" async=""></script><script>
!function(w,d){if(!w.rdt){var p=w.rdt=function()
{p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)}
;p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_fp03cfcwb8mz');rdt('track', 'PageVisit'); 
</script><script>
(function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:3655182,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><script>
 (function(w,d,t,r,u) { var f,n,i; w[u]=w[u]||[],f=function() { var o={ti:"56337120", enableAutoSpaTracking: true}; o.q=w[u],w[u]=new UET(o),w[u].push("pageLoad") }, n=d.createElement(t),n.src=r,n.async=1,n.onload=n.onreadystatechange=function() { var s=this.readyState; s&&s!=="loaded"&&s!=="complete"||(f(),n.onload=n.onreadystatechange=null) }, i=d.getElementsByTagName(t)[0],i.parentNode.insertBefore(n,i) }) (window,document,"script","//bat.bing.com/bat.js","uetq"); 
</script><script>
   !function(f,b,e,v,n,t,s)

  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?

  n.callMethod.apply(n,arguments):n.queue.push(arguments)};

  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';

  n.queue=[];t=b.createElement(e);t.async=!0;

  t.src=v;s=b.getElementsByTagName(e)[0];

  s.parentNode.insertBefore(t,s)}(window, document,'script',

  'https://connect.facebook.net/en_US/fbevents.js');

  fbq('init', '161755414605325');

  fbq('track', 'PageView');
</script><script>
var script = document.createElement('script');
script.src = 'https://www.googletagmanager.com/gtag/js?id=AW-974166441'; 
script.async = true;
document.body.appendChild(script);

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'AW-974166441');
</script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/js(1)" async=""></script><script>
var script = document.createElement('script');
script.src = 'https://www.googletagmanager.com/gtag/js?id=AW-1041695361'; 
script.async = true;
document.body.appendChild(script);

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'AW-1041695361');
</script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/js" async=""></script><script>
!function(w,d){if(!w.rdt){var p=w.rdt=function()
{p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)}
;p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_fp03cfcwb8mz');rdt('track', 'PageVisit'); 
</script><script>
(function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:3655182,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><script>
 (function(w,d,t,r,u) { var f,n,i; w[u]=w[u]||[],f=function() { var o={ti:"56337120", enableAutoSpaTracking: true}; o.q=w[u],w[u]=new UET(o),w[u].push("pageLoad") }, n=d.createElement(t),n.src=r,n.async=1,n.onload=n.onreadystatechange=function() { var s=this.readyState; s&&s!=="loaded"&&s!=="complete"||(f(),n.onload=n.onreadystatechange=null) }, i=d.getElementsByTagName(t)[0],i.parentNode.insertBefore(n,i) }) (window,document,"script","//bat.bing.com/bat.js","uetq"); 
</script><script>
   !function(f,b,e,v,n,t,s)

  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?

  n.callMethod.apply(n,arguments):n.queue.push(arguments)};

  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';

  n.queue=[];t=b.createElement(e);t.async=!0;

  t.src=v;s=b.getElementsByTagName(e)[0];

  s.parentNode.insertBefore(t,s)}(window, document,'script',

  'https://connect.facebook.net/en_US/fbevents.js');

  fbq('init', '161755414605325');

  fbq('track', 'PageView');
</script><script>
var script = document.createElement('script');
script.src = 'https://www.googletagmanager.com/gtag/js?id=AW-974166441'; 
script.async = true;
document.body.appendChild(script);

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'AW-974166441');
</script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/js(1)" async=""></script><script>
var script = document.createElement('script');
script.src = 'https://www.googletagmanager.com/gtag/js?id=AW-1041695361'; 
script.async = true;
document.body.appendChild(script);

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'AW-1041695361');
</script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/js" async=""></script><script>
!function(w,d){if(!w.rdt){var p=w.rdt=function()
{p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)}
;p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_fp03cfcwb8mz');rdt('track', 'PageVisit'); 
</script><script>
(function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:3655182,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><script>
 (function(w,d,t,r,u) { var f,n,i; w[u]=w[u]||[],f=function() { var o={ti:"56337120", enableAutoSpaTracking: true}; o.q=w[u],w[u]=new UET(o),w[u].push("pageLoad") }, n=d.createElement(t),n.src=r,n.async=1,n.onload=n.onreadystatechange=function() { var s=this.readyState; s&&s!=="loaded"&&s!=="complete"||(f(),n.onload=n.onreadystatechange=null) }, i=d.getElementsByTagName(t)[0],i.parentNode.insertBefore(n,i) }) (window,document,"script","//bat.bing.com/bat.js","uetq"); 
</script><script>
   !function(f,b,e,v,n,t,s)

  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?

  n.callMethod.apply(n,arguments):n.queue.push(arguments)};

  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';

  n.queue=[];t=b.createElement(e);t.async=!0;

  t.src=v;s=b.getElementsByTagName(e)[0];

  s.parentNode.insertBefore(t,s)}(window, document,'script',

  'https://connect.facebook.net/en_US/fbevents.js');

  fbq('init', '161755414605325');

  fbq('track', 'PageView');
</script><script>
var script = document.createElement('script');
script.src = 'https://www.googletagmanager.com/gtag/js?id=AW-974166441'; 
script.async = true;
document.body.appendChild(script);

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'AW-974166441');
</script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/js(1)" async=""></script><script>
var script = document.createElement('script');
script.src = 'https://www.googletagmanager.com/gtag/js?id=AW-1041695361'; 
script.async = true;
document.body.appendChild(script);

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'AW-1041695361');
</script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/js" async=""></script><script>
!function(w,d){if(!w.rdt){var p=w.rdt=function()
{p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)}
;p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_fp03cfcwb8mz');rdt('track', 'PageVisit'); 
</script><script>
(function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:3655182,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><script>
 (function(w,d,t,r,u) { var f,n,i; w[u]=w[u]||[],f=function() { var o={ti:"56337120", enableAutoSpaTracking: true}; o.q=w[u],w[u]=new UET(o),w[u].push("pageLoad") }, n=d.createElement(t),n.src=r,n.async=1,n.onload=n.onreadystatechange=function() { var s=this.readyState; s&&s!=="loaded"&&s!=="complete"||(f(),n.onload=n.onreadystatechange=null) }, i=d.getElementsByTagName(t)[0],i.parentNode.insertBefore(n,i) }) (window,document,"script","//bat.bing.com/bat.js","uetq"); 
</script><script>
   !function(f,b,e,v,n,t,s)

  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?

  n.callMethod.apply(n,arguments):n.queue.push(arguments)};

  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';

  n.queue=[];t=b.createElement(e);t.async=!0;

  t.src=v;s=b.getElementsByTagName(e)[0];

  s.parentNode.insertBefore(t,s)}(window, document,'script',

  'https://connect.facebook.net/en_US/fbevents.js');

  fbq('init', '161755414605325');

  fbq('track', 'PageView');
</script><script>
var script = document.createElement('script');
script.src = 'https://www.googletagmanager.com/gtag/js?id=AW-974166441'; 
script.async = true;
document.body.appendChild(script);

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'AW-974166441');
</script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/js(1)" async=""></script><script>
var script = document.createElement('script');
script.src = 'https://www.googletagmanager.com/gtag/js?id=AW-1041695361'; 
script.async = true;
document.body.appendChild(script);

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'AW-1041695361');
</script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/js" async=""></script><script>
!function(w,d){if(!w.rdt){var p=w.rdt=function()
{p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)}
;p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_fp03cfcwb8mz');rdt('track', 'PageVisit'); 
</script><script>
(function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:3655182,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><script>
 (function(w,d,t,r,u) { var f,n,i; w[u]=w[u]||[],f=function() { var o={ti:"56337120", enableAutoSpaTracking: true}; o.q=w[u],w[u]=new UET(o),w[u].push("pageLoad") }, n=d.createElement(t),n.src=r,n.async=1,n.onload=n.onreadystatechange=function() { var s=this.readyState; s&&s!=="loaded"&&s!=="complete"||(f(),n.onload=n.onreadystatechange=null) }, i=d.getElementsByTagName(t)[0],i.parentNode.insertBefore(n,i) }) (window,document,"script","//bat.bing.com/bat.js","uetq"); 
</script><script>
   !function(f,b,e,v,n,t,s)

  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?

  n.callMethod.apply(n,arguments):n.queue.push(arguments)};

  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';

  n.queue=[];t=b.createElement(e);t.async=!0;

  t.src=v;s=b.getElementsByTagName(e)[0];

  s.parentNode.insertBefore(t,s)}(window, document,'script',

  'https://connect.facebook.net/en_US/fbevents.js');

  fbq('init', '161755414605325');

  fbq('track', 'PageView');
</script><script>
var script = document.createElement('script');
script.src = 'https://www.googletagmanager.com/gtag/js?id=AW-974166441'; 
script.async = true;
document.body.appendChild(script);

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'AW-974166441');
</script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/js(1)" async=""></script><script>
var script = document.createElement('script');
script.src = 'https://www.googletagmanager.com/gtag/js?id=AW-1041695361'; 
script.async = true;
document.body.appendChild(script);

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'AW-1041695361');
</script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/js" async=""></script><script>
!function(w,d){if(!w.rdt){var p=w.rdt=function()
{p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)}
;p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_fp03cfcwb8mz');rdt('track', 'PageVisit'); 
</script><script>
(function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:3655182,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><script>
 (function(w,d,t,r,u) { var f,n,i; w[u]=w[u]||[],f=function() { var o={ti:"56337120", enableAutoSpaTracking: true}; o.q=w[u],w[u]=new UET(o),w[u].push("pageLoad") }, n=d.createElement(t),n.src=r,n.async=1,n.onload=n.onreadystatechange=function() { var s=this.readyState; s&&s!=="loaded"&&s!=="complete"||(f(),n.onload=n.onreadystatechange=null) }, i=d.getElementsByTagName(t)[0],i.parentNode.insertBefore(n,i) }) (window,document,"script","//bat.bing.com/bat.js","uetq"); 
</script><script>
   !function(f,b,e,v,n,t,s)

  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?

  n.callMethod.apply(n,arguments):n.queue.push(arguments)};

  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';

  n.queue=[];t=b.createElement(e);t.async=!0;

  t.src=v;s=b.getElementsByTagName(e)[0];

  s.parentNode.insertBefore(t,s)}(window, document,'script',

  'https://connect.facebook.net/en_US/fbevents.js');

  fbq('init', '161755414605325');

  fbq('track', 'PageView');
</script><script>
var script = document.createElement('script');
script.src = 'https://www.googletagmanager.com/gtag/js?id=AW-974166441'; 
script.async = true;
document.body.appendChild(script);

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'AW-974166441');
</script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/js(1)" async=""></script><script>
var script = document.createElement('script');
script.src = 'https://www.googletagmanager.com/gtag/js?id=AW-1041695361'; 
script.async = true;
document.body.appendChild(script);

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'AW-1041695361');
</script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/js" async=""></script><script>
!function(w,d){if(!w.rdt){var p=w.rdt=function()
{p.sendEvent?p.sendEvent.apply(p,arguments):p.callQueue.push(arguments)}
;p.callQueue=[];var t=d.createElement("script");t.src="https://www.redditstatic.com/ads/pixel.js",t.async=!0;var s=d.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}}(window,document);rdt('init','a2_fp03cfcwb8mz');rdt('track', 'PageVisit'); 
</script><script>
(function(h,o,t,j,a,r){
        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};
        h._hjSettings={hjid:3655182,hjsv:6};
        a=o.getElementsByTagName('head')[0];
        r=o.createElement('script');r.async=1;
        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
        a.appendChild(r);
    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script><script>
 (function(w,d,t,r,u) { var f,n,i; w[u]=w[u]||[],f=function() { var o={ti:"56337120", enableAutoSpaTracking: true}; o.q=w[u],w[u]=new UET(o),w[u].push("pageLoad") }, n=d.createElement(t),n.src=r,n.async=1,n.onload=n.onreadystatechange=function() { var s=this.readyState; s&&s!=="loaded"&&s!=="complete"||(f(),n.onload=n.onreadystatechange=null) }, i=d.getElementsByTagName(t)[0],i.parentNode.insertBefore(n,i) }) (window,document,"script","//bat.bing.com/bat.js","uetq"); 
</script><script>
   !function(f,b,e,v,n,t,s)

  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?

  n.callMethod.apply(n,arguments):n.queue.push(arguments)};

  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';

  n.queue=[];t=b.createElement(e);t.async=!0;

  t.src=v;s=b.getElementsByTagName(e)[0];

  s.parentNode.insertBefore(t,s)}(window, document,'script',

  'https://connect.facebook.net/en_US/fbevents.js');

  fbq('init', '161755414605325');

  fbq('track', 'PageView');
</script><script>
var script = document.createElement('script');
script.src = 'https://www.googletagmanager.com/gtag/js?id=AW-974166441'; 
script.async = true;
document.body.appendChild(script);

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'AW-974166441');
</script><script src="./Explore Speech Models _ Try NVIDIA NIM APIs_files/js(1)" async=""></script></body></html>